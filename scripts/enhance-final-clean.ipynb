{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978bdb4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-24T09:44:21.519594Z",
     "iopub.status.busy": "2025-08-24T09:44:21.519322Z",
     "iopub.status.idle": "2025-08-24T09:59:03.276137Z",
     "shell.execute_reply": "2025-08-24T09:59:03.275020Z"
    },
    "papermill": {
     "duration": 881.763919,
     "end_time": "2025-08-24T09:59:03.277432",
     "exception": false,
     "start_time": "2025-08-24T09:44:21.513513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: cuda\n",
      "æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å½¢çŠ¶: (2313810, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç§»é™¤äº† 84,674 æ¡é‡å¤è®°å½•\n",
      "æ ‡ç­¾åˆ†å¸ƒ:\n",
      "Multi_Label\n",
      "Benign         1892659\n",
      "DoS             193730\n",
      "DDoS            128014\n",
      "Brute_Force       9150\n",
      "Web_Attack        2143\n",
      "PortScan          1956\n",
      "Bot               1437\n",
      "Name: count, dtype: int64\n",
      "ç‰¹å¾ç»´åº¦: 77\n",
      "å¤šåˆ†ç±»ç±»åˆ«: ['Benign', 'Bot', 'Brute_Force', 'DDoS', 'DoS', 'PortScan', 'Web_Attack']\n",
      "\n",
      "=== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒTransformerå¢å¼ºçš„äºŒåˆ†ç±»é¢„è®­ç»ƒæ¨¡å‹ ===\n",
      "è®­ç»ƒé›†å¤§å°: 1,426,616\n",
      "æ¨¡å‹å‚æ•°é‡: 1,722,730\n",
      "å¼€å§‹è®­ç»ƒ TransformerBinary_Pretrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:30<00:00, 45.21it/s, Loss=0.0040]\n",
      "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 51.62it/s, Loss=0.0098]\n",
      "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 50.36it/s, Loss=0.0027]\n",
      "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 51.88it/s, Loss=0.0030]\n",
      "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.33it/s, Loss=0.0024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: è®­ç»ƒF1: 0.9831 | éªŒè¯F1: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.17it/s, Loss=0.0216]\n",
      "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 51.92it/s, Loss=0.0047]\n",
      "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 51.30it/s, Loss=0.0225]\n",
      "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 50.50it/s, Loss=0.0030]\n",
      "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 51.54it/s, Loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: è®­ç»ƒF1: 0.9862 | éªŒè¯F1: 0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 51.47it/s, Loss=0.0015]\n",
      "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 51.35it/s, Loss=0.0091]\n",
      "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.07it/s, Loss=0.0035]\n",
      "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:27<00:00, 50.37it/s, Loss=0.0032]\n",
      "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.26it/s, Loss=0.0050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: è®­ç»ƒF1: 0.9879 | éªŒè¯F1: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.43it/s, Loss=0.0109]\n",
      "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 51.95it/s, Loss=0.0033]\n",
      "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1394/1394 [00:26<00:00, 52.17it/s, Loss=0.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: 0.9899\n",
      "äºŒåˆ†ç±»é¢„è®­ç»ƒå®Œæˆï¼Œæœ€ä½³F1: 0.9899\n",
      "\n",
      "=== ç¬¬äºŒé˜¶æ®µï¼šå‡†å¤‡å¤šåˆ†ç±»æ•°æ®å’Œåˆ†å±‚é‡‡æ · ===\n",
      "æ¶æ„æµé‡æ ·æœ¬æ•°: 336,430\n",
      "æ¶æ„æµé‡ç±»åˆ«åˆ†å¸ƒ:\n",
      "  Bot: 1,437 æ ·æœ¬ (0.43%)\n",
      "  Brute_Force: 9,150 æ ·æœ¬ (2.72%)\n",
      "  DDoS: 128,014 æ ·æœ¬ (38.05%)\n",
      "  DoS: 193,730 æ ·æœ¬ (57.58%)\n",
      "  PortScan: 1,956 æ ·æœ¬ (0.58%)\n",
      "  Web_Attack: 2,143 æ ·æœ¬ (0.64%)\n",
      "å¤šåˆ†ç±»è®­ç»ƒé›†: 215,315\n",
      "SMOTEé‡‡æ ·å®Œæˆ: 215,315 â†’ 404,292 æ ·æœ¬\n",
      "\n",
      "=== ç¬¬ä¸‰é˜¶æ®µï¼šç«¯åˆ°ç«¯å¾®è°ƒå¤šåˆ†ç±»æ¨¡å‹ ===\n",
      "æˆåŠŸåŠ è½½é¢„è®­ç»ƒç¼–ç å™¨: /kaggle/working/best_TransformerBinary_Pretrain.pth\n",
      "å¼€å§‹è®­ç»ƒ TransformerMultiClass_FineTuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.55it/s, Loss=0.0112]\n",
      "Epoch 2/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.76it/s, Loss=0.0258]\n",
      "Epoch 3/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.58it/s, Loss=0.0044]\n",
      "Epoch 4/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.02it/s, Loss=0.0029]\n",
      "Epoch 5/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 39.61it/s, Loss=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: è®­ç»ƒF1: 0.9904 | éªŒè¯F1: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.30it/s, Loss=0.0036]\n",
      "Epoch 7/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.38it/s, Loss=0.0039]\n",
      "Epoch 8/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.09it/s, Loss=0.0020]\n",
      "Epoch 9/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 40.39it/s, Loss=0.0056]\n",
      "Epoch 10/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:20<00:00, 39.31it/s, Loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: è®­ç»ƒF1: 0.9916 | éªŒè¯F1: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 790/790 [00:19<00:00, 39.55it/s, Loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: 0.9687\n",
      "å¤šåˆ†ç±»å¾®è°ƒå®Œæˆï¼Œæœ€ä½³F1: 0.9687\n",
      "\n",
      "=== å±‚æ¬¡åŒ–Transformerå¢å¼ºæ¨¡å‹è¯„ä¼° ===\n",
      "\n",
      "è¯„ä¼° Transformerå¢å¼ºäºŒåˆ†ç±»æ¨¡å‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformerå¢å¼ºäºŒåˆ†ç±»æ¨¡å‹ åˆ†ç±»æŠ¥å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9979    0.9960    0.9970    378532\n",
      "   Malicious     0.9777    0.9883    0.9830     67286\n",
      "\n",
      "    accuracy                         0.9948    445818\n",
      "   macro avg     0.9878    0.9922    0.9900    445818\n",
      "weighted avg     0.9949    0.9948    0.9948    445818\n",
      "\n",
      "\n",
      "Transformerå¢å¼ºäºŒåˆ†ç±»æ¨¡å‹ æ•´ä½“æ€§èƒ½:\n",
      "  å‡†ç¡®ç‡: 0.9948\n",
      "  å®å¹³å‡F1: 0.9900\n",
      "  åŠ æƒå¹³å‡F1: 0.9948\n",
      "  å¹³å‡ä¸ç¡®å®šæ€§: 0.0001\n",
      "\n",
      "è¯„ä¼° Transformerå¢å¼ºå¤šåˆ†ç±»æ¨¡å‹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformerå¢å¼ºå¤šåˆ†ç±»æ¨¡å‹ åˆ†ç±»æŠ¥å‘Š:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bot     0.9379    1.0000    0.9680       287\n",
      " Brute_Force     1.0000    0.9781    0.9890      1830\n",
      "        DDoS     0.9999    0.9993    0.9996     25603\n",
      "         DoS     0.9994    0.9979    0.9986     38746\n",
      "    PortScan     0.9842    0.9540    0.9688       391\n",
      "  Web_Attack     0.7992    0.9930    0.8857       429\n",
      "\n",
      "    accuracy                         0.9976     67286\n",
      "   macro avg     0.9534    0.9871    0.9683     67286\n",
      "weighted avg     0.9979    0.9976    0.9977     67286\n",
      "\n",
      "\n",
      "Transformerå¢å¼ºå¤šåˆ†ç±»æ¨¡å‹ æ•´ä½“æ€§èƒ½:\n",
      "  å‡†ç¡®ç‡: 0.9976\n",
      "  å®å¹³å‡F1: 0.9683\n",
      "  åŠ æƒå¹³å‡F1: 0.9977\n",
      "  å¹³å‡ä¸ç¡®å®šæ€§: 0.0027\n",
      "\n",
      "=== å±‚æ¬¡åŒ–Transformerå¢å¼ºæ£€æµ‹ç³»ç»Ÿæ•ˆæœæ€»ç»“ ===\n",
      "\n",
      "äºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½:\n",
      "   å‡†ç¡®ç‡: 0.9948\n",
      "   å®å¹³å‡F1: 0.9900\n",
      "\n",
      "å¤šåˆ†ç±»æ¨¡å‹æ€§èƒ½:\n",
      "   å‡†ç¡®ç‡: 0.9976\n",
      "   å®å¹³å‡F1: 0.9683\n",
      "\n",
      "ç»“æœå·²ä¿å­˜åˆ°: /kaggle/working/hierarchical_transformer_results.json\n",
      "\n",
      "å±‚æ¬¡åŒ–Transformerå¢å¼ºçš„ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿå®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Hierarchical Transformer-Enhanced Network Intrusion Detection System\n",
    "# ç®€åŒ–è¾“å‡ºç‰ˆæœ¬ - ä¿æŒæ ¸å¿ƒåŠŸèƒ½ï¼Œç²¾ç®€è¾“å‡ºä¿¡æ¯\n",
    "# =====================================================================\n",
    "\n",
    "# Cell 1: ç¯å¢ƒå‡†å¤‡å’Œå¯¼å…¥åº“\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# å®‰è£…å¿…è¦çš„åŒ…\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\", \"imbalanced-learn\"], \n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 2: Transformerå¢å¼ºçš„æ¨¡å‹æ¶æ„\n",
    "# =====================================================================\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶ï¼šç»“åˆç±»åˆ«ã€æ—¶é—´ã€ç©ºé—´æ³¨æ„åŠ›\"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # ç±»åˆ«ç‰¹å®šæ³¨æ„åŠ›ï¼ˆä¿æŒåŸæœ‰è®¾è®¡ï¼‰\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(input_dim // 4, input_dim),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        \n",
    "        # ğŸ†• æ—¶é—´åºåˆ—æ³¨æ„åŠ› - å­¦ä¹ æµé‡çš„æ—¶åºæ¨¡å¼\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            input_dim, num_heads=4, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # ğŸ†• ç©ºé—´ç‰¹å¾æ³¨æ„åŠ› - å­¦ä¹ ç‰¹å¾é—´çš„ç©ºé—´å…³ç³»\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(input_dim // 8, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # ğŸ†• æ³¨æ„åŠ›èåˆæƒé‡\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. ç±»åˆ«ç‰¹å®šæ³¨æ„åŠ›ï¼ˆåŸæœ‰æœºåˆ¶ï¼‰\n",
    "        class_features = []\n",
    "        for i in range(self.num_classes):\n",
    "            attention_weights = self.class_attention[i](x)\n",
    "            attended_features = attention_weights * x\n",
    "            class_features.append(attended_features)\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        \n",
    "        # 2. ğŸ†• æ—¶é—´åºåˆ—æ³¨æ„åŠ›\n",
    "        x_temporal = x.unsqueeze(1)  # [B, 1, D]\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)  # [B, D]\n",
    "        \n",
    "        # 3. ğŸ†• ç©ºé—´ç‰¹å¾æ³¨æ„åŠ›\n",
    "        spatial_weights = self.spatial_attention(x)\n",
    "        spatial_attended = spatial_weights * x\n",
    "        \n",
    "        # 4. ğŸ†• å¤šå°ºåº¦èåˆ\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        # åŠ æƒèåˆä¸‰ç§æ³¨æ„åŠ›\n",
    "        fused_features = (weights[0] * class_attended_mean + \n",
    "                         weights[1] * temporal_attended + \n",
    "                         weights[2] * spatial_attended)\n",
    "        \n",
    "        return class_attended, fused_features\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    \"\"\"Transformerå¢å¼ºçš„é›†æˆæ¨¡å‹ï¼šæ”¯æŒå±‚æ¬¡åŒ–æ£€æµ‹æ¶æ„\"\"\"\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3, use_pretrained=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.use_pretrained = use_pretrained\n",
    "        \n",
    "        # å…±äº«ç¼–ç å™¨ï¼ˆå¯ä»¥è¢«é¢„è®­ç»ƒå’Œè¿ç§»ï¼‰\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate / 2)\n",
    "        )\n",
    "        \n",
    "        # ğŸ†• Transformerç¼–ç å™¨ - å­¦ä¹ é•¿è·ç¦»ä¾èµ–å…³ç³»\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.feature_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation='gelu'  # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # ğŸ†• ä½ç½®ç¼–ç  - ä¸ºç‰¹å¾æ·»åŠ ä½ç½®ä¿¡æ¯\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        \n",
    "        # å¤šå°ºåº¦æ³¨æ„åŠ›æœºåˆ¶\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        \n",
    "        # ç±»åˆ«ç‰¹å®šåˆ†ç±»å¤´ï¼ˆä¿æŒåŸæœ‰è®¾è®¡ï¼‰\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1)\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        \n",
    "        # å…¨å±€åˆ†ç±»å™¨ï¼ˆæ”¹è¿›ç‰ˆï¼‰\n",
    "        self.global_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),  # ğŸ†• ä½¿ç”¨LayerNorm\n",
    "            nn.GELU(),          # ğŸ†• ä½¿ç”¨GELUæ¿€æ´»\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # ğŸ†• è‡ªé€‚åº”èåˆç½‘ç»œ - å­¦ä¹ æœ€ä¼˜èåˆç­–ç•¥\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # ç‰¹å¾æå–å™¨ï¼ˆç”¨äºè¿ç§»å­¦ä¹ ï¼‰\n",
    "        self.feature_extractor = nn.Identity()\n",
    "        \n",
    "        # ğŸ†• ä¸ç¡®å®šæ€§ä¼°è®¡å¤´ - è¯„ä¼°é¢„æµ‹ç½®ä¿¡åº¦\n",
    "        self.uncertainty_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, return_features=False, return_uncertainty=False):\n",
    "        # å…±äº«ç‰¹å¾æå–\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return shared_features\n",
    "        \n",
    "        # ğŸ†• Transformerå¤„ç† - å­¦ä¹ ç‰¹å¾é—´çš„å…¨å±€ä¾èµ–\n",
    "        # æ·»åŠ ä½ç½®ç¼–ç å¹¶è½¬æ¢ä¸ºåºåˆ—æ ¼å¼\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input)\n",
    "        transformer_features = transformer_features.squeeze(1)  # [B, 256]\n",
    "        \n",
    "        # ğŸ†• æ®‹å·®è¿æ¥ + Layer Normalization\n",
    "        enhanced_features = F.layer_norm(\n",
    "            shared_features + transformer_features, \n",
    "            normalized_shape=[self.feature_dim]\n",
    "        )\n",
    "        \n",
    "        # å¤šå°ºåº¦æ³¨æ„åŠ›å¤„ç†\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        \n",
    "        # ç±»åˆ«ç‰¹å®šè¾“å‡º\n",
    "        class_specific_outputs = []\n",
    "        for i in range(self.num_classes):\n",
    "            output = self.class_specific_heads[i](class_attended_features[:, i, :])\n",
    "            class_specific_outputs.append(output)\n",
    "        class_specific_logits = torch.cat(class_specific_outputs, dim=1)\n",
    "        \n",
    "        # å…¨å±€è¾“å‡ºï¼ˆä½¿ç”¨èåˆåçš„ç‰¹å¾ï¼‰\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        \n",
    "        # ğŸ†• å­¦ä¹ çš„è‡ªé€‚åº”èåˆ\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = (fusion_weights[:, 0:1] * class_specific_logits + \n",
    "                       fusion_weights[:, 1:2] * global_logits)\n",
    "        \n",
    "        # ğŸ†• ä¸ç¡®å®šæ€§ä¼°è®¡\n",
    "        if return_uncertainty:\n",
    "            uncertainty = self.uncertainty_head(enhanced_features)\n",
    "            return final_logits, uncertainty\n",
    "        \n",
    "        return final_logits\n",
    "    \n",
    "    def load_pretrained_encoder(self, pretrained_model_path):\n",
    "        \"\"\"åŠ è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨æƒé‡\"\"\"\n",
    "        if os.path.exists(pretrained_model_path):\n",
    "            pretrained_state = torch.load(pretrained_model_path, map_location=device)\n",
    "            # åªåŠ è½½ç¼–ç å™¨éƒ¨åˆ†çš„æƒé‡\n",
    "            encoder_state = {}\n",
    "            for key, value in pretrained_state.items():\n",
    "                if key.startswith('shared_encoder'):\n",
    "                    encoder_state[key] = value\n",
    "            \n",
    "            self.load_state_dict(encoder_state, strict=False)\n",
    "            print(f\"æˆåŠŸåŠ è½½é¢„è®­ç»ƒç¼–ç å™¨: {pretrained_model_path}\")\n",
    "        else:\n",
    "            print(f\"é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {pretrained_model_path}\")\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 3: æ”¹è¿›çš„æŸå¤±å‡½æ•°å’Œè®­ç»ƒå·¥å…·\n",
    "# =====================================================================\n",
    "\n",
    "class UncertaintyAwareFocalLoss(nn.Module):\n",
    "    \"\"\"ä¸ç¡®å®šæ€§æ„ŸçŸ¥çš„è‡ªé€‚åº”ç„¦ç‚¹æŸå¤±\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, class_specific_gamma=None, uncertainty_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_specific_gamma = class_specific_gamma or {}\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "        \n",
    "    def forward(self, inputs, targets, uncertainty=None):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # ä¸ºä¸åŒç±»åˆ«ä½¿ç”¨ä¸åŒçš„gammaå€¼\n",
    "        gamma_values = torch.full_like(targets, self.gamma, dtype=torch.float)\n",
    "        for class_idx, gamma_val in self.class_specific_gamma.items():\n",
    "            mask = (targets == class_idx)\n",
    "            gamma_values[mask] = gamma_val\n",
    "        \n",
    "        # åŸºç¡€focal loss\n",
    "        focal_loss = (1 - pt) ** gamma_values * ce_loss\n",
    "        \n",
    "        # ğŸ†• ä¸ç¡®å®šæ€§åŠ æƒï¼šé«˜ä¸ç¡®å®šæ€§æ ·æœ¬è·å¾—æ›´å¤šå…³æ³¨\n",
    "        if uncertainty is not None:\n",
    "            uncertainty_weight = 1 + self.uncertainty_weight * uncertainty.squeeze()\n",
    "            focal_loss = uncertainty_weight * focal_loss\n",
    "        \n",
    "        # Alphaæƒé‡å¹³è¡¡\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "def create_stratified_sampler(y_train, sampling_strategy):\n",
    "    \"\"\"åˆ›å»ºåˆ†å±‚é‡‡æ ·ç­–ç•¥\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    \n",
    "    # åº”ç”¨é‡‡æ ·ç­–ç•¥\n",
    "    if sampling_strategy['method'] == 'hierarchical_smote':\n",
    "        return apply_hierarchical_smote(y_train, sampling_strategy)\n",
    "    elif sampling_strategy['method'] == 'adaptive_smote':\n",
    "        return apply_adaptive_smote(y_train, sampling_strategy)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_hierarchical_smote(y_train, strategy):\n",
    "    \"\"\"åˆ†å±‚SMOTEé‡‡æ ·\"\"\"\n",
    "    tier1_classes = strategy.get('tier1_classes', [])  # å…³é”®å°‘æ•°ç±»\n",
    "    tier2_classes = strategy.get('tier2_classes', [])  # æ™®é€šå°‘æ•°ç±»\n",
    "    \n",
    "    sampling_ratios = {}\n",
    "    class_counts = Counter(y_train)\n",
    "    max_count = max(class_counts.values())\n",
    "    \n",
    "    for class_idx, count in class_counts.items():\n",
    "        if class_idx in tier1_classes:\n",
    "            # å…³é”®ç±»åˆ«ï¼šé‡‡æ ·åˆ°æœ€å¤§ç±»åˆ«çš„50%\n",
    "            sampling_ratios[class_idx] = int(max_count * 0.5)\n",
    "        elif class_idx in tier2_classes:\n",
    "            # æ™®é€šå°‘æ•°ç±»ï¼šé‡‡æ ·åˆ°æœ€å¤§ç±»åˆ«çš„30%\n",
    "            sampling_ratios[class_idx] = int(max_count * 0.3)\n",
    "        else:\n",
    "            # å¤šæ•°ç±»ï¼šä¿æŒåŸæ ·\n",
    "            sampling_ratios[class_idx] = count\n",
    "    \n",
    "    return sampling_ratios\n",
    "\n",
    "def apply_adaptive_smote(y_train, strategy):\n",
    "    \"\"\"è‡ªé€‚åº”SMOTEé‡‡æ ·\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    \n",
    "    sampling_ratios = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        # åŸºäºç±»åˆ«å æ¯”çš„è‡ªé€‚åº”é‡‡æ ·\n",
    "        class_ratio = count / total_samples\n",
    "        if class_ratio < 0.01:  # æå°‘æ•°ç±»\n",
    "            target_ratio = 0.05\n",
    "        elif class_ratio < 0.05:  # å°‘æ•°ç±»\n",
    "            target_ratio = 0.1\n",
    "        else:  # å¤šæ•°ç±»\n",
    "            target_ratio = class_ratio\n",
    "        \n",
    "        sampling_ratios[class_idx] = int(total_samples * target_ratio)\n",
    "    \n",
    "    return sampling_ratios\n",
    "\n",
    "def train_transformer_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"æ”¹è¿›çš„Transformeræ¨¡å‹è®­ç»ƒå‡½æ•°\"\"\"\n",
    "    print(f\"å¼€å§‹è®­ç»ƒ {config['model_name']}\")\n",
    "    \n",
    "    # ğŸ†• åˆ†å±‚å­¦ä¹ ç‡ä¼˜åŒ–å™¨è®¾ç½®\n",
    "    if config.get('use_pretrained', False):\n",
    "        # å¾®è°ƒæ¨¡å¼ï¼šä¸åŒç»„ä»¶ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡\n",
    "        encoder_params = []\n",
    "        transformer_params = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'shared_encoder' in name:\n",
    "                encoder_params.append(param)\n",
    "            elif 'transformer' in name or 'pos_encoding' in name:\n",
    "                transformer_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': config['lr'] * 0.1},      # ç¼–ç å™¨è¾ƒä½å­¦ä¹ ç‡\n",
    "            {'params': transformer_params, 'lr': config['lr'] * 0.5},  # Transformerä¸­ç­‰å­¦ä¹ ç‡\n",
    "            {'params': other_params, 'lr': config['lr']}               # å…¶ä»–ç»„ä»¶æ­£å¸¸å­¦ä¹ ç‡\n",
    "        ], weight_decay=config.get('weight_decay', 1e-4))\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'], \n",
    "                              weight_decay=config.get('weight_decay', 1e-4))\n",
    "    \n",
    "    # ğŸ†• ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=config['lr'] if not config.get('use_pretrained', False) else [config['lr']*0.1, config['lr']*0.5, config['lr']],\n",
    "        epochs=config['num_epochs'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # ğŸ†• ä¸ç¡®å®šæ€§æ„ŸçŸ¥æŸå¤±å‡½æ•°\n",
    "    if 'class_weights' in config and config['class_weights'] is not None:\n",
    "        alpha = torch.FloatTensor(config['class_weights']).to(device)\n",
    "    else:\n",
    "        alpha = None\n",
    "    \n",
    "    # ä¸ºå…³é”®å°‘æ•°ç±»ä½¿ç”¨æ›´é«˜çš„gammaå€¼\n",
    "    class_specific_gamma = {}\n",
    "    if 'minority_classes' in config:\n",
    "        for class_idx in config['minority_classes']:\n",
    "            class_specific_gamma[class_idx] = 3.0  # æ›´å…³æ³¨å›°éš¾æ ·æœ¬\n",
    "    \n",
    "    criterion = UncertaintyAwareFocalLoss(\n",
    "        alpha=alpha, \n",
    "        gamma=2.0,\n",
    "        class_specific_gamma=class_specific_gamma,\n",
    "        uncertainty_weight=0.5\n",
    "    )\n",
    "    \n",
    "    # è®­ç»ƒå¾ªç¯\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_path = f\"/kaggle/working/best_{config['model_name']}.pth\"\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ğŸ†• å‰å‘ä¼ æ’­ï¼ˆåŒ…å«ä¸ç¡®å®šæ€§ä¼°è®¡ï¼‰\n",
    "            outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "            loss = criterion(outputs, labels, uncertainty)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # ğŸ†• æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # æ”¶é›†é¢„æµ‹ç»“æœ\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        val_uncertainties = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "                loss = criterion(outputs, labels, uncertainty)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_uncertainties.extend(uncertainty.cpu().numpy())\n",
    "        \n",
    "        # è®¡ç®—æŒ‡æ ‡\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:  # åªæ¯5ä¸ªepochæ‰“å°ä¸€æ¬¡\n",
    "            print(f\"Epoch {epoch+1}: è®­ç»ƒF1: {train_f1:.4f} | éªŒè¯F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.get('patience', 7):\n",
    "                print(f\"æ—©åœè§¦å‘ï¼Œæœ€ä½³F1: {best_val_f1:.4f}\")\n",
    "                break\n",
    "    \n",
    "    return best_model_path, best_val_f1\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 4: æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼ˆä¿®æ­£ç‰ˆ - ä¸simple.ipynbä¿æŒä¸€è‡´ï¼‰\n",
    "# =====================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"åŠ è½½å’Œé¢„å¤„ç†CIC-IDS2017æ•°æ®\"\"\"\n",
    "    print(\"æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ä¸­...\")\n",
    "    \n",
    "    # åŠ è½½æ•°æ®\n",
    "    data_path = '/kaggle/input/cicids2017'\n",
    "    \n",
    "    parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path) \n",
    "                     if f.endswith('.parquet')]\n",
    "    \n",
    "    df_list = []\n",
    "    for file in tqdm(parquet_files, desc=\"åŠ è½½æ•°æ®æ–‡ä»¶\", leave=False):\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        df_list.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    del df_list\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"æ•°æ®å½¢çŠ¶: {df.shape}\")\n",
    "    \n",
    "    # æ•°æ®æ¸…ç†\n",
    "    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n",
    "    label_column = 'Label'\n",
    "    \n",
    "    # å¤„ç†æ— ç©·å€¼å’ŒNaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # ç¦»ç¾¤å€¼ä¿®å‰ª\n",
    "    numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "    numeric_columns = [col for col in numeric_columns if col != label_column]\n",
    "    \n",
    "    for col in tqdm(numeric_columns, desc=\"æ•°æ®æ¸…ç†\", leave=False):\n",
    "        q99, q01 = df[col].quantile(0.99), df[col].quantile(0.01)\n",
    "        df[col] = df[col].clip(lower=q01, upper=q99)\n",
    "    \n",
    "    # å»é‡\n",
    "    rows_before = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if rows_before != df.shape[0]:\n",
    "        print(f\"ç§»é™¤äº† {rows_before - df.shape[0]:,} æ¡é‡å¤è®°å½•\")\n",
    "    \n",
    "    # æ ‡ç­¾æ¸…ç†å’Œæ˜ å°„\n",
    "    df[label_column] = df[label_column].astype(str).str.replace(\n",
    "        r'[^a-zA-Z0-9\\s-]', '', regex=True\n",
    "    ).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    # åˆ›å»ºæ ‡ç­¾æ˜ å°„\n",
    "    # 1. äºŒåˆ†ç±»æ ‡ç­¾\n",
    "    df['Binary_Label'] = df[label_column].apply(\n",
    "        lambda x: 'Benign' if x == 'Benign' else 'Malicious'\n",
    "    )\n",
    "    \n",
    "    # 2. ä½¿ç”¨ä¸simple.ipynbç›¸åŒçš„å¤šåˆ†ç±»æ ‡ç­¾æ˜ å°„\n",
    "    multi_class_mapping = {\n",
    "        'DoS Hulk': 'DoS',\n",
    "        'DoS GoldenEye': 'DoS', \n",
    "        'DoS slowloris': 'DoS',\n",
    "        'DoS Slowhttptest': 'DoS',\n",
    "        'FTP-Patator': 'Brute_Force',\n",
    "        'SSH-Patator': 'Brute_Force',\n",
    "        'Web Attack Brute Force': 'Web_Attack',\n",
    "        'Web Attack XSS': 'Web_Attack',\n",
    "        'Web Attack Sql Injection': 'Web_Attack',\n",
    "        'PortScan': 'PortScan',\n",
    "        'Bot': 'Bot',\n",
    "        'Infiltration': 'Rare_Attacks',\n",
    "        'Heartbleed': 'Rare_Attacks'\n",
    "    }\n",
    "    \n",
    "    df['Multi_Label'] = df[label_column].replace(multi_class_mapping)\n",
    "    \n",
    "    # ç§»é™¤æå°‘æ•°ç±»åˆ«ï¼ˆæ ·æœ¬å¤ªå°‘ï¼‰\n",
    "    df = df[~df['Multi_Label'].isin(['Rare_Attacks'])]\n",
    "    \n",
    "    print(\"æ ‡ç­¾åˆ†å¸ƒ:\")\n",
    "    print(df['Multi_Label'].value_counts())\n",
    "    \n",
    "    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾\n",
    "    feature_columns = [col for col in df.columns \n",
    "                      if col not in [label_column, 'Binary_Label', 'Multi_Label']]\n",
    "    X = df[feature_columns].copy()\n",
    "    \n",
    "    # ç¼–ç æ ‡ç­¾\n",
    "    le_binary = LabelEncoder()\n",
    "    y_binary = le_binary.fit_transform(df['Binary_Label'])\n",
    "    \n",
    "    le_multi = LabelEncoder()\n",
    "    y_multi = le_multi.fit_transform(df['Multi_Label'])\n",
    "    \n",
    "    print(f\"ç‰¹å¾ç»´åº¦: {X.shape[1]}\")\n",
    "    print(f\"å¤šåˆ†ç±»ç±»åˆ«: {list(le_multi.classes_)}\")\n",
    "    \n",
    "    return X, y_binary, y_multi, le_binary, le_multi\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®åŠ è½½\n",
    "X, y_binary, y_multi, le_binary, le_multi = load_and_preprocess_data()\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 5: ç¬¬ä¸€é˜¶æ®µ - è®­ç»ƒäºŒåˆ†ç±»é¢„è®­ç»ƒæ¨¡å‹\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒTransformerå¢å¼ºçš„äºŒåˆ†ç±»é¢„è®­ç»ƒæ¨¡å‹ ===\")\n",
    "\n",
    "# æ•°æ®åˆ†å‰²\n",
    "X_train, X_test, y_train_binary, y_test_binary = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "X_train, X_val, y_train_binary, y_val_binary = train_test_split(\n",
    "    X_train, y_train_binary, test_size=0.2, random_state=42, stratify=y_train_binary\n",
    ")\n",
    "\n",
    "# ç‰¹å¾æ ‡å‡†åŒ–\n",
    "scaler_binary = StandardScaler()\n",
    "X_train_scaled = scaler_binary.fit_transform(X_train)\n",
    "X_val_scaled = scaler_binary.transform(X_val)\n",
    "X_test_scaled = scaler_binary.transform(X_test)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å¤§å°: {len(X_train_scaled):,}\")\n",
    "\n",
    "# è®¡ç®—ç±»åˆ«æƒé‡\n",
    "class_weights_binary = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_train_binary), \n",
    "    y=y_train_binary\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "train_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_train_scaled).float(), \n",
    "                 torch.from_numpy(y_train_binary).long()),\n",
    "    batch_size=1024, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "val_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_val_scaled).float(), \n",
    "                 torch.from_numpy(y_val_binary).long()),\n",
    "    batch_size=1024, num_workers=2\n",
    ")\n",
    "\n",
    "# åˆ›å»ºTransformerå¢å¼ºçš„äºŒåˆ†ç±»æ¨¡å‹\n",
    "binary_model = TransformerEnhancedEnsembleModel(\n",
    "    input_dim=X.shape[1], \n",
    "    num_classes=2, \n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in binary_model.parameters()):,}\")\n",
    "\n",
    "# è®­ç»ƒé…ç½®\n",
    "binary_config = {\n",
    "    'model_name': 'TransformerBinary_Pretrain',\n",
    "    'num_epochs': 20,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 5,\n",
    "    'class_weights': class_weights_binary,\n",
    "    'minority_classes': [1],\n",
    "    'use_pretrained': False\n",
    "}\n",
    "\n",
    "# è®­ç»ƒTransformerå¢å¼ºçš„äºŒåˆ†ç±»æ¨¡å‹\n",
    "best_binary_path, best_binary_f1 = train_transformer_model(\n",
    "    binary_model, train_loader_binary, val_loader_binary, device, binary_config\n",
    ")\n",
    "\n",
    "print(f\"äºŒåˆ†ç±»é¢„è®­ç»ƒå®Œæˆï¼Œæœ€ä½³F1: {best_binary_f1:.4f}\")\n",
    "\n",
    "# æ¸…ç†å†…å­˜\n",
    "del train_loader_binary, val_loader_binary\n",
    "gc.collect()\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 6: ç¬¬äºŒé˜¶æ®µ - å‡†å¤‡å¤šåˆ†ç±»æ•°æ®å’Œåˆ†å±‚é‡‡æ ·\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== ç¬¬äºŒé˜¶æ®µï¼šå‡†å¤‡å¤šåˆ†ç±»æ•°æ®å’Œåˆ†å±‚é‡‡æ · ===\")\n",
    "\n",
    "# ä¿æŒåŸæœ‰é€»è¾‘ï¼šåªåœ¨æ¶æ„æµé‡ä¸­è¿›è¡Œå¤šåˆ†ç±»\n",
    "malicious_indices = (y_binary == 1)\n",
    "X_malicious = X[malicious_indices].copy()\n",
    "y_malicious_original = y_multi[malicious_indices].copy()\n",
    "\n",
    "print(f\"æ¶æ„æµé‡æ ·æœ¬æ•°: {len(X_malicious):,}\")\n",
    "\n",
    "# æ£€æŸ¥åŸå§‹æ ‡ç­¾çš„å”¯ä¸€å€¼\n",
    "unique_labels = np.unique(y_malicious_original)\n",
    "\n",
    "# é‡æ–°æ˜ å°„æ ‡ç­¾ï¼Œç¡®ä¿ä»0å¼€å§‹è¿ç»­\n",
    "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "reverse_mapping = {new_label: old_label for old_label, new_label in label_mapping.items()}\n",
    "\n",
    "y_malicious = np.array([label_mapping[label] for label in y_malicious_original])\n",
    "\n",
    "# åˆ›å»ºæ–°çš„æ ‡ç­¾ç¼–ç å™¨\n",
    "le_multi_subset = LabelEncoder()\n",
    "class_names_subset = [le_multi.classes_[reverse_mapping[i]] for i in range(len(unique_labels))]\n",
    "le_multi_subset.classes_ = np.array(class_names_subset)\n",
    "\n",
    "print(\"æ¶æ„æµé‡ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "multi_class_counts = Counter(y_malicious)\n",
    "for class_idx, count in sorted(multi_class_counts.items()):\n",
    "    class_name = class_names_subset[class_idx]\n",
    "    percentage = count / len(y_malicious) * 100\n",
    "    print(f\"  {class_name}: {count:,} æ ·æœ¬ ({percentage:.2f}%)\")\n",
    "\n",
    "# è¯†åˆ«å…³é”®å°‘æ•°ç±»åˆ«\n",
    "tier1_classes = []\n",
    "tier2_classes = []\n",
    "total_malicious = len(y_malicious)\n",
    "\n",
    "for class_idx, count in multi_class_counts.items():\n",
    "    ratio = count / total_malicious\n",
    "    if ratio < 0.05:\n",
    "        tier1_classes.append(class_idx)\n",
    "    elif ratio < 0.2:\n",
    "        tier2_classes.append(class_idx)\n",
    "\n",
    "# æ•°æ®åˆ†å‰²\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_malicious, y_malicious, test_size=0.2, random_state=42, stratify=y_malicious\n",
    ")\n",
    "X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(\n",
    "    X_train_m, y_train_m, test_size=0.2, random_state=42, stratify=y_train_m\n",
    ")\n",
    "\n",
    "print(f\"å¤šåˆ†ç±»è®­ç»ƒé›†: {len(X_train_m):,}\")\n",
    "\n",
    "# åº”ç”¨åˆ†å±‚SMOTEé‡‡æ ·\n",
    "max_count = max(Counter(y_train_m).values())\n",
    "sampling_ratios = {}\n",
    "\n",
    "for class_idx in range(len(class_names_subset)):\n",
    "    current_count = sum(y_train_m == class_idx)\n",
    "    if class_idx in tier1_classes:\n",
    "        target_count = max(current_count, int(max_count * 0.4))\n",
    "    elif class_idx in tier2_classes:\n",
    "        target_count = max(current_count, int(max_count * 0.25))\n",
    "    else:\n",
    "        target_count = current_count\n",
    "    \n",
    "    sampling_ratios[class_idx] = target_count\n",
    "\n",
    "# åº”ç”¨SMOTEé‡‡æ ·\n",
    "try:\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_ratios,\n",
    "        random_state=42,\n",
    "        k_neighbors=min(5, min(Counter(y_train_m).values()) - 1)\n",
    "    )\n",
    "    \n",
    "    X_train_m_resampled, y_train_m_resampled = smote.fit_resample(X_train_m, y_train_m)\n",
    "    print(f\"SMOTEé‡‡æ ·å®Œæˆ: {len(X_train_m):,} â†’ {len(X_train_m_resampled):,} æ ·æœ¬\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"SMOTEé‡‡æ ·å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨åŸå§‹æ•°æ®\")\n",
    "    X_train_m_resampled, y_train_m_resampled = X_train_m, y_train_m\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 7: ç¬¬ä¸‰é˜¶æ®µ - ç«¯åˆ°ç«¯å¾®è°ƒå¤šåˆ†ç±»æ¨¡å‹\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== ç¬¬ä¸‰é˜¶æ®µï¼šç«¯åˆ°ç«¯å¾®è°ƒå¤šåˆ†ç±»æ¨¡å‹ ===\")\n",
    "\n",
    "# ç‰¹å¾æ ‡å‡†åŒ–\n",
    "scaler_multi = StandardScaler()\n",
    "X_train_m_scaled = scaler_multi.fit_transform(X_train_m_resampled)\n",
    "X_val_m_scaled = scaler_multi.transform(X_val_m)\n",
    "X_test_m_scaled = scaler_multi.transform(X_test_m)\n",
    "\n",
    "# è®¡ç®—å¤šåˆ†ç±»çš„ç±»åˆ«æƒé‡\n",
    "unique_classes = np.arange(len(class_names_subset))\n",
    "class_weights_multi = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=unique_classes, \n",
    "    y=y_train_m_resampled\n",
    ")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨\n",
    "train_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_train_m_scaled).float(), \n",
    "                 torch.from_numpy(y_train_m_resampled).long()),\n",
    "    batch_size=512, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_val_m_scaled).float(), \n",
    "                 torch.from_numpy(y_val_m).long()),\n",
    "    batch_size=512, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test_m_scaled).float(), \n",
    "                 torch.from_numpy(y_test_m).long()),\n",
    "    batch_size=512, num_workers=0\n",
    ")\n",
    "\n",
    "# åˆ›å»ºå¤šåˆ†ç±»æ¨¡å‹\n",
    "num_classes = len(class_names_subset)\n",
    "\n",
    "multi_model = TransformerEnhancedEnsembleModel(\n",
    "    input_dim=X.shape[1], \n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.4,\n",
    "    use_pretrained=True\n",
    ").to(device)\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒçš„ç¼–ç å™¨æƒé‡\n",
    "multi_model.load_pretrained_encoder(best_binary_path)\n",
    "\n",
    "# å¾®è°ƒé…ç½®\n",
    "multi_config = {\n",
    "    'model_name': 'TransformerMultiClass_FineTuned',\n",
    "    'num_epochs': 25,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 8,\n",
    "    'class_weights': class_weights_multi,\n",
    "    'minority_classes': tier1_classes + tier2_classes,\n",
    "    'use_pretrained': True\n",
    "}\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒå¤šåˆ†ç±»æ¨¡å‹\n",
    "best_multi_path, best_multi_f1 = train_transformer_model(\n",
    "    multi_model, train_loader_multi, val_loader_multi, device, multi_config\n",
    ")\n",
    "\n",
    "print(f\"å¤šåˆ†ç±»å¾®è°ƒå®Œæˆï¼Œæœ€ä½³F1: {best_multi_f1:.4f}\")\n",
    "\n",
    "# ä¿å­˜æ ‡ç­¾æ˜ å°„ä¿¡æ¯\n",
    "label_mapping_info = {\n",
    "    'original_to_new': label_mapping,\n",
    "    'new_to_original': reverse_mapping,\n",
    "    'class_names_subset': class_names_subset,\n",
    "    'tier1_classes': tier1_classes,\n",
    "    'tier2_classes': tier2_classes\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open('/kaggle/working/label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping_info, f)\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 8: æ¨¡å‹è¯„ä¼°å’Œå¯¹æ¯”\n",
    "# =====================================================================\n",
    "\n",
    "def evaluate_transformer_model(model, model_path, test_loader, device, \n",
    "                               class_names, model_name):\n",
    "    \"\"\"è¯„ä¼°Transformeræ¨¡å‹æ€§èƒ½\"\"\"\n",
    "    print(f\"\\nè¯„ä¼° {model_name}\")\n",
    "    \n",
    "    # åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    y_true, y_pred, y_scores, uncertainties = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"æ¨¡å‹è¯„ä¼°\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "            \n",
    "            # è·å–é¢„æµ‹æ¦‚ç‡\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            y_scores.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # è·å–é¢„æµ‹æ ‡ç­¾\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "            uncertainties.extend(uncertainty.cpu().numpy())\n",
    "    \n",
    "    # è½¬æ¢ä¸ºnumpyæ•°ç»„\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    uncertainties = np.array(uncertainties)\n",
    "    \n",
    "    # ç¡®ä¿æ ‡ç­¾èŒƒå›´æ­£ç¡®\n",
    "    max_label = max(y_true.max(), y_pred.max())\n",
    "    if max_label >= len(class_names):\n",
    "        y_true = np.clip(y_true, 0, len(class_names)-1)\n",
    "        y_pred = np.clip(y_pred, 0, len(class_names)-1)\n",
    "    \n",
    "    # è·å–å®é™…å­˜åœ¨çš„æ ‡ç­¾\n",
    "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    actual_class_names = [class_names[i] for i in unique_labels if i < len(class_names)]\n",
    "    \n",
    "    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š\n",
    "    try:\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            labels=unique_labels,\n",
    "            target_names=actual_class_names,\n",
    "            digits=4, \n",
    "            zero_division=0\n",
    "        )\n",
    "        print(f\"\\n{model_name} åˆ†ç±»æŠ¥å‘Š:\")\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"ç”Ÿæˆåˆ†ç±»æŠ¥å‘Šæ—¶å‡ºé”™: {e}\")\n",
    "    \n",
    "    # è®¡ç®—æ•´ä½“æŒ‡æ ‡\n",
    "    try:\n",
    "        macro_f1 = f1_score(y_true, y_pred, labels=unique_labels, average='macro')\n",
    "        weighted_f1 = f1_score(y_true, y_pred, labels=unique_labels, average='weighted')\n",
    "    except:\n",
    "        macro_f1 = 0.0\n",
    "        weighted_f1 = 0.0\n",
    "    \n",
    "    overall_accuracy = np.mean(y_true == y_pred)\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    \n",
    "    print(f\"\\n{model_name} æ•´ä½“æ€§èƒ½:\")\n",
    "    print(f\"  å‡†ç¡®ç‡: {overall_accuracy:.4f}\")\n",
    "    print(f\"  å®å¹³å‡F1: {macro_f1:.4f}\")\n",
    "    print(f\"  åŠ æƒå¹³å‡F1: {weighted_f1:.4f}\")\n",
    "    print(f\"  å¹³å‡ä¸ç¡®å®šæ€§: {avg_uncertainty:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'accuracy': overall_accuracy,\n",
    "        'avg_uncertainty': avg_uncertainty,\n",
    "        'class_names': actual_class_names,\n",
    "        'unique_labels': unique_labels\n",
    "    }\n",
    "\n",
    "print(\"\\n=== å±‚æ¬¡åŒ–Transformerå¢å¼ºæ¨¡å‹è¯„ä¼° ===\")\n",
    "\n",
    "# è¯„ä¼°äºŒåˆ†ç±»æ¨¡å‹\n",
    "test_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test_scaled).float(), \n",
    "                 torch.from_numpy(y_test_binary).long()),\n",
    "    batch_size=1024, num_workers=0\n",
    ")\n",
    "\n",
    "binary_class_names = le_binary.classes_.tolist()\n",
    "binary_results = evaluate_transformer_model(\n",
    "    binary_model, best_binary_path, test_loader_binary, device,\n",
    "    binary_class_names, \"Transformerå¢å¼ºäºŒåˆ†ç±»æ¨¡å‹\"\n",
    ")\n",
    "\n",
    "# è¯„ä¼°å¤šåˆ†ç±»æ¨¡å‹\n",
    "multi_results = evaluate_transformer_model(\n",
    "    multi_model, best_multi_path, test_loader_multi, device,\n",
    "    class_names_subset, \"Transformerå¢å¼ºå¤šåˆ†ç±»æ¨¡å‹\"\n",
    ")\n",
    "\n",
    "# æ€§èƒ½æ€»ç»“\n",
    "print(\"\\n=== å±‚æ¬¡åŒ–Transformerå¢å¼ºæ£€æµ‹ç³»ç»Ÿæ•ˆæœæ€»ç»“ ===\")\n",
    "\n",
    "print(f\"\\näºŒåˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\n",
    "print(f\"   å‡†ç¡®ç‡: {binary_results['accuracy']:.4f}\")\n",
    "print(f\"   å®å¹³å‡F1: {binary_results['macro_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nå¤šåˆ†ç±»æ¨¡å‹æ€§èƒ½:\")\n",
    "print(f\"   å‡†ç¡®ç‡: {multi_results['accuracy']:.4f}\")\n",
    "print(f\"   å®å¹³å‡F1: {multi_results['macro_f1']:.4f}\")\n",
    "\n",
    "# ä¿å­˜å®Œæ•´ç»“æœ\n",
    "results_summary = {\n",
    "    'transformer_binary_model': {\n",
    "        'path': best_binary_path,\n",
    "        'accuracy': binary_results['accuracy'],\n",
    "        'macro_f1': binary_results['macro_f1']\n",
    "    },\n",
    "    'transformer_multi_model': {\n",
    "        'path': best_multi_path,\n",
    "        'accuracy': multi_results['accuracy'],\n",
    "        'macro_f1': multi_results['macro_f1']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "try:\n",
    "    with open('/kaggle/working/hierarchical_transformer_results.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    print(\"\\nç»“æœå·²ä¿å­˜åˆ°: /kaggle/working/hierarchical_transformer_results.json\")\n",
    "except Exception as e:\n",
    "    print(f\"ä¿å­˜ç»“æœæ—¶å‡ºé”™: {e}\")\n",
    "\n",
    "print(\"\\nå±‚æ¬¡åŒ–Transformerå¢å¼ºçš„ç½‘ç»œå…¥ä¾µæ£€æµ‹ç³»ç»Ÿå®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cfd534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:59:06.702304Z",
     "iopub.status.busy": "2025-08-24T09:59:06.701892Z",
     "iopub.status.idle": "2025-08-24T09:59:06.774746Z",
     "shell.execute_reply": "2025-08-24T09:59:06.773791Z"
    },
    "papermill": {
     "duration": 1.771145,
     "end_time": "2025-08-24T09:59:06.776097",
     "exception": false,
     "start_time": "2025-08-24T09:59:05.004952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ‰€æœ‰å¿…è¦å˜é‡æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹æ‰“åŒ…...\n",
      "\n",
      "ğŸ“‹ éªŒè¯æ¨¡å‹åŒ…...\n",
      "  âœ… scaler.pkl\n",
      "  âœ… label_encoder.pkl\n",
      "  âœ… feature_selector.pkl\n",
      "  âœ… selected_features.json\n",
      "  âœ… model.pth\n",
      "  âœ… model_info.json\n",
      "  âœ… model_loader.py\n",
      "\n",
      "ğŸ‰ æ¨¡å‹åŒ…å·²ç”Ÿæˆäº: /kaggle/working/model_package\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# æ¨¡å‹æ‰“åŒ…å’Œåç«¯é›†æˆå‡†å¤‡\n",
    "# =====================================================================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å˜é‡éƒ½å­˜åœ¨\n",
    "try:\n",
    "    assert 'X' in locals(), \"X å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'scaler_multi' in locals(), \"scaler_multi å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'le_binary' in locals(), \"le_binary å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'le_multi_subset' in locals(), \"le_multi_subset å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'class_names_subset' in locals(), \"class_names_subset å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'label_mapping_info' in locals(), \"label_mapping_info å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'tier1_classes' in locals(), \"tier1_classes å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'tier2_classes' in locals(), \"tier2_classes å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'best_binary_path' in locals(), \"best_binary_path å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'best_multi_path' in locals(), \"best_multi_path å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'binary_results' in locals(), \"binary_results å˜é‡ä¸å­˜åœ¨\"\n",
    "    assert 'multi_results' in locals(), \"multi_results å˜é‡ä¸å­˜åœ¨\"\n",
    "    print(\"âœ… æ‰€æœ‰å¿…è¦å˜é‡æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹æ‰“åŒ…...\")\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ å˜é‡æ£€æŸ¥å¤±è´¥: {e}\")\n",
    "    print(\"è¯·ç¡®ä¿åœ¨å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ä¹‹åå†è¿è¡Œæ­¤æ‰“åŒ…ä»£ç ã€‚\")\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹åŒ…ç›®å½•\n",
    "model_package_dir = \"/kaggle/working/model_package\"\n",
    "os.makedirs(model_package_dir, exist_ok=True)\n",
    "\n",
    "# 1. ä¿å­˜ç‰¹å¾ç¼©æ”¾å™¨ (scaler.pkl)\n",
    "scaler_path = os.path.join(model_package_dir, \"scaler.pkl\")\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler_multi, f)\n",
    "\n",
    "# 2. ä¿å­˜æ ‡ç­¾ç¼–ç å™¨ (label_encoder.pkl)\n",
    "label_encoders = {\n",
    "    'binary_encoder': le_binary,\n",
    "    'multi_encoder': le_multi_subset,\n",
    "    'binary_classes': le_binary.classes_.tolist(),\n",
    "    'multi_classes': class_names_subset,\n",
    "    'label_mapping': label_mapping_info\n",
    "}\n",
    "label_encoder_path = os.path.join(model_package_dir, \"label_encoder.pkl\")\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# 3. ä¿å­˜ç‰¹å¾é€‰æ‹©å™¨ (feature_selector.pkl)\n",
    "feature_selector = {\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'selected_features': X.columns.tolist(),\n",
    "    'feature_count': len(X.columns),\n",
    "    'selection_method': 'all_features'\n",
    "}\n",
    "feature_selector_path = os.path.join(model_package_dir, \"feature_selector.pkl\")\n",
    "with open(feature_selector_path, 'wb') as f:\n",
    "    pickle.dump(feature_selector, f)\n",
    "\n",
    "# 4. ä¿å­˜é€‰ä¸­ç‰¹å¾åˆ—è¡¨ (selected_features.json)\n",
    "selected_features = {\n",
    "    'features': X.columns.tolist(),\n",
    "    'count': len(X.columns)\n",
    "}\n",
    "selected_features_path = os.path.join(model_package_dir, \"selected_features.json\")\n",
    "with open(selected_features_path, 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2)\n",
    "\n",
    "# 5. ä¿å­˜æ¨¡å‹æƒé‡ (model.pth)\n",
    "model_state = {\n",
    "    'binary_model_state': torch.load(best_binary_path, map_location='cpu'),\n",
    "    'multi_model_state': torch.load(best_multi_path, map_location='cpu')\n",
    "}\n",
    "model_path = os.path.join(model_package_dir, \"model.pth\")\n",
    "torch.save(model_state, model_path)\n",
    "\n",
    "# 6. ä¿å­˜æ¨¡å‹å…ƒä¿¡æ¯ (model_info.json)\n",
    "model_info = {\n",
    "    'model_name': 'HierarchicalTransformerIDS',\n",
    "    'model_version': '1.0.0',\n",
    "    'architecture': {\n",
    "        'input_features': len(X.columns),\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    },\n",
    "    'classes': {\n",
    "        'binary': le_binary.classes_.tolist(),\n",
    "        'multi': class_names_subset\n",
    "    },\n",
    "    'performance': {\n",
    "        'binary_stage': {\n",
    "            'accuracy': float(binary_results['accuracy']),\n",
    "            'macro_f1': float(binary_results['macro_f1'])\n",
    "        },\n",
    "        'multi_stage': {\n",
    "            'accuracy': float(multi_results['accuracy']),\n",
    "            'macro_f1': float(multi_results['macro_f1'])\n",
    "        }\n",
    "    },\n",
    "    'features': {\n",
    "        'total_features': len(X.columns),\n",
    "        'feature_names': X.columns.tolist()\n",
    "    }\n",
    "}\n",
    "model_info_path = os.path.join(model_package_dir, \"model_info.json\")\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "# 7. åˆ›å»ºæ¨¡å‹åŠ è½½å™¨ç±» (model_loader.py)\n",
    "model_loader_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 4), nn.ReLU(), nn.Dropout(0.1),\n",
    "                nn.Linear(input_dim // 4, input_dim), nn.Sigmoid()\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        self.temporal_attention = nn.MultiheadAttention(input_dim, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 8), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(input_dim // 8, input_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        class_features = [self.class_attention[i](x) * x for i in range(self.num_classes)]\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        x_temporal = x.unsqueeze(1)\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)\n",
    "        spatial_weights = self.spatial_attention(x)\n",
    "        spatial_attended = spatial_weights * x\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)\n",
    "        fused_features = (weights[0] * class_attended_mean + \n",
    "                         weights[1] * temporal_attended + \n",
    "                         weights[2] * spatial_attended)\n",
    "        return class_attended, fused_features\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate / 2)\n",
    "        )\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.feature_dim, nhead=8, dim_feedforward=512,\n",
    "            dropout=0.1, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "                nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        self.global_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(),\n",
    "            nn.Dropout(dropout_rate), nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.fusion_network = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 2), nn.Softmax(dim=-1))\n",
    "        self.uncertainty_head = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, return_uncertainty=False):\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input).squeeze(1)\n",
    "        enhanced_features = F.layer_norm(shared_features + transformer_features, normalized_shape=[self.feature_dim])\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        class_specific_outputs = [self.class_specific_heads[i](class_attended_features[:, i, :]) for i in range(self.num_classes)]\n",
    "        class_specific_logits = torch.cat(class_specific_outputs, dim=1)\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = (fusion_weights[:, 0:1] * class_specific_logits + fusion_weights[:, 1:2] * global_logits)\n",
    "        if return_uncertainty:\n",
    "            return final_logits, self.uncertainty_head(enhanced_features)\n",
    "        return final_logits\n",
    "\n",
    "class HierarchicalTransformerIDSLoader:\n",
    "    def __init__(self, model_package_path: str):\n",
    "        self.model_package_path = model_package_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_components()\n",
    "    \n",
    "    def _load_components(self):\n",
    "        with open(os.path.join(self.model_package_path, \"model_info.json\"), 'r') as f:\n",
    "            self.model_info = json.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"scaler.pkl\"), 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "        model_checkpoint = torch.load(os.path.join(self.model_package_path, \"model.pth\"), map_location=self.device)\n",
    "        \n",
    "        arch_info = self.model_info['architecture']\n",
    "        self.binary_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['binary_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        self.multi_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['multi_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.binary_model.load_state_dict(model_checkpoint['binary_model_state'])\n",
    "        self.multi_model.load_state_dict(model_checkpoint['multi_model_state'])\n",
    "        \n",
    "        self.binary_model.eval()\n",
    "        self.multi_model.eval()\n",
    "    \n",
    "    def preprocess(self, data: np.ndarray) -> torch.Tensor:\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return torch.from_numpy(scaled_data).float().to(self.device)\n",
    "    \n",
    "    def predict(self, data: np.ndarray):\n",
    "        with torch.no_grad():\n",
    "            processed_data = self.preprocess(data)\n",
    "            binary_logits, binary_uncertainty = self.binary_model(processed_data, return_uncertainty=True)\n",
    "            binary_probs = F.softmax(binary_logits, dim=1)\n",
    "            binary_preds = torch.argmax(binary_probs, dim=1)\n",
    "            \n",
    "            multi_probs = torch.zeros(len(data), len(self.model_info['classes']['multi']))\n",
    "            multi_uncertainty = torch.zeros(len(data), 1)\n",
    "            \n",
    "            malicious_mask = binary_preds == 1\n",
    "            if malicious_mask.sum() > 0:\n",
    "                malicious_data = processed_data[malicious_mask]\n",
    "                multi_logits, multi_unc = self.multi_model(malicious_data, return_uncertainty=True)\n",
    "                multi_probs[malicious_mask] = F.softmax(multi_logits, dim=1)\n",
    "                multi_uncertainty[malicious_mask] = multi_unc\n",
    "            \n",
    "            return {\n",
    "                'binary': {\n",
    "                    'predictions': binary_preds.cpu().numpy(),\n",
    "                    'probabilities': binary_probs.cpu().numpy(),\n",
    "                    'uncertainty': binary_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['binary']\n",
    "                },\n",
    "                'multi': {\n",
    "                    'probabilities': multi_probs.cpu().numpy(),\n",
    "                    'uncertainty': multi_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['multi']\n",
    "                }\n",
    "            }\n",
    "'''\n",
    "model_loader_path = os.path.join(model_package_dir, \"model_loader.py\")\n",
    "with open(model_loader_path, 'w') as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "# 8. éªŒè¯æ¨¡å‹åŒ…å®Œæ•´æ€§\n",
    "print(\"\\nğŸ“‹ éªŒè¯æ¨¡å‹åŒ…...\")\n",
    "required_files = [\n",
    "    \"scaler.pkl\", \"label_encoder.pkl\", \"feature_selector.pkl\",\n",
    "    \"selected_features.json\", \"model.pth\", \"model_info.json\", \"model_loader.py\"\n",
    "]\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(model_package_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  âœ… {file}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file} - ç¼ºå¤±\")\n",
    "\n",
    "print(f\"\\nğŸ‰ æ¨¡å‹åŒ…å·²ç”Ÿæˆäº: {model_package_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9c3f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:59:10.192791Z",
     "iopub.status.busy": "2025-08-24T09:59:10.192491Z",
     "iopub.status.idle": "2025-08-24T09:59:10.279254Z",
     "shell.execute_reply": "2025-08-24T09:59:10.278307Z"
    },
    "papermill": {
     "duration": 1.764043,
     "end_time": "2025-08-24T09:59:10.280604",
     "exception": false,
     "start_time": "2025-08-24T09:59:08.516561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ å¼€å§‹æ¨¡å‹æ‰“åŒ…...\n",
      "âœ… å˜é‡æ£€æŸ¥é€šè¿‡\n",
      "ğŸ“¦ ä¿å­˜æ¨¡å‹ç»„ä»¶...\n",
      "ğŸ“‹ éªŒè¯æ¨¡å‹åŒ…...\n",
      "  âœ… scaler.pkl\n",
      "  âœ… label_encoder.pkl\n",
      "  âœ… feature_selector.pkl\n",
      "  âœ… selected_features.json\n",
      "  âœ… model.pth\n",
      "  âœ… model_info.json\n",
      "  âœ… model_loader.py\n",
      "  âœ… test_model.py\n",
      "\n",
      "ğŸ‰ æ¨¡å‹åŒ…ç”Ÿæˆå®Œæˆ!\n",
      "ğŸ“ ä½ç½®: /kaggle/working/model_package\n",
      "ğŸ“Š å¤§å°: 14.4MB\n",
      "ğŸ§ª è¿è¡Œæµ‹è¯•: cd /kaggle/working/model_package && python test_model.py\n",
      "\n",
      "ğŸš€ é›†æˆç¤ºä¾‹:\n",
      "from model_loader import HierarchicalTransformerIDSLoader\n",
      "loader = HierarchicalTransformerIDSLoader('/path/to/model_package')\n",
      "results = loader.predict(data)\n",
      "\n",
      "âœ¨ æ‰“åŒ…å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# æ¨¡å‹æ‰“åŒ…å’Œåç«¯é›†æˆå‡†å¤‡ - ç®€åŒ–ç‰ˆ\n",
    "# =====================================================================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ğŸ”§ å¼€å§‹æ¨¡å‹æ‰“åŒ…...\")\n",
    "\n",
    "# å˜é‡æ£€æŸ¥\n",
    "required_vars = ['X', 'scaler_multi', 'le_binary', 'le_multi_subset', 'class_names_subset', \n",
    "                'label_mapping_info', 'tier1_classes', 'tier2_classes', 'best_binary_path', \n",
    "                'best_multi_path', 'binary_results', 'multi_results']\n",
    "\n",
    "try:\n",
    "    for var in required_vars:\n",
    "        assert var in locals(), f\"{var} å˜é‡ä¸å­˜åœ¨\"\n",
    "    print(\"âœ… å˜é‡æ£€æŸ¥é€šè¿‡\")\n",
    "except AssertionError as e:\n",
    "    print(f\"âŒ {e}\")\n",
    "    print(\"è¯·ç¡®ä¿åœ¨å®Œæ•´è®­ç»ƒæµç¨‹åè¿è¡Œæ­¤ä»£ç \")\n",
    "    exit()\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹åŒ…ç›®å½•\n",
    "model_package_dir = \"/kaggle/working/model_package\"\n",
    "os.makedirs(model_package_dir, exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“¦ ä¿å­˜æ¨¡å‹ç»„ä»¶...\")\n",
    "\n",
    "# 1. ç‰¹å¾ç¼©æ”¾å™¨\n",
    "with open(os.path.join(model_package_dir, \"scaler.pkl\"), 'wb') as f:\n",
    "    pickle.dump(scaler_multi, f)\n",
    "\n",
    "# 2. æ ‡ç­¾ç¼–ç å™¨\n",
    "label_encoders = {\n",
    "    'binary_encoder': le_binary,\n",
    "    'multi_encoder': le_multi_subset,\n",
    "    'binary_classes': le_binary.classes_.tolist(),\n",
    "    'multi_classes': class_names_subset,\n",
    "    'label_mapping': label_mapping_info\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"label_encoder.pkl\"), 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# 3. ç‰¹å¾é€‰æ‹©å™¨\n",
    "feature_selector = {\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'selected_features': X.columns.tolist(),\n",
    "    'feature_count': len(X.columns),\n",
    "    'selection_method': 'all_features'\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"feature_selector.pkl\"), 'wb') as f:\n",
    "    pickle.dump(feature_selector, f)\n",
    "\n",
    "# 4. ç‰¹å¾åˆ—è¡¨\n",
    "selected_features = {\n",
    "    'features': X.columns.tolist(),\n",
    "    'count': len(X.columns),\n",
    "    'creation_date': datetime.now().isoformat()\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"selected_features.json\"), 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2)\n",
    "\n",
    "# 5. æ¨¡å‹æƒé‡\n",
    "model_state = {\n",
    "    'binary_model_state': torch.load(best_binary_path, map_location='cpu'),\n",
    "    'multi_model_state': torch.load(best_multi_path, map_location='cpu'),\n",
    "    'model_architecture': {\n",
    "        'input_dim': X.shape[1],\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    }\n",
    "}\n",
    "torch.save(model_state, os.path.join(model_package_dir, \"model.pth\"))\n",
    "\n",
    "# 6. æ¨¡å‹ä¿¡æ¯\n",
    "model_info = {\n",
    "    'model_name': 'HierarchicalTransformerIDS',\n",
    "    'model_version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'architecture': {\n",
    "        'input_features': len(X.columns),\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    },\n",
    "    'classes': {\n",
    "        'binary': le_binary.classes_.tolist(),\n",
    "        'multi': class_names_subset,\n",
    "        'tier1_critical': [class_names_subset[i] for i in tier1_classes],\n",
    "        'tier2_minority': [class_names_subset[i] for i in tier2_classes]\n",
    "    },\n",
    "    'performance': {\n",
    "        'binary_stage': {\n",
    "            'accuracy': float(binary_results['accuracy']),\n",
    "            'macro_f1': float(binary_results['macro_f1']),\n",
    "            'weighted_f1': float(binary_results.get('weighted_f1', 0))\n",
    "        },\n",
    "        'multi_stage': {\n",
    "            'accuracy': float(multi_results['accuracy']),\n",
    "            'macro_f1': float(multi_results['macro_f1']),\n",
    "            'weighted_f1': float(multi_results.get('weighted_f1', 0))\n",
    "        }\n",
    "    },\n",
    "    'features': {\n",
    "        'total_features': len(X.columns),\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'preprocessing': 'StandardScaler + Outlier Clipping'\n",
    "    },\n",
    "    'deployment': {\n",
    "        'input_format': 'numpy array of shape (batch_size, n_features)',\n",
    "        'requires_preprocessing': True,\n",
    "        'batch_inference': True\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"model_info.json\"), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "# 7. æ¨¡å‹åŠ è½½å™¨\n",
    "model_loader_code = '''import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(input_dim, input_dim//4), nn.ReLU(), nn.Dropout(0.1),\n",
    "                         nn.Linear(input_dim//4, input_dim), nn.Sigmoid())\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "        self.temporal_attention = nn.MultiheadAttention(input_dim, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.spatial_attention = nn.Sequential(nn.Linear(input_dim, input_dim//8), nn.ReLU(), nn.Dropout(0.1),\n",
    "                                             nn.Linear(input_dim//8, input_dim), nn.Sigmoid())\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ç±»åˆ«æ³¨æ„åŠ›\n",
    "        class_features = [self.class_attention[i](x) * x for i in range(self.num_classes)]\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        # æ—¶é—´æ³¨æ„åŠ›\n",
    "        x_temporal = x.unsqueeze(1)\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)\n",
    "        # ç©ºé—´æ³¨æ„åŠ›\n",
    "        spatial_attended = self.spatial_attention(x) * x\n",
    "        # èåˆ\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)\n",
    "        fused = weights[0] * class_attended_mean + weights[1] * temporal_attended + weights[2] * spatial_attended\n",
    "        return class_attended, fused\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # ç¼–ç å™¨\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate/2)\n",
    "        )\n",
    "        # Transformer\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.feature_dim, nhead=8, dim_feedforward=512,\n",
    "                                                 dropout=0.1, batch_first=True, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        # æ³¨æ„åŠ›æœºåˆ¶\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        # åˆ†ç±»å¤´\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "                         nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "        self.global_classifier = nn.Sequential(nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(),\n",
    "                                             nn.Dropout(dropout_rate), nn.Linear(128, num_classes))\n",
    "        self.fusion_network = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 2), nn.Softmax(dim=-1))\n",
    "        self.uncertainty_head = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, return_uncertainty=False):\n",
    "        # ç‰¹å¾æå–\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        # Transformerå¤„ç†\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input).squeeze(1)\n",
    "        enhanced_features = F.layer_norm(shared_features + transformer_features, normalized_shape=[self.feature_dim])\n",
    "        # å¤šå°ºåº¦æ³¨æ„åŠ›\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        # åˆ†ç±»\n",
    "        class_outputs = [self.class_specific_heads[i](class_attended_features[:, i, :]) for i in range(self.num_classes)]\n",
    "        class_logits = torch.cat(class_outputs, dim=1)\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = fusion_weights[:, 0:1] * class_logits + fusion_weights[:, 1:2] * global_logits\n",
    "        \n",
    "        if return_uncertainty:\n",
    "            return final_logits, self.uncertainty_head(enhanced_features)\n",
    "        return final_logits\n",
    "\n",
    "class HierarchicalTransformerIDSLoader:\n",
    "    def __init__(self, model_package_path: str):\n",
    "        self.model_package_path = model_package_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_components()\n",
    "    \n",
    "    def _load_components(self):\n",
    "        # åŠ è½½é…ç½®å’Œç»„ä»¶\n",
    "        with open(os.path.join(self.model_package_path, \"model_info.json\"), 'r') as f:\n",
    "            self.model_info = json.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"scaler.pkl\"), 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "        \n",
    "        # åŠ è½½æ¨¡å‹\n",
    "        model_checkpoint = torch.load(os.path.join(self.model_package_path, \"model.pth\"), map_location=self.device)\n",
    "        arch_info = self.model_info['architecture']\n",
    "        \n",
    "        self.binary_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['binary_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']).to(self.device)\n",
    "        self.multi_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['multi_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']).to(self.device)\n",
    "        \n",
    "        self.binary_model.load_state_dict(model_checkpoint['binary_model_state'])\n",
    "        self.multi_model.load_state_dict(model_checkpoint['multi_model_state'])\n",
    "        self.binary_model.eval()\n",
    "        self.multi_model.eval()\n",
    "    \n",
    "    def preprocess(self, data: np.ndarray) -> torch.Tensor:\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return torch.from_numpy(scaled_data).float().to(self.device)\n",
    "    \n",
    "    def predict(self, data: np.ndarray):\n",
    "        with torch.no_grad():\n",
    "            processed_data = self.preprocess(data)\n",
    "            \n",
    "            # äºŒåˆ†ç±»é¢„æµ‹\n",
    "            binary_logits, binary_uncertainty = self.binary_model(processed_data, return_uncertainty=True)\n",
    "            binary_probs = F.softmax(binary_logits, dim=1)\n",
    "            binary_preds = torch.argmax(binary_probs, dim=1)\n",
    "            \n",
    "            # å¤šåˆ†ç±»é¢„æµ‹ï¼ˆä»…æ¶æ„æµé‡ï¼‰\n",
    "            multi_probs = torch.zeros(len(data), len(self.model_info['classes']['multi']))\n",
    "            multi_uncertainty = torch.zeros(len(data), 1)\n",
    "            \n",
    "            malicious_mask = binary_preds == 1\n",
    "            if malicious_mask.sum() > 0:\n",
    "                malicious_data = processed_data[malicious_mask]\n",
    "                multi_logits, multi_unc = self.multi_model(malicious_data, return_uncertainty=True)\n",
    "                multi_probs[malicious_mask] = F.softmax(multi_logits, dim=1)\n",
    "                multi_uncertainty[malicious_mask] = multi_unc\n",
    "            \n",
    "            return {\n",
    "                'binary': {\n",
    "                    'predictions': binary_preds.cpu().numpy(),\n",
    "                    'probabilities': binary_probs.cpu().numpy(),\n",
    "                    'uncertainty': binary_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['binary']\n",
    "                },\n",
    "                'multi': {\n",
    "                    'probabilities': multi_probs.cpu().numpy(),\n",
    "                    'uncertainty': multi_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['multi']\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'model_name': self.model_info['model_name'],\n",
    "                    'num_samples': len(data)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        return self.model_info\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "# loader = HierarchicalTransformerIDSLoader(\"/path/to/model_package\")\n",
    "# results = loader.predict(network_traffic_data)\n",
    "'''\n",
    "\n",
    "with open(os.path.join(model_package_dir, \"model_loader.py\"), 'w') as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "# 8. åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬\n",
    "test_script = f'''import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "from model_loader import HierarchicalTransformerIDSLoader\n",
    "import numpy as np\n",
    "\n",
    "def test_model():\n",
    "    try:\n",
    "        print(\"ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½...\")\n",
    "        loader = HierarchicalTransformerIDSLoader(\".\")\n",
    "        print(\"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "        \n",
    "        print(\"ğŸ”„ æµ‹è¯•é¢„æµ‹...\")\n",
    "        sample_data = np.random.randn(3, {len(X.columns)})\n",
    "        results = loader.predict(sample_data)\n",
    "        print(\"âœ… é¢„æµ‹æˆåŠŸ\")\n",
    "        print(f\"äºŒåˆ†ç±»å½¢çŠ¶: {{results['binary']['probabilities'].shape}}\")\n",
    "        print(f\"å¤šåˆ†ç±»å½¢çŠ¶: {{results['multi']['probabilities'].shape}}\")\n",
    "        \n",
    "        model_info = loader.get_model_info()\n",
    "        print(f\"\\\\nğŸ“‹ æ¨¡å‹: {{model_info['model_name']}} v{{model_info['model_version']}}\")\n",
    "        print(f\"æ€§èƒ½: äºŒåˆ†ç±»F1={model_info['performance']['binary_stage']['macro_f1']:.3f}, \"\n",
    "              f\"å¤šåˆ†ç±»F1={model_info['performance']['multi_stage']['macro_f1']:.3f}\")\n",
    "        print(\"\\\\nğŸ‰ æµ‹è¯•é€šè¿‡ï¼\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æµ‹è¯•å¤±è´¥: {{e}}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_model()\n",
    "    sys.exit(0 if success else 1)\n",
    "'''\n",
    "\n",
    "with open(os.path.join(model_package_dir, \"test_model.py\"), 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# 9. éªŒè¯å®Œæ•´æ€§\n",
    "print(\"ğŸ“‹ éªŒè¯æ¨¡å‹åŒ…...\")\n",
    "required_files = [\"scaler.pkl\", \"label_encoder.pkl\", \"feature_selector.pkl\", \n",
    "                  \"selected_features.json\", \"model.pth\", \"model_info.json\", \n",
    "                  \"model_loader.py\", \"test_model.py\"]\n",
    "\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(os.path.join(model_package_dir, file)):\n",
    "        print(f\"  âœ… {file}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {file} - ç¼ºå¤±\")\n",
    "        all_good = False\n",
    "\n",
    "# 10. æ€»ç»“\n",
    "if all_good:\n",
    "    total_size = sum(os.path.getsize(os.path.join(model_package_dir, f)) \n",
    "                    for f in os.listdir(model_package_dir)) / (1024*1024)\n",
    "    print(f\"\\nğŸ‰ æ¨¡å‹åŒ…ç”Ÿæˆå®Œæˆ!\")\n",
    "    print(f\"ğŸ“ ä½ç½®: {model_package_dir}\")\n",
    "    print(f\"ğŸ“Š å¤§å°: {total_size:.1f}MB\")\n",
    "    print(f\"ğŸ§ª è¿è¡Œæµ‹è¯•: cd {model_package_dir} && python test_model.py\")\n",
    "    print(f\"\\nğŸš€ é›†æˆç¤ºä¾‹:\")\n",
    "    print(f\"from model_loader import HierarchicalTransformerIDSLoader\")\n",
    "    print(f\"loader = HierarchicalTransformerIDSLoader('/path/to/model_package')\")\n",
    "    print(f\"results = loader.predict(data)\")\n",
    "else:\n",
    "    print(\"\\nâŒ æ¨¡å‹åŒ…ç”Ÿæˆä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯\")\n",
    "\n",
    "print(\"\\nâœ¨ æ‰“åŒ…å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 897.24506,
   "end_time": "2025-08-24T09:59:14.629044",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-24T09:44:17.383984",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
