{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978bdb4e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-24T09:44:21.519594Z",
     "iopub.status.busy": "2025-08-24T09:44:21.519322Z",
     "iopub.status.idle": "2025-08-24T09:59:03.276137Z",
     "shell.execute_reply": "2025-08-24T09:59:03.275020Z"
    },
    "papermill": {
     "duration": 881.763919,
     "end_time": "2025-08-24T09:59:03.277432",
     "exception": false,
     "start_time": "2025-08-24T09:44:21.513513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境初始化完成，使用设备: cuda\n",
      "数据加载和预处理中...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据形状: (2313810, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "移除了 84,674 条重复记录\n",
      "标签分布:\n",
      "Multi_Label\n",
      "Benign         1892659\n",
      "DoS             193730\n",
      "DDoS            128014\n",
      "Brute_Force       9150\n",
      "Web_Attack        2143\n",
      "PortScan          1956\n",
      "Bot               1437\n",
      "Name: count, dtype: int64\n",
      "特征维度: 77\n",
      "多分类类别: ['Benign', 'Bot', 'Brute_Force', 'DDoS', 'DoS', 'PortScan', 'Web_Attack']\n",
      "\n",
      "=== 第一阶段：训练Transformer增强的二分类预训练模型 ===\n",
      "训练集大小: 1,426,616\n",
      "模型参数量: 1,722,730\n",
      "开始训练 TransformerBinary_Pretrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1394/1394 [00:30<00:00, 45.21it/s, Loss=0.0040]\n",
      "Epoch 2/20: 100%|██████████| 1394/1394 [00:27<00:00, 51.62it/s, Loss=0.0098]\n",
      "Epoch 3/20: 100%|██████████| 1394/1394 [00:27<00:00, 50.36it/s, Loss=0.0027]\n",
      "Epoch 4/20: 100%|██████████| 1394/1394 [00:26<00:00, 51.88it/s, Loss=0.0030]\n",
      "Epoch 5/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.33it/s, Loss=0.0024]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 训练F1: 0.9831 | 验证F1: 0.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.17it/s, Loss=0.0216]\n",
      "Epoch 7/20: 100%|██████████| 1394/1394 [00:26<00:00, 51.92it/s, Loss=0.0047]\n",
      "Epoch 8/20: 100%|██████████| 1394/1394 [00:27<00:00, 51.30it/s, Loss=0.0225]\n",
      "Epoch 9/20: 100%|██████████| 1394/1394 [00:27<00:00, 50.50it/s, Loss=0.0030]\n",
      "Epoch 10/20: 100%|██████████| 1394/1394 [00:27<00:00, 51.54it/s, Loss=0.0015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 训练F1: 0.9862 | 验证F1: 0.9898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1394/1394 [00:27<00:00, 51.47it/s, Loss=0.0015]\n",
      "Epoch 12/20: 100%|██████████| 1394/1394 [00:27<00:00, 51.35it/s, Loss=0.0091]\n",
      "Epoch 13/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.07it/s, Loss=0.0035]\n",
      "Epoch 14/20: 100%|██████████| 1394/1394 [00:27<00:00, 50.37it/s, Loss=0.0032]\n",
      "Epoch 15/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.26it/s, Loss=0.0050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 训练F1: 0.9879 | 验证F1: 0.9866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.43it/s, Loss=0.0109]\n",
      "Epoch 17/20: 100%|██████████| 1394/1394 [00:26<00:00, 51.95it/s, Loss=0.0033]\n",
      "Epoch 18/20: 100%|██████████| 1394/1394 [00:26<00:00, 52.17it/s, Loss=0.0052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早停触发，最佳F1: 0.9899\n",
      "二分类预训练完成，最佳F1: 0.9899\n",
      "\n",
      "=== 第二阶段：准备多分类数据和分层采样 ===\n",
      "恶意流量样本数: 336,430\n",
      "恶意流量类别分布:\n",
      "  Bot: 1,437 样本 (0.43%)\n",
      "  Brute_Force: 9,150 样本 (2.72%)\n",
      "  DDoS: 128,014 样本 (38.05%)\n",
      "  DoS: 193,730 样本 (57.58%)\n",
      "  PortScan: 1,956 样本 (0.58%)\n",
      "  Web_Attack: 2,143 样本 (0.64%)\n",
      "多分类训练集: 215,315\n",
      "SMOTE采样完成: 215,315 → 404,292 样本\n",
      "\n",
      "=== 第三阶段：端到端微调多分类模型 ===\n",
      "成功加载预训练编码器: /kaggle/working/best_TransformerBinary_Pretrain.pth\n",
      "开始训练 TransformerMultiClass_FineTuned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 790/790 [00:19<00:00, 40.55it/s, Loss=0.0112]\n",
      "Epoch 2/25: 100%|██████████| 790/790 [00:19<00:00, 40.76it/s, Loss=0.0258]\n",
      "Epoch 3/25: 100%|██████████| 790/790 [00:19<00:00, 40.58it/s, Loss=0.0044]\n",
      "Epoch 4/25: 100%|██████████| 790/790 [00:19<00:00, 40.02it/s, Loss=0.0029]\n",
      "Epoch 5/25: 100%|██████████| 790/790 [00:19<00:00, 39.61it/s, Loss=0.0177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 训练F1: 0.9904 | 验证F1: 0.9090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 790/790 [00:19<00:00, 40.30it/s, Loss=0.0036]\n",
      "Epoch 7/25: 100%|██████████| 790/790 [00:19<00:00, 40.38it/s, Loss=0.0039]\n",
      "Epoch 8/25: 100%|██████████| 790/790 [00:19<00:00, 40.09it/s, Loss=0.0020]\n",
      "Epoch 9/25: 100%|██████████| 790/790 [00:19<00:00, 40.39it/s, Loss=0.0056]\n",
      "Epoch 10/25: 100%|██████████| 790/790 [00:20<00:00, 39.31it/s, Loss=0.0022]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 训练F1: 0.9916 | 验证F1: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 790/790 [00:19<00:00, 39.55it/s, Loss=0.0009]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "早停触发，最佳F1: 0.9687\n",
      "多分类微调完成，最佳F1: 0.9687\n",
      "\n",
      "=== 层次化Transformer增强模型评估 ===\n",
      "\n",
      "评估 Transformer增强二分类模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformer增强二分类模型 分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Benign     0.9979    0.9960    0.9970    378532\n",
      "   Malicious     0.9777    0.9883    0.9830     67286\n",
      "\n",
      "    accuracy                         0.9948    445818\n",
      "   macro avg     0.9878    0.9922    0.9900    445818\n",
      "weighted avg     0.9949    0.9948    0.9948    445818\n",
      "\n",
      "\n",
      "Transformer增强二分类模型 整体性能:\n",
      "  准确率: 0.9948\n",
      "  宏平均F1: 0.9900\n",
      "  加权平均F1: 0.9948\n",
      "  平均不确定性: 0.0001\n",
      "\n",
      "评估 Transformer增强多分类模型\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformer增强多分类模型 分类报告:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Bot     0.9379    1.0000    0.9680       287\n",
      " Brute_Force     1.0000    0.9781    0.9890      1830\n",
      "        DDoS     0.9999    0.9993    0.9996     25603\n",
      "         DoS     0.9994    0.9979    0.9986     38746\n",
      "    PortScan     0.9842    0.9540    0.9688       391\n",
      "  Web_Attack     0.7992    0.9930    0.8857       429\n",
      "\n",
      "    accuracy                         0.9976     67286\n",
      "   macro avg     0.9534    0.9871    0.9683     67286\n",
      "weighted avg     0.9979    0.9976    0.9977     67286\n",
      "\n",
      "\n",
      "Transformer增强多分类模型 整体性能:\n",
      "  准确率: 0.9976\n",
      "  宏平均F1: 0.9683\n",
      "  加权平均F1: 0.9977\n",
      "  平均不确定性: 0.0027\n",
      "\n",
      "=== 层次化Transformer增强检测系统效果总结 ===\n",
      "\n",
      "二分类模型性能:\n",
      "   准确率: 0.9948\n",
      "   宏平均F1: 0.9900\n",
      "\n",
      "多分类模型性能:\n",
      "   准确率: 0.9976\n",
      "   宏平均F1: 0.9683\n",
      "\n",
      "结果已保存到: /kaggle/working/hierarchical_transformer_results.json\n",
      "\n",
      "层次化Transformer增强的网络入侵检测系统完成！\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# Hierarchical Transformer-Enhanced Network Intrusion Detection System\n",
    "# 简化输出版本 - 保持核心功能，精简输出信息\n",
    "# =====================================================================\n",
    "\n",
    "# Cell 1: 环境准备和导入库\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 安装必要的包\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"scikit-learn\", \"imbalanced-learn\"], \n",
    "                     stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTETomek\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"环境初始化完成，使用设备: {device}\")\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 2: Transformer增强的模型架构\n",
    "# =====================================================================\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    \"\"\"多尺度注意力机制：结合类别、时间、空间注意力\"\"\"\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # 类别特定注意力（保持原有设计）\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(input_dim // 4, input_dim),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        \n",
    "        # 🆕 时间序列注意力 - 学习流量的时序模式\n",
    "        self.temporal_attention = nn.MultiheadAttention(\n",
    "            input_dim, num_heads=4, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # 🆕 空间特征注意力 - 学习特征间的空间关系\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(input_dim // 8, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 🆕 注意力融合权重\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 1. 类别特定注意力（原有机制）\n",
    "        class_features = []\n",
    "        for i in range(self.num_classes):\n",
    "            attention_weights = self.class_attention[i](x)\n",
    "            attended_features = attention_weights * x\n",
    "            class_features.append(attended_features)\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        \n",
    "        # 2. 🆕 时间序列注意力\n",
    "        x_temporal = x.unsqueeze(1)  # [B, 1, D]\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)  # [B, D]\n",
    "        \n",
    "        # 3. 🆕 空间特征注意力\n",
    "        spatial_weights = self.spatial_attention(x)\n",
    "        spatial_attended = spatial_weights * x\n",
    "        \n",
    "        # 4. 🆕 多尺度融合\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)  # [B, D]\n",
    "        \n",
    "        # 加权融合三种注意力\n",
    "        fused_features = (weights[0] * class_attended_mean + \n",
    "                         weights[1] * temporal_attended + \n",
    "                         weights[2] * spatial_attended)\n",
    "        \n",
    "        return class_attended, fused_features\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    \"\"\"Transformer增强的集成模型：支持层次化检测架构\"\"\"\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3, use_pretrained=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.use_pretrained = use_pretrained\n",
    "        \n",
    "        # 共享编码器（可以被预训练和迁移）\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate / 2)\n",
    "        )\n",
    "        \n",
    "        # 🆕 Transformer编码器 - 学习长距离依赖关系\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.feature_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "            batch_first=True,\n",
    "            activation='gelu'  # 使用GELU激活函数\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # 🆕 位置编码 - 为特征添加位置信息\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        \n",
    "        # 多尺度注意力机制\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        \n",
    "        # 类别特定分类头（保持原有设计）\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout_rate),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 1)\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        \n",
    "        # 全局分类器（改进版）\n",
    "        self.global_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),  # 🆕 使用LayerNorm\n",
    "            nn.GELU(),          # 🆕 使用GELU激活\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 🆕 自适应融合网络 - 学习最优融合策略\n",
    "        self.fusion_network = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # 特征提取器（用于迁移学习）\n",
    "        self.feature_extractor = nn.Identity()\n",
    "        \n",
    "        # 🆕 不确定性估计头 - 评估预测置信度\n",
    "        self.uncertainty_head = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, return_features=False, return_uncertainty=False):\n",
    "        # 共享特征提取\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return shared_features\n",
    "        \n",
    "        # 🆕 Transformer处理 - 学习特征间的全局依赖\n",
    "        # 添加位置编码并转换为序列格式\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input)\n",
    "        transformer_features = transformer_features.squeeze(1)  # [B, 256]\n",
    "        \n",
    "        # 🆕 残差连接 + Layer Normalization\n",
    "        enhanced_features = F.layer_norm(\n",
    "            shared_features + transformer_features, \n",
    "            normalized_shape=[self.feature_dim]\n",
    "        )\n",
    "        \n",
    "        # 多尺度注意力处理\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        \n",
    "        # 类别特定输出\n",
    "        class_specific_outputs = []\n",
    "        for i in range(self.num_classes):\n",
    "            output = self.class_specific_heads[i](class_attended_features[:, i, :])\n",
    "            class_specific_outputs.append(output)\n",
    "        class_specific_logits = torch.cat(class_specific_outputs, dim=1)\n",
    "        \n",
    "        # 全局输出（使用融合后的特征）\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        \n",
    "        # 🆕 学习的自适应融合\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = (fusion_weights[:, 0:1] * class_specific_logits + \n",
    "                       fusion_weights[:, 1:2] * global_logits)\n",
    "        \n",
    "        # 🆕 不确定性估计\n",
    "        if return_uncertainty:\n",
    "            uncertainty = self.uncertainty_head(enhanced_features)\n",
    "            return final_logits, uncertainty\n",
    "        \n",
    "        return final_logits\n",
    "    \n",
    "    def load_pretrained_encoder(self, pretrained_model_path):\n",
    "        \"\"\"加载预训练的编码器权重\"\"\"\n",
    "        if os.path.exists(pretrained_model_path):\n",
    "            pretrained_state = torch.load(pretrained_model_path, map_location=device)\n",
    "            # 只加载编码器部分的权重\n",
    "            encoder_state = {}\n",
    "            for key, value in pretrained_state.items():\n",
    "                if key.startswith('shared_encoder'):\n",
    "                    encoder_state[key] = value\n",
    "            \n",
    "            self.load_state_dict(encoder_state, strict=False)\n",
    "            print(f\"成功加载预训练编码器: {pretrained_model_path}\")\n",
    "        else:\n",
    "            print(f\"预训练模型不存在: {pretrained_model_path}\")\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 3: 改进的损失函数和训练工具\n",
    "# =====================================================================\n",
    "\n",
    "class UncertaintyAwareFocalLoss(nn.Module):\n",
    "    \"\"\"不确定性感知的自适应焦点损失\"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, class_specific_gamma=None, uncertainty_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_specific_gamma = class_specific_gamma or {}\n",
    "        self.uncertainty_weight = uncertainty_weight\n",
    "        \n",
    "    def forward(self, inputs, targets, uncertainty=None):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        \n",
    "        # 为不同类别使用不同的gamma值\n",
    "        gamma_values = torch.full_like(targets, self.gamma, dtype=torch.float)\n",
    "        for class_idx, gamma_val in self.class_specific_gamma.items():\n",
    "            mask = (targets == class_idx)\n",
    "            gamma_values[mask] = gamma_val\n",
    "        \n",
    "        # 基础focal loss\n",
    "        focal_loss = (1 - pt) ** gamma_values * ce_loss\n",
    "        \n",
    "        # 🆕 不确定性加权：高不确定性样本获得更多关注\n",
    "        if uncertainty is not None:\n",
    "            uncertainty_weight = 1 + self.uncertainty_weight * uncertainty.squeeze()\n",
    "            focal_loss = uncertainty_weight * focal_loss\n",
    "        \n",
    "        # Alpha权重平衡\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.device != inputs.device:\n",
    "                self.alpha = self.alpha.to(inputs.device)\n",
    "            alpha_t = self.alpha[targets]\n",
    "            focal_loss = alpha_t * focal_loss\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "\n",
    "def create_stratified_sampler(y_train, sampling_strategy):\n",
    "    \"\"\"创建分层采样策略\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    \n",
    "    # 应用采样策略\n",
    "    if sampling_strategy['method'] == 'hierarchical_smote':\n",
    "        return apply_hierarchical_smote(y_train, sampling_strategy)\n",
    "    elif sampling_strategy['method'] == 'adaptive_smote':\n",
    "        return apply_adaptive_smote(y_train, sampling_strategy)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def apply_hierarchical_smote(y_train, strategy):\n",
    "    \"\"\"分层SMOTE采样\"\"\"\n",
    "    tier1_classes = strategy.get('tier1_classes', [])  # 关键少数类\n",
    "    tier2_classes = strategy.get('tier2_classes', [])  # 普通少数类\n",
    "    \n",
    "    sampling_ratios = {}\n",
    "    class_counts = Counter(y_train)\n",
    "    max_count = max(class_counts.values())\n",
    "    \n",
    "    for class_idx, count in class_counts.items():\n",
    "        if class_idx in tier1_classes:\n",
    "            # 关键类别：采样到最大类别的50%\n",
    "            sampling_ratios[class_idx] = int(max_count * 0.5)\n",
    "        elif class_idx in tier2_classes:\n",
    "            # 普通少数类：采样到最大类别的30%\n",
    "            sampling_ratios[class_idx] = int(max_count * 0.3)\n",
    "        else:\n",
    "            # 多数类：保持原样\n",
    "            sampling_ratios[class_idx] = count\n",
    "    \n",
    "    return sampling_ratios\n",
    "\n",
    "def apply_adaptive_smote(y_train, strategy):\n",
    "    \"\"\"自适应SMOTE采样\"\"\"\n",
    "    class_counts = Counter(y_train)\n",
    "    total_samples = len(y_train)\n",
    "    \n",
    "    sampling_ratios = {}\n",
    "    for class_idx, count in class_counts.items():\n",
    "        # 基于类别占比的自适应采样\n",
    "        class_ratio = count / total_samples\n",
    "        if class_ratio < 0.01:  # 极少数类\n",
    "            target_ratio = 0.05\n",
    "        elif class_ratio < 0.05:  # 少数类\n",
    "            target_ratio = 0.1\n",
    "        else:  # 多数类\n",
    "            target_ratio = class_ratio\n",
    "        \n",
    "        sampling_ratios[class_idx] = int(total_samples * target_ratio)\n",
    "    \n",
    "    return sampling_ratios\n",
    "\n",
    "def train_transformer_model(model, train_loader, val_loader, device, config):\n",
    "    \"\"\"改进的Transformer模型训练函数\"\"\"\n",
    "    print(f\"开始训练 {config['model_name']}\")\n",
    "    \n",
    "    # 🆕 分层学习率优化器设置\n",
    "    if config.get('use_pretrained', False):\n",
    "        # 微调模式：不同组件使用不同学习率\n",
    "        encoder_params = []\n",
    "        transformer_params = []\n",
    "        other_params = []\n",
    "        \n",
    "        for name, param in model.named_parameters():\n",
    "            if 'shared_encoder' in name:\n",
    "                encoder_params.append(param)\n",
    "            elif 'transformer' in name or 'pos_encoding' in name:\n",
    "                transformer_params.append(param)\n",
    "            else:\n",
    "                other_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW([\n",
    "            {'params': encoder_params, 'lr': config['lr'] * 0.1},      # 编码器较低学习率\n",
    "            {'params': transformer_params, 'lr': config['lr'] * 0.5},  # Transformer中等学习率\n",
    "            {'params': other_params, 'lr': config['lr']}               # 其他组件正常学习率\n",
    "        ], weight_decay=config.get('weight_decay', 1e-4))\n",
    "    else:\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=config['lr'], \n",
    "                              weight_decay=config.get('weight_decay', 1e-4))\n",
    "    \n",
    "    # 🆕 余弦退火学习率调度器\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=config['lr'] if not config.get('use_pretrained', False) else [config['lr']*0.1, config['lr']*0.5, config['lr']],\n",
    "        epochs=config['num_epochs'],\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # 🆕 不确定性感知损失函数\n",
    "    if 'class_weights' in config and config['class_weights'] is not None:\n",
    "        alpha = torch.FloatTensor(config['class_weights']).to(device)\n",
    "    else:\n",
    "        alpha = None\n",
    "    \n",
    "    # 为关键少数类使用更高的gamma值\n",
    "    class_specific_gamma = {}\n",
    "    if 'minority_classes' in config:\n",
    "        for class_idx in config['minority_classes']:\n",
    "            class_specific_gamma[class_idx] = 3.0  # 更关注困难样本\n",
    "    \n",
    "    criterion = UncertaintyAwareFocalLoss(\n",
    "        alpha=alpha, \n",
    "        gamma=2.0,\n",
    "        class_specific_gamma=class_specific_gamma,\n",
    "        uncertainty_weight=0.5\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    best_model_path = f\"/kaggle/working/best_{config['model_name']}.pth\"\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_preds, train_labels = [], []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "        for batch_idx, (inputs, labels) in enumerate(pbar):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 🆕 前向传播（包含不确定性估计）\n",
    "            outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "            loss = criterion(outputs, labels, uncertainty)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # 🆕 梯度裁剪防止梯度爆炸\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # 收集预测结果\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_preds.extend(predicted.cpu().numpy())\n",
    "            train_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_labels = [], []\n",
    "        val_uncertainties = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "                loss = criterion(outputs, labels, uncertainty)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_preds.extend(predicted.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_uncertainties.extend(uncertainty.cpu().numpy())\n",
    "        \n",
    "        # 计算指标\n",
    "        train_f1 = f1_score(train_labels, train_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:  # 只每5个epoch打印一次\n",
    "            print(f\"Epoch {epoch+1}: 训练F1: {train_f1:.4f} | 验证F1: {val_f1:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= config.get('patience', 7):\n",
    "                print(f\"早停触发，最佳F1: {best_val_f1:.4f}\")\n",
    "                break\n",
    "    \n",
    "    return best_model_path, best_val_f1\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 4: 数据加载和预处理（修正版 - 与simple.ipynb保持一致）\n",
    "# =====================================================================\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"加载和预处理CIC-IDS2017数据\"\"\"\n",
    "    print(\"数据加载和预处理中...\")\n",
    "    \n",
    "    # 加载数据\n",
    "    data_path = '/kaggle/input/cicids2017'\n",
    "    \n",
    "    parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path) \n",
    "                     if f.endswith('.parquet')]\n",
    "    \n",
    "    df_list = []\n",
    "    for file in tqdm(parquet_files, desc=\"加载数据文件\", leave=False):\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        df_list.append(df_temp)\n",
    "    \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    del df_list\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"数据形状: {df.shape}\")\n",
    "    \n",
    "    # 数据清理\n",
    "    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n",
    "    label_column = 'Label'\n",
    "    \n",
    "    # 处理无穷值和NaN\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # 离群值修剪\n",
    "    numeric_columns = df.select_dtypes(include=np.number).columns\n",
    "    numeric_columns = [col for col in numeric_columns if col != label_column]\n",
    "    \n",
    "    for col in tqdm(numeric_columns, desc=\"数据清理\", leave=False):\n",
    "        q99, q01 = df[col].quantile(0.99), df[col].quantile(0.01)\n",
    "        df[col] = df[col].clip(lower=q01, upper=q99)\n",
    "    \n",
    "    # 去重\n",
    "    rows_before = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    if rows_before != df.shape[0]:\n",
    "        print(f\"移除了 {rows_before - df.shape[0]:,} 条重复记录\")\n",
    "    \n",
    "    # 标签清理和映射\n",
    "    df[label_column] = df[label_column].astype(str).str.replace(\n",
    "        r'[^a-zA-Z0-9\\s-]', '', regex=True\n",
    "    ).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    \n",
    "    # 创建标签映射\n",
    "    # 1. 二分类标签\n",
    "    df['Binary_Label'] = df[label_column].apply(\n",
    "        lambda x: 'Benign' if x == 'Benign' else 'Malicious'\n",
    "    )\n",
    "    \n",
    "    # 2. 使用与simple.ipynb相同的多分类标签映射\n",
    "    multi_class_mapping = {\n",
    "        'DoS Hulk': 'DoS',\n",
    "        'DoS GoldenEye': 'DoS', \n",
    "        'DoS slowloris': 'DoS',\n",
    "        'DoS Slowhttptest': 'DoS',\n",
    "        'FTP-Patator': 'Brute_Force',\n",
    "        'SSH-Patator': 'Brute_Force',\n",
    "        'Web Attack Brute Force': 'Web_Attack',\n",
    "        'Web Attack XSS': 'Web_Attack',\n",
    "        'Web Attack Sql Injection': 'Web_Attack',\n",
    "        'PortScan': 'PortScan',\n",
    "        'Bot': 'Bot',\n",
    "        'Infiltration': 'Rare_Attacks',\n",
    "        'Heartbleed': 'Rare_Attacks'\n",
    "    }\n",
    "    \n",
    "    df['Multi_Label'] = df[label_column].replace(multi_class_mapping)\n",
    "    \n",
    "    # 移除极少数类别（样本太少）\n",
    "    df = df[~df['Multi_Label'].isin(['Rare_Attacks'])]\n",
    "    \n",
    "    print(\"标签分布:\")\n",
    "    print(df['Multi_Label'].value_counts())\n",
    "    \n",
    "    # 准备特征和标签\n",
    "    feature_columns = [col for col in df.columns \n",
    "                      if col not in [label_column, 'Binary_Label', 'Multi_Label']]\n",
    "    X = df[feature_columns].copy()\n",
    "    \n",
    "    # 编码标签\n",
    "    le_binary = LabelEncoder()\n",
    "    y_binary = le_binary.fit_transform(df['Binary_Label'])\n",
    "    \n",
    "    le_multi = LabelEncoder()\n",
    "    y_multi = le_multi.fit_transform(df['Multi_Label'])\n",
    "    \n",
    "    print(f\"特征维度: {X.shape[1]}\")\n",
    "    print(f\"多分类类别: {list(le_multi.classes_)}\")\n",
    "    \n",
    "    return X, y_binary, y_multi, le_binary, le_multi\n",
    "\n",
    "# 执行数据加载\n",
    "X, y_binary, y_multi, le_binary, le_multi = load_and_preprocess_data()\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 5: 第一阶段 - 训练二分类预训练模型\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== 第一阶段：训练Transformer增强的二分类预训练模型 ===\")\n",
    "\n",
    "# 数据分割\n",
    "X_train, X_test, y_train_binary, y_test_binary = train_test_split(\n",
    "    X, y_binary, test_size=0.2, random_state=42, stratify=y_binary\n",
    ")\n",
    "X_train, X_val, y_train_binary, y_val_binary = train_test_split(\n",
    "    X_train, y_train_binary, test_size=0.2, random_state=42, stratify=y_train_binary\n",
    ")\n",
    "\n",
    "# 特征标准化\n",
    "scaler_binary = StandardScaler()\n",
    "X_train_scaled = scaler_binary.fit_transform(X_train)\n",
    "X_val_scaled = scaler_binary.transform(X_val)\n",
    "X_test_scaled = scaler_binary.transform(X_test)\n",
    "\n",
    "print(f\"训练集大小: {len(X_train_scaled):,}\")\n",
    "\n",
    "# 计算类别权重\n",
    "class_weights_binary = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(y_train_binary), \n",
    "    y=y_train_binary\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_train_scaled).float(), \n",
    "                 torch.from_numpy(y_train_binary).long()),\n",
    "    batch_size=1024, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "val_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_val_scaled).float(), \n",
    "                 torch.from_numpy(y_val_binary).long()),\n",
    "    batch_size=1024, num_workers=2\n",
    ")\n",
    "\n",
    "# 创建Transformer增强的二分类模型\n",
    "binary_model = TransformerEnhancedEnsembleModel(\n",
    "    input_dim=X.shape[1], \n",
    "    num_classes=2, \n",
    "    dropout_rate=0.3\n",
    ").to(device)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in binary_model.parameters()):,}\")\n",
    "\n",
    "# 训练配置\n",
    "binary_config = {\n",
    "    'model_name': 'TransformerBinary_Pretrain',\n",
    "    'num_epochs': 20,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 5,\n",
    "    'class_weights': class_weights_binary,\n",
    "    'minority_classes': [1],\n",
    "    'use_pretrained': False\n",
    "}\n",
    "\n",
    "# 训练Transformer增强的二分类模型\n",
    "best_binary_path, best_binary_f1 = train_transformer_model(\n",
    "    binary_model, train_loader_binary, val_loader_binary, device, binary_config\n",
    ")\n",
    "\n",
    "print(f\"二分类预训练完成，最佳F1: {best_binary_f1:.4f}\")\n",
    "\n",
    "# 清理内存\n",
    "del train_loader_binary, val_loader_binary\n",
    "gc.collect()\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 6: 第二阶段 - 准备多分类数据和分层采样\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== 第二阶段：准备多分类数据和分层采样 ===\")\n",
    "\n",
    "# 保持原有逻辑：只在恶意流量中进行多分类\n",
    "malicious_indices = (y_binary == 1)\n",
    "X_malicious = X[malicious_indices].copy()\n",
    "y_malicious_original = y_multi[malicious_indices].copy()\n",
    "\n",
    "print(f\"恶意流量样本数: {len(X_malicious):,}\")\n",
    "\n",
    "# 检查原始标签的唯一值\n",
    "unique_labels = np.unique(y_malicious_original)\n",
    "\n",
    "# 重新映射标签，确保从0开始连续\n",
    "label_mapping = {old_label: new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "reverse_mapping = {new_label: old_label for old_label, new_label in label_mapping.items()}\n",
    "\n",
    "y_malicious = np.array([label_mapping[label] for label in y_malicious_original])\n",
    "\n",
    "# 创建新的标签编码器\n",
    "le_multi_subset = LabelEncoder()\n",
    "class_names_subset = [le_multi.classes_[reverse_mapping[i]] for i in range(len(unique_labels))]\n",
    "le_multi_subset.classes_ = np.array(class_names_subset)\n",
    "\n",
    "print(\"恶意流量类别分布:\")\n",
    "multi_class_counts = Counter(y_malicious)\n",
    "for class_idx, count in sorted(multi_class_counts.items()):\n",
    "    class_name = class_names_subset[class_idx]\n",
    "    percentage = count / len(y_malicious) * 100\n",
    "    print(f\"  {class_name}: {count:,} 样本 ({percentage:.2f}%)\")\n",
    "\n",
    "# 识别关键少数类别\n",
    "tier1_classes = []\n",
    "tier2_classes = []\n",
    "total_malicious = len(y_malicious)\n",
    "\n",
    "for class_idx, count in multi_class_counts.items():\n",
    "    ratio = count / total_malicious\n",
    "    if ratio < 0.05:\n",
    "        tier1_classes.append(class_idx)\n",
    "    elif ratio < 0.2:\n",
    "        tier2_classes.append(class_idx)\n",
    "\n",
    "# 数据分割\n",
    "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
    "    X_malicious, y_malicious, test_size=0.2, random_state=42, stratify=y_malicious\n",
    ")\n",
    "X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(\n",
    "    X_train_m, y_train_m, test_size=0.2, random_state=42, stratify=y_train_m\n",
    ")\n",
    "\n",
    "print(f\"多分类训练集: {len(X_train_m):,}\")\n",
    "\n",
    "# 应用分层SMOTE采样\n",
    "max_count = max(Counter(y_train_m).values())\n",
    "sampling_ratios = {}\n",
    "\n",
    "for class_idx in range(len(class_names_subset)):\n",
    "    current_count = sum(y_train_m == class_idx)\n",
    "    if class_idx in tier1_classes:\n",
    "        target_count = max(current_count, int(max_count * 0.4))\n",
    "    elif class_idx in tier2_classes:\n",
    "        target_count = max(current_count, int(max_count * 0.25))\n",
    "    else:\n",
    "        target_count = current_count\n",
    "    \n",
    "    sampling_ratios[class_idx] = target_count\n",
    "\n",
    "# 应用SMOTE采样\n",
    "try:\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy=sampling_ratios,\n",
    "        random_state=42,\n",
    "        k_neighbors=min(5, min(Counter(y_train_m).values()) - 1)\n",
    "    )\n",
    "    \n",
    "    X_train_m_resampled, y_train_m_resampled = smote.fit_resample(X_train_m, y_train_m)\n",
    "    print(f\"SMOTE采样完成: {len(X_train_m):,} → {len(X_train_m_resampled):,} 样本\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"SMOTE采样失败: {str(e)}，使用原始数据\")\n",
    "    X_train_m_resampled, y_train_m_resampled = X_train_m, y_train_m\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 7: 第三阶段 - 端到端微调多分类模型\n",
    "# =====================================================================\n",
    "\n",
    "print(\"\\n=== 第三阶段：端到端微调多分类模型 ===\")\n",
    "\n",
    "# 特征标准化\n",
    "scaler_multi = StandardScaler()\n",
    "X_train_m_scaled = scaler_multi.fit_transform(X_train_m_resampled)\n",
    "X_val_m_scaled = scaler_multi.transform(X_val_m)\n",
    "X_test_m_scaled = scaler_multi.transform(X_test_m)\n",
    "\n",
    "# 计算多分类的类别权重\n",
    "unique_classes = np.arange(len(class_names_subset))\n",
    "class_weights_multi = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=unique_classes, \n",
    "    y=y_train_m_resampled\n",
    ")\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_train_m_scaled).float(), \n",
    "                 torch.from_numpy(y_train_m_resampled).long()),\n",
    "    batch_size=512, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_val_m_scaled).float(), \n",
    "                 torch.from_numpy(y_val_m).long()),\n",
    "    batch_size=512, num_workers=0\n",
    ")\n",
    "\n",
    "test_loader_multi = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test_m_scaled).float(), \n",
    "                 torch.from_numpy(y_test_m).long()),\n",
    "    batch_size=512, num_workers=0\n",
    ")\n",
    "\n",
    "# 创建多分类模型\n",
    "num_classes = len(class_names_subset)\n",
    "\n",
    "multi_model = TransformerEnhancedEnsembleModel(\n",
    "    input_dim=X.shape[1], \n",
    "    num_classes=num_classes,\n",
    "    dropout_rate=0.4,\n",
    "    use_pretrained=True\n",
    ").to(device)\n",
    "\n",
    "# 加载预训练的编码器权重\n",
    "multi_model.load_pretrained_encoder(best_binary_path)\n",
    "\n",
    "# 微调配置\n",
    "multi_config = {\n",
    "    'model_name': 'TransformerMultiClass_FineTuned',\n",
    "    'num_epochs': 25,\n",
    "    'lr': 0.001,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 8,\n",
    "    'class_weights': class_weights_multi,\n",
    "    'minority_classes': tier1_classes + tier2_classes,\n",
    "    'use_pretrained': True\n",
    "}\n",
    "\n",
    "# 开始训练多分类模型\n",
    "best_multi_path, best_multi_f1 = train_transformer_model(\n",
    "    multi_model, train_loader_multi, val_loader_multi, device, multi_config\n",
    ")\n",
    "\n",
    "print(f\"多分类微调完成，最佳F1: {best_multi_f1:.4f}\")\n",
    "\n",
    "# 保存标签映射信息\n",
    "label_mapping_info = {\n",
    "    'original_to_new': label_mapping,\n",
    "    'new_to_original': reverse_mapping,\n",
    "    'class_names_subset': class_names_subset,\n",
    "    'tier1_classes': tier1_classes,\n",
    "    'tier2_classes': tier2_classes\n",
    "}\n",
    "\n",
    "import pickle\n",
    "with open('/kaggle/working/label_mapping.pkl', 'wb') as f:\n",
    "    pickle.dump(label_mapping_info, f)\n",
    "\n",
    "# =====================================================================\n",
    "# Cell 8: 模型评估和对比\n",
    "# =====================================================================\n",
    "\n",
    "def evaluate_transformer_model(model, model_path, test_loader, device, \n",
    "                               class_names, model_name):\n",
    "    \"\"\"评估Transformer模型性能\"\"\"\n",
    "    print(f\"\\n评估 {model_name}\")\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    y_true, y_pred, y_scores, uncertainties = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"模型评估\", leave=False):\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, uncertainty = model(inputs, return_uncertainty=True)\n",
    "            \n",
    "            # 获取预测概率\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            y_scores.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # 获取预测标签\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "            uncertainties.extend(uncertainty.cpu().numpy())\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    uncertainties = np.array(uncertainties)\n",
    "    \n",
    "    # 确保标签范围正确\n",
    "    max_label = max(y_true.max(), y_pred.max())\n",
    "    if max_label >= len(class_names):\n",
    "        y_true = np.clip(y_true, 0, len(class_names)-1)\n",
    "        y_pred = np.clip(y_pred, 0, len(class_names)-1)\n",
    "    \n",
    "    # 获取实际存在的标签\n",
    "    unique_labels = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    actual_class_names = [class_names[i] for i in unique_labels if i < len(class_names)]\n",
    "    \n",
    "    # 生成分类报告\n",
    "    try:\n",
    "        report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            labels=unique_labels,\n",
    "            target_names=actual_class_names,\n",
    "            digits=4, \n",
    "            zero_division=0\n",
    "        )\n",
    "        print(f\"\\n{model_name} 分类报告:\")\n",
    "        print(report)\n",
    "    except Exception as e:\n",
    "        print(f\"生成分类报告时出错: {e}\")\n",
    "    \n",
    "    # 计算整体指标\n",
    "    try:\n",
    "        macro_f1 = f1_score(y_true, y_pred, labels=unique_labels, average='macro')\n",
    "        weighted_f1 = f1_score(y_true, y_pred, labels=unique_labels, average='weighted')\n",
    "    except:\n",
    "        macro_f1 = 0.0\n",
    "        weighted_f1 = 0.0\n",
    "    \n",
    "    overall_accuracy = np.mean(y_true == y_pred)\n",
    "    avg_uncertainty = np.mean(uncertainties)\n",
    "    \n",
    "    print(f\"\\n{model_name} 整体性能:\")\n",
    "    print(f\"  准确率: {overall_accuracy:.4f}\")\n",
    "    print(f\"  宏平均F1: {macro_f1:.4f}\")\n",
    "    print(f\"  加权平均F1: {weighted_f1:.4f}\")\n",
    "    print(f\"  平均不确定性: {avg_uncertainty:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'accuracy': overall_accuracy,\n",
    "        'avg_uncertainty': avg_uncertainty,\n",
    "        'class_names': actual_class_names,\n",
    "        'unique_labels': unique_labels\n",
    "    }\n",
    "\n",
    "print(\"\\n=== 层次化Transformer增强模型评估 ===\")\n",
    "\n",
    "# 评估二分类模型\n",
    "test_loader_binary = DataLoader(\n",
    "    TensorDataset(torch.from_numpy(X_test_scaled).float(), \n",
    "                 torch.from_numpy(y_test_binary).long()),\n",
    "    batch_size=1024, num_workers=0\n",
    ")\n",
    "\n",
    "binary_class_names = le_binary.classes_.tolist()\n",
    "binary_results = evaluate_transformer_model(\n",
    "    binary_model, best_binary_path, test_loader_binary, device,\n",
    "    binary_class_names, \"Transformer增强二分类模型\"\n",
    ")\n",
    "\n",
    "# 评估多分类模型\n",
    "multi_results = evaluate_transformer_model(\n",
    "    multi_model, best_multi_path, test_loader_multi, device,\n",
    "    class_names_subset, \"Transformer增强多分类模型\"\n",
    ")\n",
    "\n",
    "# 性能总结\n",
    "print(\"\\n=== 层次化Transformer增强检测系统效果总结 ===\")\n",
    "\n",
    "print(f\"\\n二分类模型性能:\")\n",
    "print(f\"   准确率: {binary_results['accuracy']:.4f}\")\n",
    "print(f\"   宏平均F1: {binary_results['macro_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\n多分类模型性能:\")\n",
    "print(f\"   准确率: {multi_results['accuracy']:.4f}\")\n",
    "print(f\"   宏平均F1: {multi_results['macro_f1']:.4f}\")\n",
    "\n",
    "# 保存完整结果\n",
    "results_summary = {\n",
    "    'transformer_binary_model': {\n",
    "        'path': best_binary_path,\n",
    "        'accuracy': binary_results['accuracy'],\n",
    "        'macro_f1': binary_results['macro_f1']\n",
    "    },\n",
    "    'transformer_multi_model': {\n",
    "        'path': best_multi_path,\n",
    "        'accuracy': multi_results['accuracy'],\n",
    "        'macro_f1': multi_results['macro_f1']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "try:\n",
    "    with open('/kaggle/working/hierarchical_transformer_results.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2, default=str)\n",
    "    print(\"\\n结果已保存到: /kaggle/working/hierarchical_transformer_results.json\")\n",
    "except Exception as e:\n",
    "    print(f\"保存结果时出错: {e}\")\n",
    "\n",
    "print(\"\\n层次化Transformer增强的网络入侵检测系统完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6cfd534",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:59:06.702304Z",
     "iopub.status.busy": "2025-08-24T09:59:06.701892Z",
     "iopub.status.idle": "2025-08-24T09:59:06.774746Z",
     "shell.execute_reply": "2025-08-24T09:59:06.773791Z"
    },
    "papermill": {
     "duration": 1.771145,
     "end_time": "2025-08-24T09:59:06.776097",
     "exception": false,
     "start_time": "2025-08-24T09:59:05.004952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 所有必要变量检查通过，开始打包...\n",
      "\n",
      "📋 验证模型包...\n",
      "  ✅ scaler.pkl\n",
      "  ✅ label_encoder.pkl\n",
      "  ✅ feature_selector.pkl\n",
      "  ✅ selected_features.json\n",
      "  ✅ model.pth\n",
      "  ✅ model_info.json\n",
      "  ✅ model_loader.py\n",
      "\n",
      "🎉 模型包已生成于: /kaggle/working/model_package\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# 模型打包和后端集成准备\n",
    "# =====================================================================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# 确保所有必要的变量都存在\n",
    "try:\n",
    "    assert 'X' in locals(), \"X 变量不存在\"\n",
    "    assert 'scaler_multi' in locals(), \"scaler_multi 变量不存在\"\n",
    "    assert 'le_binary' in locals(), \"le_binary 变量不存在\"\n",
    "    assert 'le_multi_subset' in locals(), \"le_multi_subset 变量不存在\"\n",
    "    assert 'class_names_subset' in locals(), \"class_names_subset 变量不存在\"\n",
    "    assert 'label_mapping_info' in locals(), \"label_mapping_info 变量不存在\"\n",
    "    assert 'tier1_classes' in locals(), \"tier1_classes 变量不存在\"\n",
    "    assert 'tier2_classes' in locals(), \"tier2_classes 变量不存在\"\n",
    "    assert 'best_binary_path' in locals(), \"best_binary_path 变量不存在\"\n",
    "    assert 'best_multi_path' in locals(), \"best_multi_path 变量不存在\"\n",
    "    assert 'binary_results' in locals(), \"binary_results 变量不存在\"\n",
    "    assert 'multi_results' in locals(), \"multi_results 变量不存在\"\n",
    "    print(\"✅ 所有必要变量检查通过，开始打包...\")\n",
    "except AssertionError as e:\n",
    "    print(f\"❌ 变量检查失败: {e}\")\n",
    "    print(\"请确保在完整的训练和评估流程之后再运行此打包代码。\")\n",
    "\n",
    "# 创建模型包目录\n",
    "model_package_dir = \"/kaggle/working/model_package\"\n",
    "os.makedirs(model_package_dir, exist_ok=True)\n",
    "\n",
    "# 1. 保存特征缩放器 (scaler.pkl)\n",
    "scaler_path = os.path.join(model_package_dir, \"scaler.pkl\")\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler_multi, f)\n",
    "\n",
    "# 2. 保存标签编码器 (label_encoder.pkl)\n",
    "label_encoders = {\n",
    "    'binary_encoder': le_binary,\n",
    "    'multi_encoder': le_multi_subset,\n",
    "    'binary_classes': le_binary.classes_.tolist(),\n",
    "    'multi_classes': class_names_subset,\n",
    "    'label_mapping': label_mapping_info\n",
    "}\n",
    "label_encoder_path = os.path.join(model_package_dir, \"label_encoder.pkl\")\n",
    "with open(label_encoder_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# 3. 保存特征选择器 (feature_selector.pkl)\n",
    "feature_selector = {\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'selected_features': X.columns.tolist(),\n",
    "    'feature_count': len(X.columns),\n",
    "    'selection_method': 'all_features'\n",
    "}\n",
    "feature_selector_path = os.path.join(model_package_dir, \"feature_selector.pkl\")\n",
    "with open(feature_selector_path, 'wb') as f:\n",
    "    pickle.dump(feature_selector, f)\n",
    "\n",
    "# 4. 保存选中特征列表 (selected_features.json)\n",
    "selected_features = {\n",
    "    'features': X.columns.tolist(),\n",
    "    'count': len(X.columns)\n",
    "}\n",
    "selected_features_path = os.path.join(model_package_dir, \"selected_features.json\")\n",
    "with open(selected_features_path, 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2)\n",
    "\n",
    "# 5. 保存模型权重 (model.pth)\n",
    "model_state = {\n",
    "    'binary_model_state': torch.load(best_binary_path, map_location='cpu'),\n",
    "    'multi_model_state': torch.load(best_multi_path, map_location='cpu')\n",
    "}\n",
    "model_path = os.path.join(model_package_dir, \"model.pth\")\n",
    "torch.save(model_state, model_path)\n",
    "\n",
    "# 6. 保存模型元信息 (model_info.json)\n",
    "model_info = {\n",
    "    'model_name': 'HierarchicalTransformerIDS',\n",
    "    'model_version': '1.0.0',\n",
    "    'architecture': {\n",
    "        'input_features': len(X.columns),\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    },\n",
    "    'classes': {\n",
    "        'binary': le_binary.classes_.tolist(),\n",
    "        'multi': class_names_subset\n",
    "    },\n",
    "    'performance': {\n",
    "        'binary_stage': {\n",
    "            'accuracy': float(binary_results['accuracy']),\n",
    "            'macro_f1': float(binary_results['macro_f1'])\n",
    "        },\n",
    "        'multi_stage': {\n",
    "            'accuracy': float(multi_results['accuracy']),\n",
    "            'macro_f1': float(multi_results['macro_f1'])\n",
    "        }\n",
    "    },\n",
    "    'features': {\n",
    "        'total_features': len(X.columns),\n",
    "        'feature_names': X.columns.tolist()\n",
    "    }\n",
    "}\n",
    "model_info_path = os.path.join(model_package_dir, \"model_info.json\")\n",
    "with open(model_info_path, 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "# 7. 创建模型加载器类 (model_loader.py)\n",
    "model_loader_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_dim = input_dim\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, input_dim // 4), nn.ReLU(), nn.Dropout(0.1),\n",
    "                nn.Linear(input_dim // 4, input_dim), nn.Sigmoid()\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        self.temporal_attention = nn.MultiheadAttention(input_dim, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim // 8), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(input_dim // 8, input_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        class_features = [self.class_attention[i](x) * x for i in range(self.num_classes)]\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        x_temporal = x.unsqueeze(1)\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)\n",
    "        spatial_weights = self.spatial_attention(x)\n",
    "        spatial_attended = spatial_weights * x\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)\n",
    "        fused_features = (weights[0] * class_attended_mean + \n",
    "                         weights[1] * temporal_attended + \n",
    "                         weights[2] * spatial_attended)\n",
    "        return class_attended, fused_features\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate / 2)\n",
    "        )\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.feature_dim, nhead=8, dim_feedforward=512,\n",
    "            dropout=0.1, batch_first=True, activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "                nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1)\n",
    "            ) for _ in range(num_classes)\n",
    "        ])\n",
    "        self.global_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(),\n",
    "            nn.Dropout(dropout_rate), nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.fusion_network = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 2), nn.Softmax(dim=-1))\n",
    "        self.uncertainty_head = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, return_uncertainty=False):\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input).squeeze(1)\n",
    "        enhanced_features = F.layer_norm(shared_features + transformer_features, normalized_shape=[self.feature_dim])\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        class_specific_outputs = [self.class_specific_heads[i](class_attended_features[:, i, :]) for i in range(self.num_classes)]\n",
    "        class_specific_logits = torch.cat(class_specific_outputs, dim=1)\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = (fusion_weights[:, 0:1] * class_specific_logits + fusion_weights[:, 1:2] * global_logits)\n",
    "        if return_uncertainty:\n",
    "            return final_logits, self.uncertainty_head(enhanced_features)\n",
    "        return final_logits\n",
    "\n",
    "class HierarchicalTransformerIDSLoader:\n",
    "    def __init__(self, model_package_path: str):\n",
    "        self.model_package_path = model_package_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_components()\n",
    "    \n",
    "    def _load_components(self):\n",
    "        with open(os.path.join(self.model_package_path, \"model_info.json\"), 'r') as f:\n",
    "            self.model_info = json.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"scaler.pkl\"), 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "        model_checkpoint = torch.load(os.path.join(self.model_package_path, \"model.pth\"), map_location=self.device)\n",
    "        \n",
    "        arch_info = self.model_info['architecture']\n",
    "        self.binary_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['binary_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        self.multi_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['multi_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.binary_model.load_state_dict(model_checkpoint['binary_model_state'])\n",
    "        self.multi_model.load_state_dict(model_checkpoint['multi_model_state'])\n",
    "        \n",
    "        self.binary_model.eval()\n",
    "        self.multi_model.eval()\n",
    "    \n",
    "    def preprocess(self, data: np.ndarray) -> torch.Tensor:\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return torch.from_numpy(scaled_data).float().to(self.device)\n",
    "    \n",
    "    def predict(self, data: np.ndarray):\n",
    "        with torch.no_grad():\n",
    "            processed_data = self.preprocess(data)\n",
    "            binary_logits, binary_uncertainty = self.binary_model(processed_data, return_uncertainty=True)\n",
    "            binary_probs = F.softmax(binary_logits, dim=1)\n",
    "            binary_preds = torch.argmax(binary_probs, dim=1)\n",
    "            \n",
    "            multi_probs = torch.zeros(len(data), len(self.model_info['classes']['multi']))\n",
    "            multi_uncertainty = torch.zeros(len(data), 1)\n",
    "            \n",
    "            malicious_mask = binary_preds == 1\n",
    "            if malicious_mask.sum() > 0:\n",
    "                malicious_data = processed_data[malicious_mask]\n",
    "                multi_logits, multi_unc = self.multi_model(malicious_data, return_uncertainty=True)\n",
    "                multi_probs[malicious_mask] = F.softmax(multi_logits, dim=1)\n",
    "                multi_uncertainty[malicious_mask] = multi_unc\n",
    "            \n",
    "            return {\n",
    "                'binary': {\n",
    "                    'predictions': binary_preds.cpu().numpy(),\n",
    "                    'probabilities': binary_probs.cpu().numpy(),\n",
    "                    'uncertainty': binary_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['binary']\n",
    "                },\n",
    "                'multi': {\n",
    "                    'probabilities': multi_probs.cpu().numpy(),\n",
    "                    'uncertainty': multi_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['multi']\n",
    "                }\n",
    "            }\n",
    "'''\n",
    "model_loader_path = os.path.join(model_package_dir, \"model_loader.py\")\n",
    "with open(model_loader_path, 'w') as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "# 8. 验证模型包完整性\n",
    "print(\"\\n📋 验证模型包...\")\n",
    "required_files = [\n",
    "    \"scaler.pkl\", \"label_encoder.pkl\", \"feature_selector.pkl\",\n",
    "    \"selected_features.json\", \"model.pth\", \"model_info.json\", \"model_loader.py\"\n",
    "]\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(model_package_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} - 缺失\")\n",
    "\n",
    "print(f\"\\n🎉 模型包已生成于: {model_package_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9c3f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T09:59:10.192791Z",
     "iopub.status.busy": "2025-08-24T09:59:10.192491Z",
     "iopub.status.idle": "2025-08-24T09:59:10.279254Z",
     "shell.execute_reply": "2025-08-24T09:59:10.278307Z"
    },
    "papermill": {
     "duration": 1.764043,
     "end_time": "2025-08-24T09:59:10.280604",
     "exception": false,
     "start_time": "2025-08-24T09:59:08.516561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 开始模型打包...\n",
      "✅ 变量检查通过\n",
      "📦 保存模型组件...\n",
      "📋 验证模型包...\n",
      "  ✅ scaler.pkl\n",
      "  ✅ label_encoder.pkl\n",
      "  ✅ feature_selector.pkl\n",
      "  ✅ selected_features.json\n",
      "  ✅ model.pth\n",
      "  ✅ model_info.json\n",
      "  ✅ model_loader.py\n",
      "  ✅ test_model.py\n",
      "\n",
      "🎉 模型包生成完成!\n",
      "📍 位置: /kaggle/working/model_package\n",
      "📊 大小: 14.4MB\n",
      "🧪 运行测试: cd /kaggle/working/model_package && python test_model.py\n",
      "\n",
      "🚀 集成示例:\n",
      "from model_loader import HierarchicalTransformerIDSLoader\n",
      "loader = HierarchicalTransformerIDSLoader('/path/to/model_package')\n",
      "results = loader.predict(data)\n",
      "\n",
      "✨ 打包完成！\n"
     ]
    }
   ],
   "source": [
    "# =====================================================================\n",
    "# 模型打包和后端集成准备 - 简化版\n",
    "# =====================================================================\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔧 开始模型打包...\")\n",
    "\n",
    "# 变量检查\n",
    "required_vars = ['X', 'scaler_multi', 'le_binary', 'le_multi_subset', 'class_names_subset', \n",
    "                'label_mapping_info', 'tier1_classes', 'tier2_classes', 'best_binary_path', \n",
    "                'best_multi_path', 'binary_results', 'multi_results']\n",
    "\n",
    "try:\n",
    "    for var in required_vars:\n",
    "        assert var in locals(), f\"{var} 变量不存在\"\n",
    "    print(\"✅ 变量检查通过\")\n",
    "except AssertionError as e:\n",
    "    print(f\"❌ {e}\")\n",
    "    print(\"请确保在完整训练流程后运行此代码\")\n",
    "    exit()\n",
    "\n",
    "# 创建模型包目录\n",
    "model_package_dir = \"/kaggle/working/model_package\"\n",
    "os.makedirs(model_package_dir, exist_ok=True)\n",
    "\n",
    "print(\"📦 保存模型组件...\")\n",
    "\n",
    "# 1. 特征缩放器\n",
    "with open(os.path.join(model_package_dir, \"scaler.pkl\"), 'wb') as f:\n",
    "    pickle.dump(scaler_multi, f)\n",
    "\n",
    "# 2. 标签编码器\n",
    "label_encoders = {\n",
    "    'binary_encoder': le_binary,\n",
    "    'multi_encoder': le_multi_subset,\n",
    "    'binary_classes': le_binary.classes_.tolist(),\n",
    "    'multi_classes': class_names_subset,\n",
    "    'label_mapping': label_mapping_info\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"label_encoder.pkl\"), 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "\n",
    "# 3. 特征选择器\n",
    "feature_selector = {\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'selected_features': X.columns.tolist(),\n",
    "    'feature_count': len(X.columns),\n",
    "    'selection_method': 'all_features'\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"feature_selector.pkl\"), 'wb') as f:\n",
    "    pickle.dump(feature_selector, f)\n",
    "\n",
    "# 4. 特征列表\n",
    "selected_features = {\n",
    "    'features': X.columns.tolist(),\n",
    "    'count': len(X.columns),\n",
    "    'creation_date': datetime.now().isoformat()\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"selected_features.json\"), 'w') as f:\n",
    "    json.dump(selected_features, f, indent=2)\n",
    "\n",
    "# 5. 模型权重\n",
    "model_state = {\n",
    "    'binary_model_state': torch.load(best_binary_path, map_location='cpu'),\n",
    "    'multi_model_state': torch.load(best_multi_path, map_location='cpu'),\n",
    "    'model_architecture': {\n",
    "        'input_dim': X.shape[1],\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    }\n",
    "}\n",
    "torch.save(model_state, os.path.join(model_package_dir, \"model.pth\"))\n",
    "\n",
    "# 6. 模型信息\n",
    "model_info = {\n",
    "    'model_name': 'HierarchicalTransformerIDS',\n",
    "    'model_version': '1.0.0',\n",
    "    'created_date': datetime.now().isoformat(),\n",
    "    'architecture': {\n",
    "        'input_features': len(X.columns),\n",
    "        'binary_classes': 2,\n",
    "        'multi_classes': len(class_names_subset),\n",
    "        'dropout_rate': 0.3\n",
    "    },\n",
    "    'classes': {\n",
    "        'binary': le_binary.classes_.tolist(),\n",
    "        'multi': class_names_subset,\n",
    "        'tier1_critical': [class_names_subset[i] for i in tier1_classes],\n",
    "        'tier2_minority': [class_names_subset[i] for i in tier2_classes]\n",
    "    },\n",
    "    'performance': {\n",
    "        'binary_stage': {\n",
    "            'accuracy': float(binary_results['accuracy']),\n",
    "            'macro_f1': float(binary_results['macro_f1']),\n",
    "            'weighted_f1': float(binary_results.get('weighted_f1', 0))\n",
    "        },\n",
    "        'multi_stage': {\n",
    "            'accuracy': float(multi_results['accuracy']),\n",
    "            'macro_f1': float(multi_results['macro_f1']),\n",
    "            'weighted_f1': float(multi_results.get('weighted_f1', 0))\n",
    "        }\n",
    "    },\n",
    "    'features': {\n",
    "        'total_features': len(X.columns),\n",
    "        'feature_names': X.columns.tolist(),\n",
    "        'preprocessing': 'StandardScaler + Outlier Clipping'\n",
    "    },\n",
    "    'deployment': {\n",
    "        'input_format': 'numpy array of shape (batch_size, n_features)',\n",
    "        'requires_preprocessing': True,\n",
    "        'batch_inference': True\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(model_package_dir, \"model_info.json\"), 'w') as f:\n",
    "    json.dump(model_info, f, indent=2, default=str)\n",
    "\n",
    "# 7. 模型加载器\n",
    "model_loader_code = '''import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MultiScaleAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.class_attention = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(input_dim, input_dim//4), nn.ReLU(), nn.Dropout(0.1),\n",
    "                         nn.Linear(input_dim//4, input_dim), nn.Sigmoid())\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "        self.temporal_attention = nn.MultiheadAttention(input_dim, num_heads=4, dropout=0.1, batch_first=True)\n",
    "        self.spatial_attention = nn.Sequential(nn.Linear(input_dim, input_dim//8), nn.ReLU(), nn.Dropout(0.1),\n",
    "                                             nn.Linear(input_dim//8, input_dim), nn.Sigmoid())\n",
    "        self.fusion_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 类别注意力\n",
    "        class_features = [self.class_attention[i](x) * x for i in range(self.num_classes)]\n",
    "        class_attended = torch.stack(class_features, dim=1)\n",
    "        # 时间注意力\n",
    "        x_temporal = x.unsqueeze(1)\n",
    "        temporal_attended, _ = self.temporal_attention(x_temporal, x_temporal, x_temporal)\n",
    "        temporal_attended = temporal_attended.squeeze(1)\n",
    "        # 空间注意力\n",
    "        spatial_attended = self.spatial_attention(x) * x\n",
    "        # 融合\n",
    "        weights = F.softmax(self.fusion_weights, dim=0)\n",
    "        class_attended_mean = class_attended.mean(dim=1)\n",
    "        fused = weights[0] * class_attended_mean + weights[1] * temporal_attended + weights[2] * spatial_attended\n",
    "        return class_attended, fused\n",
    "\n",
    "class TransformerEnhancedEnsembleModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        # 编码器\n",
    "        self.shared_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate/2)\n",
    "        )\n",
    "        # Transformer\n",
    "        self.feature_dim = 256\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.feature_dim, nhead=8, dim_feedforward=512,\n",
    "                                                 dropout=0.1, batch_first=True, activation='gelu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1, self.feature_dim) * 0.1)\n",
    "        # 注意力机制\n",
    "        self.multi_scale_attention = MultiScaleAttention(256, num_classes)\n",
    "        # 分类头\n",
    "        self.class_specific_heads = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "                         nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "            for _ in range(num_classes)\n",
    "        ])\n",
    "        self.global_classifier = nn.Sequential(nn.Linear(256, 128), nn.LayerNorm(128), nn.GELU(),\n",
    "                                             nn.Dropout(dropout_rate), nn.Linear(128, num_classes))\n",
    "        self.fusion_network = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 2), nn.Softmax(dim=-1))\n",
    "        self.uncertainty_head = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, x, return_uncertainty=False):\n",
    "        # 特征提取\n",
    "        shared_features = self.shared_encoder(x)\n",
    "        # Transformer处理\n",
    "        transformer_input = shared_features.unsqueeze(1) + self.pos_encoding\n",
    "        transformer_features = self.transformer_encoder(transformer_input).squeeze(1)\n",
    "        enhanced_features = F.layer_norm(shared_features + transformer_features, normalized_shape=[self.feature_dim])\n",
    "        # 多尺度注意力\n",
    "        class_attended_features, fused_attention_features = self.multi_scale_attention(enhanced_features)\n",
    "        # 分类\n",
    "        class_outputs = [self.class_specific_heads[i](class_attended_features[:, i, :]) for i in range(self.num_classes)]\n",
    "        class_logits = torch.cat(class_outputs, dim=1)\n",
    "        global_logits = self.global_classifier(fused_attention_features)\n",
    "        fusion_weights = self.fusion_network(enhanced_features)\n",
    "        final_logits = fusion_weights[:, 0:1] * class_logits + fusion_weights[:, 1:2] * global_logits\n",
    "        \n",
    "        if return_uncertainty:\n",
    "            return final_logits, self.uncertainty_head(enhanced_features)\n",
    "        return final_logits\n",
    "\n",
    "class HierarchicalTransformerIDSLoader:\n",
    "    def __init__(self, model_package_path: str):\n",
    "        self.model_package_path = model_package_path\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self._load_components()\n",
    "    \n",
    "    def _load_components(self):\n",
    "        # 加载配置和组件\n",
    "        with open(os.path.join(self.model_package_path, \"model_info.json\"), 'r') as f:\n",
    "            self.model_info = json.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"scaler.pkl\"), 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        with open(os.path.join(self.model_package_path, \"label_encoder.pkl\"), 'rb') as f:\n",
    "            self.label_encoders = pickle.load(f)\n",
    "        \n",
    "        # 加载模型\n",
    "        model_checkpoint = torch.load(os.path.join(self.model_package_path, \"model.pth\"), map_location=self.device)\n",
    "        arch_info = self.model_info['architecture']\n",
    "        \n",
    "        self.binary_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['binary_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']).to(self.device)\n",
    "        self.multi_model = TransformerEnhancedEnsembleModel(\n",
    "            input_dim=arch_info['input_features'], num_classes=arch_info['multi_classes'],\n",
    "            dropout_rate=arch_info['dropout_rate']).to(self.device)\n",
    "        \n",
    "        self.binary_model.load_state_dict(model_checkpoint['binary_model_state'])\n",
    "        self.multi_model.load_state_dict(model_checkpoint['multi_model_state'])\n",
    "        self.binary_model.eval()\n",
    "        self.multi_model.eval()\n",
    "    \n",
    "    def preprocess(self, data: np.ndarray) -> torch.Tensor:\n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        return torch.from_numpy(scaled_data).float().to(self.device)\n",
    "    \n",
    "    def predict(self, data: np.ndarray):\n",
    "        with torch.no_grad():\n",
    "            processed_data = self.preprocess(data)\n",
    "            \n",
    "            # 二分类预测\n",
    "            binary_logits, binary_uncertainty = self.binary_model(processed_data, return_uncertainty=True)\n",
    "            binary_probs = F.softmax(binary_logits, dim=1)\n",
    "            binary_preds = torch.argmax(binary_probs, dim=1)\n",
    "            \n",
    "            # 多分类预测（仅恶意流量）\n",
    "            multi_probs = torch.zeros(len(data), len(self.model_info['classes']['multi']))\n",
    "            multi_uncertainty = torch.zeros(len(data), 1)\n",
    "            \n",
    "            malicious_mask = binary_preds == 1\n",
    "            if malicious_mask.sum() > 0:\n",
    "                malicious_data = processed_data[malicious_mask]\n",
    "                multi_logits, multi_unc = self.multi_model(malicious_data, return_uncertainty=True)\n",
    "                multi_probs[malicious_mask] = F.softmax(multi_logits, dim=1)\n",
    "                multi_uncertainty[malicious_mask] = multi_unc\n",
    "            \n",
    "            return {\n",
    "                'binary': {\n",
    "                    'predictions': binary_preds.cpu().numpy(),\n",
    "                    'probabilities': binary_probs.cpu().numpy(),\n",
    "                    'uncertainty': binary_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['binary']\n",
    "                },\n",
    "                'multi': {\n",
    "                    'probabilities': multi_probs.cpu().numpy(),\n",
    "                    'uncertainty': multi_uncertainty.cpu().numpy(),\n",
    "                    'classes': self.model_info['classes']['multi']\n",
    "                },\n",
    "                'metadata': {\n",
    "                    'model_name': self.model_info['model_name'],\n",
    "                    'num_samples': len(data)\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    def get_model_info(self):\n",
    "        return self.model_info\n",
    "\n",
    "# 使用示例\n",
    "# loader = HierarchicalTransformerIDSLoader(\"/path/to/model_package\")\n",
    "# results = loader.predict(network_traffic_data)\n",
    "'''\n",
    "\n",
    "with open(os.path.join(model_package_dir, \"model_loader.py\"), 'w') as f:\n",
    "    f.write(model_loader_code)\n",
    "\n",
    "# 8. 创建快速测试脚本\n",
    "test_script = f'''import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "from model_loader import HierarchicalTransformerIDSLoader\n",
    "import numpy as np\n",
    "\n",
    "def test_model():\n",
    "    try:\n",
    "        print(\"🔄 测试模型加载...\")\n",
    "        loader = HierarchicalTransformerIDSLoader(\".\")\n",
    "        print(\"✅ 模型加载成功\")\n",
    "        \n",
    "        print(\"🔄 测试预测...\")\n",
    "        sample_data = np.random.randn(3, {len(X.columns)})\n",
    "        results = loader.predict(sample_data)\n",
    "        print(\"✅ 预测成功\")\n",
    "        print(f\"二分类形状: {{results['binary']['probabilities'].shape}}\")\n",
    "        print(f\"多分类形状: {{results['multi']['probabilities'].shape}}\")\n",
    "        \n",
    "        model_info = loader.get_model_info()\n",
    "        print(f\"\\\\n📋 模型: {{model_info['model_name']}} v{{model_info['model_version']}}\")\n",
    "        print(f\"性能: 二分类F1={model_info['performance']['binary_stage']['macro_f1']:.3f}, \"\n",
    "              f\"多分类F1={model_info['performance']['multi_stage']['macro_f1']:.3f}\")\n",
    "        print(\"\\\\n🎉 测试通过！\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 测试失败: {{e}}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = test_model()\n",
    "    sys.exit(0 if success else 1)\n",
    "'''\n",
    "\n",
    "with open(os.path.join(model_package_dir, \"test_model.py\"), 'w') as f:\n",
    "    f.write(test_script)\n",
    "\n",
    "# 9. 验证完整性\n",
    "print(\"📋 验证模型包...\")\n",
    "required_files = [\"scaler.pkl\", \"label_encoder.pkl\", \"feature_selector.pkl\", \n",
    "                  \"selected_features.json\", \"model.pth\", \"model_info.json\", \n",
    "                  \"model_loader.py\", \"test_model.py\"]\n",
    "\n",
    "all_good = True\n",
    "for file in required_files:\n",
    "    if os.path.exists(os.path.join(model_package_dir, file)):\n",
    "        print(f\"  ✅ {file}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {file} - 缺失\")\n",
    "        all_good = False\n",
    "\n",
    "# 10. 总结\n",
    "if all_good:\n",
    "    total_size = sum(os.path.getsize(os.path.join(model_package_dir, f)) \n",
    "                    for f in os.listdir(model_package_dir)) / (1024*1024)\n",
    "    print(f\"\\n🎉 模型包生成完成!\")\n",
    "    print(f\"📍 位置: {model_package_dir}\")\n",
    "    print(f\"📊 大小: {total_size:.1f}MB\")\n",
    "    print(f\"🧪 运行测试: cd {model_package_dir} && python test_model.py\")\n",
    "    print(f\"\\n🚀 集成示例:\")\n",
    "    print(f\"from model_loader import HierarchicalTransformerIDSLoader\")\n",
    "    print(f\"loader = HierarchicalTransformerIDSLoader('/path/to/model_package')\")\n",
    "    print(f\"results = loader.predict(data)\")\n",
    "else:\n",
    "    print(\"\\n❌ 模型包生成不完整，请检查错误信息\")\n",
    "\n",
    "print(\"\\n✨ 打包完成！\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 897.24506,
   "end_time": "2025-08-24T09:59:14.629044",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-24T09:44:17.383984",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
