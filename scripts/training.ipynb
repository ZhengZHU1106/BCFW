{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-12T13:37:21.631234Z",
     "iopub.status.busy": "2025-07-12T13:37:21.630916Z",
     "iopub.status.idle": "2025-07-12T14:09:54.377529Z",
     "shell.execute_reply": "2025-07-12T14:09:54.376445Z",
     "shell.execute_reply.started": "2025-07-12T13:37:21.631195Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¾è·¯å¾‘è®€å–æ•¸æ“š: /kaggle/input/cicids2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¼‰å…¥ Parquet æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00,  8.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•¸æ“šè¼‰å…¥å®Œæˆï¼ŒåŸå§‹å½¢ç‹€: (2313810, 78)\n",
      "\n",
      "--- æ­£åœ¨é€²è¡Œæ•¸æ“šæ¸…æ´— ---\n",
      "ç§»é™¤äº† 84674 ç­†é‡è¤‡è¨˜éŒ„ã€‚\n",
      "å°‡ 4 å€‹ç¨€æœ‰é¡åˆ¥åˆä½µç‚º 'Rare_Attack'\n",
      "\n",
      "--- æ­£åœ¨ç§»é™¤ä½æ–¹å·®/å¸¸æ•¸ç‰¹å¾µ ---\n",
      "ç§»é™¤ä½æ–¹å·®ç‰¹å¾µå¾Œï¼Œå‰©é¤˜ 65 å€‹ç‰¹å¾µã€‚\n",
      "\n",
      "ç¸½å…±æœ‰ 12 å€‹é¡åˆ¥\n",
      "\n",
      "æˆåŠŸé¸å–äº† 64 å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚\n",
      "\n",
      "æ‰€æœ‰æ•¸æ“šé è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹å·²å®Œæˆï¼\n",
      "\n",
      "ä½¿ç”¨è¨­å‚™: cuda\n",
      "æ¨¡å‹ç¸½åƒæ•¸æ•¸é‡: 351,130\n",
      "\n",
      "--- é–‹å§‹è¨“ç·´ Ensemble_Hybrid æ¨¡å‹ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | è¨“ç·´æå¤±: 0.6440 | é©—è­‰æå¤±: 0.5526 | é©—è­‰æº–ç¢ºç‡: 99.17%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | è¨“ç·´æå¤±: 0.5721 | é©—è­‰æå¤±: 0.5466 | é©—è­‰æº–ç¢ºç‡: 99.24%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | è¨“ç·´æå¤±: 0.5621 | é©—è­‰æå¤±: 0.5429 | é©—è­‰æº–ç¢ºç‡: 99.43%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | è¨“ç·´æå¤±: 0.5565 | é©—è­‰æå¤±: 0.5414 | é©—è­‰æº–ç¢ºç‡: 99.48%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | è¨“ç·´æå¤±: 0.5519 | é©—è­‰æå¤±: 0.5401 | é©—è­‰æº–ç¢ºç‡: 99.49%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | è¨“ç·´æå¤±: 0.5497 | é©—è­‰æå¤±: 0.5395 | é©—è­‰æº–ç¢ºç‡: 99.51%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | è¨“ç·´æå¤±: 0.5482 | é©—è­‰æå¤±: 0.5390 | é©—è­‰æº–ç¢ºç‡: 99.52%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | è¨“ç·´æå¤±: 0.5471 | é©—è­‰æå¤±: 0.5385 | é©—è­‰æº–ç¢ºç‡: 99.53%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | è¨“ç·´æå¤±: 0.5463 | é©—è­‰æå¤±: 0.5383 | é©—è­‰æº–ç¢ºç‡: 99.51%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | è¨“ç·´æå¤±: 0.5455 | é©—è­‰æå¤±: 0.5378 | é©—è­‰æº–ç¢ºç‡: 99.56%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | è¨“ç·´æå¤±: 0.5411 | é©—è­‰æå¤±: 0.5376 | é©—è­‰æº–ç¢ºç‡: 99.55%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | è¨“ç·´æå¤±: 0.5401 | é©—è­‰æå¤±: 0.5373 | é©—è­‰æº–ç¢ºç‡: 99.57%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | è¨“ç·´æå¤±: 0.5396 | é©—è­‰æå¤±: 0.5383 | é©—è­‰æº–ç¢ºç‡: 99.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | è¨“ç·´æå¤±: 0.5392 | é©—è­‰æå¤±: 0.5369 | é©—è­‰æº–ç¢ºç‡: 99.57%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | è¨“ç·´æå¤±: 0.5388 | é©—è­‰æå¤±: 0.5369 | é©—è­‰æº–ç¢ºç‡: 99.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | è¨“ç·´æå¤±: 0.5385 | é©—è­‰æå¤±: 0.5366 | é©—è­‰æº–ç¢ºç‡: 99.58%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | è¨“ç·´æå¤±: 0.5383 | é©—è­‰æå¤±: 0.5367 | é©—è­‰æº–ç¢ºç‡: 99.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | è¨“ç·´æå¤±: 0.5381 | é©—è­‰æå¤±: 0.5364 | é©—è­‰æº–ç¢ºç‡: 99.61%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | è¨“ç·´æå¤±: 0.5378 | é©—è­‰æå¤±: 0.5363 | é©—è­‰æº–ç¢ºç‡: 99.61%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | è¨“ç·´æå¤±: 0.5378 | é©—è­‰æå¤±: 0.5362 | é©—è­‰æº–ç¢ºç‡: 99.62%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | è¨“ç·´æå¤±: 0.5376 | é©—è­‰æå¤±: 0.5360 | é©—è­‰æº–ç¢ºç‡: 99.62%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | è¨“ç·´æå¤±: 0.5375 | é©—è­‰æå¤±: 0.5362 | é©—è­‰æº–ç¢ºç‡: 99.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | è¨“ç·´æå¤±: 0.5373 | é©—è­‰æå¤±: 0.5359 | é©—è­‰æº–ç¢ºç‡: 99.63%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | è¨“ç·´æå¤±: 0.5372 | é©—è­‰æå¤±: 0.5359 | é©—è­‰æº–ç¢ºç‡: 99.61%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | è¨“ç·´æå¤±: 0.5371 | é©—è­‰æå¤±: 0.5358 | é©—è­‰æº–ç¢ºç‡: 99.63%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | è¨“ç·´æå¤±: 0.5370 | é©—è­‰æå¤±: 0.5357 | é©—è­‰æº–ç¢ºç‡: 99.64%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | è¨“ç·´æå¤±: 0.5369 | é©—è­‰æå¤±: 0.5356 | é©—è­‰æº–ç¢ºç‡: 99.64%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | è¨“ç·´æå¤±: 0.5368 | é©—è­‰æå¤±: 0.5355 | é©—è­‰æº–ç¢ºç‡: 99.63%\n",
      "  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | è¨“ç·´æå¤±: 0.5367 | é©—è­‰æå¤±: 0.5357 | é©—è­‰æº–ç¢ºç‡: 99.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | è¨“ç·´æå¤±: 0.5366 | é©—è­‰æå¤±: 0.5360 | é©—è­‰æº–ç¢ºç‡: 99.59%\n",
      "\n",
      "è¨“ç·´å®Œæˆï¼\n",
      "\n",
      "--- è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œæœ€çµ‚è©•ä¼° ---\n",
      "æˆåŠŸè¼‰å…¥ä¾†è‡ª Epoch 28 çš„æœ€ä½³æ¨¡å‹ã€‚\n",
      "\n",
      "--- æ­£åœ¨è©•ä¼° Ensemble_Hybrid æ¨¡å‹ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è©•ä¼°æ¸¬è©¦é›†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 871/871 [00:06<00:00, 144.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ --- æœ€çµ‚è©•ä¼°å ±å‘Š (æ¸¬è©¦é›†) --- ğŸ‰ğŸ‰ğŸ‰\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 Benign     0.9978    0.9979    0.9979    378533\n",
      "                    Bot     0.6020    0.8432    0.7025       287\n",
      "                   DDoS     0.9983    0.9991    0.9987     25603\n",
      "          DoS GoldenEye     0.9898    0.9893    0.9895      2054\n",
      "               DoS Hulk     0.9874    0.9918    0.9896     34569\n",
      "       DoS Slowhttptest     0.8907    0.9895    0.9375      1046\n",
      "          DoS slowloris     0.9861    0.9870    0.9865      1077\n",
      "            FTP-Patator     0.9899    0.9933    0.9916      1186\n",
      "               PortScan     0.9248    0.9437    0.9342       391\n",
      "            Rare_Attack     1.0000    0.0278    0.0541       144\n",
      "            SSH-Patator     0.9983    0.9208    0.9580       644\n",
      "Web Attack  Brute Force     1.0000    0.0986    0.1796       294\n",
      "\n",
      "               accuracy                         0.9962    445828\n",
      "              macro avg     0.9471    0.8152    0.8100    445828\n",
      "           weighted avg     0.9964    0.9962    0.9959    445828\n",
      "\n",
      "\n",
      "--- æ­£åœ¨ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶åŒ…è‡³ /kaggle/working/ensemble_hybrid_model_package/ ---\n",
      "âœ“ å·²ä¿å­˜ model.pth\n",
      "âœ“ å·²ä¿å­˜ scaler.pkl\n",
      "âœ“ å·²ä¿å­˜ label_encoder.pkl\n",
      "âœ“ å·²ä¿å­˜ feature_selector.pkl\n",
      "âœ“ å·²ä¿å­˜ selected_features.json\n",
      "âœ“ å·²ä¿å­˜ model_info.json\n",
      "\n",
      "âœ… è¨“ç·´èˆ‡æ¨¡å‹æ–‡ä»¶æ‰“åŒ…å…¨éƒ¨å®Œæˆï¼\n",
      "\n",
      "ğŸ‰ è…³æœ¬åŸ·è¡ŒæˆåŠŸï¼\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# Ensemble_Hybrid å®Œæ•´ PyTorch è¨“ç·´èˆ‡éƒ¨ç½²æ–‡ä»¶æ‰“åŒ…è…³æœ¬\n",
    "#\n",
    "# çµåˆäº† Ensemble_Hybrid çš„é«˜æ€§èƒ½æ¨¡å‹èˆ‡ç”Ÿç”¢ç´šçš„æ¨¡å‹æ–‡ä»¶æ‰“åŒ…åŠŸèƒ½ã€‚\n",
    "#\n",
    "# ä¸»è¦ç‰¹é»:\n",
    "# 1. ä½¿ç”¨ Ensemble_Hybrid è¤‡é›œæ¨¡å‹é€²è¡Œè¨“ç·´ã€‚\n",
    "# 2. åŒ…å«å®Œæ•´çš„æ•¸æ“šæ¸…æ´—ã€ç‰¹å¾µå·¥ç¨‹èˆ‡æ¨¡å‹ç©©å®šæ€§ä¿®å¾©ã€‚\n",
    "# 3. åœ¨è¨“ç·´çµæŸå¾Œï¼Œè‡ªå‹•æ‰“åŒ…æ‰€æœ‰éƒ¨ç½²æ‰€éœ€æ–‡ä»¶ï¼š\n",
    "#    - model.pth (æ¨¡å‹æ¬Šé‡)\n",
    "#    - scaler.pkl (æ•¸æ“šç¸®æ”¾å™¨)\n",
    "#    - label_encoder.pkl (æ¨™ç±¤ç·¨ç¢¼å™¨)\n",
    "#    - feature_selector.pkl (ç‰¹å¾µé¸æ“‡å™¨)\n",
    "#    - selected_features.json (æ‰€é¸ç‰¹å¾µåˆ—è¡¨)\n",
    "#    - model_info.json (æ¨¡å‹å…ƒæ•¸æ“š)\n",
    "# ===================================================================\n",
    "\n",
    "# 1. å°å…¥å¿…è¦çš„åº«\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.amp\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# å¼•å…¥ sklearn ç›¸é—œå‡½å¼åº«\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# å¼•å…¥ tqdm å‡½å¼åº«ä¾†é¡¯ç¤ºé€²åº¦æ¢\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===================================================================\n",
    "# 2. æ¨¡å‹çµ„ä»¶å®šç¾©\n",
    "# ===================================================================\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"æ®˜å·®å¡Š - æ”¹å–„æ¢¯åº¦æµå‹•\"\"\"\n",
    "    def __init__(self, dim, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.BatchNorm1d(dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.block(x)\n",
    "        out = out + residual\n",
    "        return F.relu(self.dropout(out))\n",
    "\n",
    "class SelfAttentionBranch(nn.Module):\n",
    "    \"\"\"è‡ªæ³¨æ„åŠ›åˆ†æ”¯\"\"\"\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.attention_dim = min(64, input_dim)\n",
    "        self.query = nn.Linear(input_dim, self.attention_dim)\n",
    "        self.key = nn.Linear(input_dim, self.attention_dim)\n",
    "        self.value = nn.Linear(input_dim, self.attention_dim)\n",
    "        self.output_projection = nn.Linear(self.attention_dim, input_dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        attention_scores = torch.sum(q * k, dim=1, keepdim=True)\n",
    "        attention_weights = torch.sigmoid(attention_scores)\n",
    "        attention_weights = torch.clamp(attention_weights, min=1e-8, max=1.0)\n",
    "        attended = attention_weights * v\n",
    "        projected = self.output_projection(attended)\n",
    "        return self.classifier(projected)\n",
    "\n",
    "class FeatureInteractionBranch(nn.Module):\n",
    "    \"\"\"ç‰¹å¾µäº¤äº’åˆ†æ”¯\"\"\"\n",
    "    def __init__(self, input_dim, num_classes, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.interaction_dim = min(8, input_dim // 8)\n",
    "        self.feature_embeddings = nn.Linear(input_dim, self.interaction_dim)\n",
    "        self.interaction_output_dim = (self.interaction_dim * (self.interaction_dim - 1)) // 2\n",
    "        if self.interaction_output_dim == 0:\n",
    "            self.interaction_output_dim = 1\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim + self.interaction_output_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeddings = torch.tanh(self.feature_embeddings(x))\n",
    "        interactions = []\n",
    "        if self.interaction_dim > 1:\n",
    "            for i in range(self.interaction_dim):\n",
    "                for j in range(i + 1, self.interaction_dim):\n",
    "                    interaction = embeddings[:, i] * embeddings[:, j]\n",
    "                    interactions.append(interaction.unsqueeze(1))\n",
    "        \n",
    "        if interactions:\n",
    "            interaction_features = torch.cat(interactions, dim=1)\n",
    "        else:\n",
    "            interaction_features = torch.zeros(x.size(0), 1, device=x.device)\n",
    "        \n",
    "        combined_features = torch.cat([x, interaction_features], dim=1)\n",
    "        return self.classifier(combined_features)\n",
    "\n",
    "class Ensemble_Hybrid(nn.Module):\n",
    "    \"\"\"é›†æˆæ··åˆç¶²çµ¡\"\"\"\n",
    "    def __init__(self, input_dim, num_classes=15, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.deep_branch = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.wide_branch = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.res_branch = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128), nn.BatchNorm1d(128), nn.ReLU(),\n",
    "            ResidualBlock(128, dropout_rate),\n",
    "            ResidualBlock(128, dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.attention_branch = SelfAttentionBranch(input_dim, num_classes, dropout_rate)\n",
    "        self.interaction_branch = FeatureInteractionBranch(input_dim, num_classes, dropout_rate)\n",
    "        \n",
    "        self.weight_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 32), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(32, 5), nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.final_fusion = nn.Sequential(\n",
    "            nn.Linear(num_classes * 5, 128), nn.BatchNorm1d(128), nn.ReLU(), nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        self.global_weights = nn.Parameter(torch.ones(5) / 5)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None: nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.BatchNorm1d):\n",
    "                nn.init.constant_(module.weight, 1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        if torch.isnan(x).any() or torch.isinf(x).any():\n",
    "            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        outputs = [\n",
    "            self.deep_branch(x), self.wide_branch(x), self.res_branch(x),\n",
    "            self.attention_branch(x), self.interaction_branch(x)\n",
    "        ]\n",
    "        \n",
    "        for i, out in enumerate(outputs):\n",
    "            if torch.isnan(out).any() or torch.isinf(out).any():\n",
    "                outputs[i] = torch.nan_to_num(out, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        deep_out, wide_out, res_out, att_out, inter_out = outputs\n",
    "        \n",
    "        adaptive_weights = torch.clamp(self.weight_net(x), min=1e-8, max=1.0)\n",
    "        global_weights = torch.clamp(F.softmax(self.global_weights, dim=0), min=1e-8, max=1.0)\n",
    "        \n",
    "        outputs_stack = torch.stack(outputs, dim=2)\n",
    "        \n",
    "        weighted_output_adaptive = torch.sum(outputs_stack * adaptive_weights.unsqueeze(1), dim=2)\n",
    "        weighted_output_global = torch.sum(outputs_stack * global_weights.unsqueeze(0).unsqueeze(0), dim=2)\n",
    "        weighted_output = 0.6 * weighted_output_adaptive + 0.4 * weighted_output_global\n",
    "        \n",
    "        concatenated = torch.cat(outputs, dim=1)\n",
    "        final_output = self.final_fusion(concatenated)\n",
    "        \n",
    "        ensemble_output = 0.7 * final_output + 0.3 * weighted_output\n",
    "        \n",
    "        if torch.isnan(ensemble_output).any() or torch.isinf(ensemble_output).any():\n",
    "            ensemble_output = torch.nan_to_num(ensemble_output, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "            \n",
    "        if return_intermediate:\n",
    "            return {\n",
    "                'ensemble': ensemble_output, 'final_fusion': final_output,\n",
    "                'weighted': weighted_output, 'branches': tuple(outputs),\n",
    "                'weights': (adaptive_weights, global_weights)\n",
    "            }\n",
    "        return ensemble_output\n",
    "\n",
    "# ===================================================================\n",
    "# 3. æå¤±å‡½æ•¸\n",
    "# ===================================================================\n",
    "\n",
    "class EnsembleLoss(nn.Module):\n",
    "    \"\"\"é›†æˆæå¤±å‡½æ•¸\"\"\"\n",
    "    def __init__(self, num_classes, alpha=0.7, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        if isinstance(outputs, dict):\n",
    "            main_loss = self.criterion(outputs['ensemble'], targets)\n",
    "            \n",
    "            branch_losses = []\n",
    "            for branch_out in outputs['branches']:\n",
    "                if not (torch.isnan(branch_out).any() or torch.isinf(branch_out).any()):\n",
    "                    branch_losses.append(self.criterion(branch_out, targets))\n",
    "            \n",
    "            auxiliary_loss = torch.mean(torch.stack(branch_losses)) if branch_losses else torch.tensor(0.0, device=targets.device)\n",
    "            fusion_loss = self.criterion(outputs['final_fusion'], targets)\n",
    "            \n",
    "            if torch.isnan(main_loss): main_loss = torch.tensor(0.0, device=targets.device)\n",
    "            if torch.isnan(auxiliary_loss): auxiliary_loss = torch.tensor(0.0, device=targets.device)\n",
    "            if torch.isnan(fusion_loss): fusion_loss = torch.tensor(0.0, device=targets.device)\n",
    "            \n",
    "            total_loss = (self.alpha * main_loss + \n",
    "                          (1 - self.alpha) * 0.7 * auxiliary_loss + \n",
    "                          (1 - self.alpha) * 0.3 * fusion_loss)\n",
    "            return total_loss\n",
    "        else:\n",
    "            return self.criterion(outputs, targets)\n",
    "\n",
    "# ===================================================================\n",
    "# 4. è¨“ç·´èˆ‡è©•ä¼°å‡½æ•¸\n",
    "# ===================================================================\n",
    "\n",
    "def train_ensemble_model(model, train_loader, val_loader, device, num_epochs, patience, use_multi_gpu):\n",
    "    \"\"\"è¨“ç·´å‡½æ•¸\"\"\"\n",
    "    print(\"\\n--- é–‹å§‹è¨“ç·´ Ensemble_Hybrid æ¨¡å‹ ---\")\n",
    "    actual_model = model.module if use_multi_gpu else model\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3)\n",
    "    criterion = EnsembleLoss(num_classes=actual_model.num_classes, label_smoothing=0.1)\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    model_output_dir = \"/kaggle/working/ensemble_hybrid_model_checkpoint\"\n",
    "    os.makedirs(model_output_dir, exist_ok=True)\n",
    "    best_model_path = os.path.join(model_output_dir, \"best_ensemble_model.pth\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [è¨“ç·´]\", leave=False)\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_pbar):\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            if torch.isnan(inputs).any():\n",
    "                inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs, return_intermediate=(epoch < 10))\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            if torch.isnan(loss): continue\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            train_pbar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                if torch.isnan(inputs).any():\n",
    "                    inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "                outputs = model(inputs)\n",
    "                if isinstance(outputs, dict): outputs = outputs['ensemble']\n",
    "                if torch.isnan(outputs).any(): continue\n",
    "                    \n",
    "                loss = criterion(outputs, labels)\n",
    "                if not torch.isnan(loss): total_val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader) if len(val_loader) > 0 else float('inf')\n",
    "        val_accuracy = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | è¨“ç·´æå¤±: {avg_train_loss:.4f} | é©—è­‰æå¤±: {avg_val_loss:.4f} | é©—è­‰æº–ç¢ºç‡: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        if avg_val_loss < best_val_loss and not np.isnan(avg_val_loss):\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            model_state_dict = model.module.state_dict() if use_multi_gpu else model.state_dict()\n",
    "            torch.save({\n",
    "                'model_state_dict': model_state_dict, 'epoch': epoch,\n",
    "                'val_loss': avg_val_loss, 'val_accuracy': val_accuracy\n",
    "            }, best_model_path)\n",
    "            print(f\"  âœ“ é©—è­‰æå¤±é™ä½ï¼Œæ¨¡å‹å·²ä¿å­˜\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\næ—©åœæ©Ÿåˆ¶è§¸ç™¼ï¼šåœ¨ {patience} å€‹é€±æœŸå…§ç„¡æ”¹é€²ã€‚\")\n",
    "                break\n",
    "    \n",
    "    print(\"\\nè¨“ç·´å®Œæˆï¼\")\n",
    "    return best_model_path\n",
    "\n",
    "def evaluate_ensemble_model(model, test_loader, device, label_encoder):\n",
    "    \"\"\"è©•ä¼°å‡½æ•¸\"\"\"\n",
    "    print(\"\\n--- æ­£åœ¨è©•ä¼° Ensemble_Hybrid æ¨¡å‹ ---\")\n",
    "    model.eval()\n",
    "    y_pred_list, y_true_list = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"è©•ä¼°æ¸¬è©¦é›†\"):\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            if torch.isnan(inputs).any():\n",
    "                inputs = torch.nan_to_num(inputs, nan=0.0)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            if isinstance(outputs, dict): outputs = outputs['ensemble']\n",
    "            if torch.isnan(outputs).any():\n",
    "                outputs = torch.nan_to_num(outputs, nan=0.0)\n",
    "\n",
    "            _, predicted_labels = torch.max(outputs, 1)\n",
    "            y_pred_list.extend(predicted_labels.cpu().numpy())\n",
    "            y_true_list.extend(labels.cpu().numpy())\n",
    "            \n",
    "    report = classification_report(\n",
    "        y_true_list, y_pred_list,\n",
    "        target_names=[str(cls) for cls in label_encoder.classes_],\n",
    "        zero_division=0, digits=4\n",
    "    )\n",
    "    return report\n",
    "\n",
    "# ===================================================================\n",
    "# 5. æ–°å¢ï¼šä¿å­˜æ‰€æœ‰æ¨¡å‹æ–‡ä»¶çš„å‡½æ•¸\n",
    "# ===================================================================\n",
    "def save_model_files(model, scaler, label_encoder, feature_selector, selected_features, \n",
    "                    model_dir='/kaggle/working/ensemble_hybrid_model_package'):\n",
    "    \"\"\"ä¿å­˜æ‰€æœ‰ç”¨æ–¼éƒ¨ç½²çš„å¿…è¦æ–‡ä»¶\"\"\"\n",
    "    print(f\"\\n--- æ­£åœ¨ä¿å­˜å®Œæ•´çš„æ¨¡å‹æ–‡ä»¶åŒ…è‡³ {model_dir}/ ---\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    actual_model = model.module if isinstance(model, nn.DataParallel) else model\n",
    "    \n",
    "    # 1. ä¿å­˜ PyTorch æ¨¡å‹\n",
    "    torch.save({\n",
    "        'model_state_dict': actual_model.state_dict(),\n",
    "        'model_architecture': 'Ensemble_Hybrid',\n",
    "        'input_dim': actual_model.input_dim,\n",
    "        'num_classes': actual_model.num_classes,\n",
    "    }, os.path.join(model_dir, 'model.pth'))\n",
    "    print(\"âœ“ å·²ä¿å­˜ model.pth\")\n",
    "    \n",
    "    # 2. ä¿å­˜ scikit-learn ç‰©ä»¶\n",
    "    joblib.dump(scaler, os.path.join(model_dir, 'scaler.pkl'))\n",
    "    print(\"âœ“ å·²ä¿å­˜ scaler.pkl\")\n",
    "    \n",
    "    joblib.dump(label_encoder, os.path.join(model_dir, 'label_encoder.pkl'))\n",
    "    print(\"âœ“ å·²ä¿å­˜ label_encoder.pkl\")\n",
    "    \n",
    "    joblib.dump(feature_selector, os.path.join(model_dir, 'feature_selector.pkl'))\n",
    "    print(\"âœ“ å·²ä¿å­˜ feature_selector.pkl\")\n",
    "    \n",
    "    # 3. ä¿å­˜ç‰¹å¾µåˆ—è¡¨ç‚º JSON\n",
    "    with open(os.path.join(model_dir, 'selected_features.json'), 'w') as f:\n",
    "        json.dump(selected_features, f, indent=2)\n",
    "    print(\"âœ“ å·²ä¿å­˜ selected_features.json\")\n",
    "    \n",
    "    # 4. ä¿å­˜æ¨¡å‹å…ƒæ•¸æ“š\n",
    "    model_info = {\n",
    "        'dataset': 'CIC-IDS2017',\n",
    "        'model_type': 'PyTorch Ensemble_Hybrid',\n",
    "        'num_features_selected': len(selected_features),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'classes': label_encoder.classes_.tolist(),\n",
    "        'preprocessing_steps': {\n",
    "            'variance_threshold': 0.001,\n",
    "            'feature_selection_method': 'SelectKBest',\n",
    "            'k_features': len(selected_features),\n",
    "            'scaling_method': 'StandardScaler'\n",
    "        }\n",
    "    }\n",
    "    with open(os.path.join(model_dir, 'model_info.json'), 'w') as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(\"âœ“ å·²ä¿å­˜ model_info.json\")\n",
    "\n",
    "# ===================================================================\n",
    "# 6. ä¸»åŸ·è¡Œå‡½æ•¸ (ä¿®æ”¹ç‰ˆ - æ•´åˆæ‰€æœ‰åŠŸèƒ½)\n",
    "# ===================================================================\n",
    "def main():\n",
    "    \"\"\"ä¸»åŸ·è¡Œæµç¨‹\"\"\"\n",
    "    # --- è¨­ç½®èˆ‡ç’°å¢ƒæª¢æŸ¥ ---\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # --- æ•¸æ“šè¼‰å…¥ ---\n",
    "    data_path = '/kaggle/input/cicids2017'\n",
    "    print(f\"æ­£åœ¨å¾è·¯å¾‘è®€å–æ•¸æ“š: {data_path}\")\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šè·¯å¾‘ '{data_path}'ã€‚\")\n",
    "    \n",
    "    parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith('.parquet')]\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"éŒ¯èª¤ï¼šè·¯å¾‘ '{data_path}' ä¸­æ‰¾ä¸åˆ° .parquet æª”æ¡ˆã€‚\")\n",
    "\n",
    "    df_list = [pd.read_parquet(file) for file in tqdm(parquet_files, desc=\"æ­£åœ¨è¼‰å…¥ Parquet æª”æ¡ˆ\")]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    del df_list; gc.collect()\n",
    "    print(f\"æ•¸æ“šè¼‰å…¥å®Œæˆï¼ŒåŸå§‹å½¢ç‹€: {df.shape}\")\n",
    "\n",
    "    # --- æ•¸æ“šé è™•ç† ---\n",
    "    label_column = 'Label' if 'Label' in df.columns else ' Label'\n",
    "    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n",
    "    label_column = label_column.strip()\n",
    "\n",
    "    print(\"\\n--- æ­£åœ¨é€²è¡Œæ•¸æ“šæ¸…æ´— ---\")\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_columns:\n",
    "        if col != label_column:\n",
    "            q99 = df[col].quantile(0.99)\n",
    "            q01 = df[col].quantile(0.01)\n",
    "            df[col] = df[col].clip(lower=q01, upper=q99)\n",
    "\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"ç§»é™¤äº† {initial_rows - df.shape[0]} ç­†é‡è¤‡è¨˜éŒ„ã€‚\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    X = df.drop(columns=[label_column])\n",
    "    y = df[label_column].copy()\n",
    "    del df; gc.collect()\n",
    "\n",
    "    y = y.str.replace(r'[^a-zA-Z0-9\\s-]', '', regex=True).str.strip()\n",
    "    label_counts = y.value_counts()\n",
    "    threshold = 1000\n",
    "    rare_labels = label_counts[label_counts < threshold].index\n",
    "    if not rare_labels.empty:\n",
    "        print(f\"å°‡ {len(rare_labels)} å€‹ç¨€æœ‰é¡åˆ¥åˆä½µç‚º 'Rare_Attack'\")\n",
    "        y.replace(rare_labels, 'Rare_Attack', inplace=True)\n",
    "    \n",
    "    print(\"\\n--- æ­£åœ¨ç§»é™¤ä½æ–¹å·®/å¸¸æ•¸ç‰¹å¾µ ---\")\n",
    "    var_selector = VarianceThreshold(threshold=0.001)\n",
    "    X_filtered = var_selector.fit_transform(X)\n",
    "    retained_columns = X.columns[var_selector.get_support()]\n",
    "    X = pd.DataFrame(X_filtered, index=X.index, columns=retained_columns)\n",
    "    print(f\"ç§»é™¤ä½æ–¹å·®ç‰¹å¾µå¾Œï¼Œå‰©é¤˜ {X.shape[1]} å€‹ç‰¹å¾µã€‚\")\n",
    "\n",
    "    X = X.fillna(0)\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    print(f\"\\nç¸½å…±æœ‰ {num_classes} å€‹é¡åˆ¥\")\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=seed, stratify=y_encoded\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_val, y_train_val, test_size=0.2, random_state=seed, stratify=y_train_val\n",
    "    )\n",
    "\n",
    "    # ç‰¹å¾µé¸æ“‡\n",
    "    K_FEATURES = min(64, X_train.shape[1])\n",
    "    selector = SelectKBest(f_classif, k=K_FEATURES)\n",
    "    X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "    X_val_selected = selector.transform(X_val)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    selected_features_mask = selector.get_support()\n",
    "    selected_features = X_train.columns[selected_features_mask].tolist()\n",
    "    print(f\"\\næˆåŠŸé¸å–äº† {len(selected_features)} å€‹ç‰¹å¾µé€²è¡Œè¨“ç·´ã€‚\")\n",
    "\n",
    "    # æ¨™æº–åŒ–\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_val_scaled = scaler.transform(X_val_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "    \n",
    "    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    X_val_scaled = np.nan_to_num(X_val_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "    print(\"\\næ‰€æœ‰æ•¸æ“šé è™•ç†èˆ‡ç‰¹å¾µå·¥ç¨‹å·²å®Œæˆï¼\")\n",
    "\n",
    "    # --- DataLoader æº–å‚™ ---\n",
    "    batch_size = 512\n",
    "    num_workers = min(2, os.cpu_count())\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.from_numpy(X_train_scaled).float(), torch.from_numpy(y_train).long())\n",
    "    val_dataset = TensorDataset(torch.from_numpy(X_val_scaled).float(), torch.from_numpy(y_val).long())\n",
    "    test_dataset = TensorDataset(torch.from_numpy(X_test_scaled).float(), torch.from_numpy(y_test).long())\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, persistent_workers=True if num_workers > 0 else False)\n",
    "\n",
    "    # --- æ¨¡å‹åˆå§‹åŒ–èˆ‡è¨“ç·´ ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nä½¿ç”¨è¨­å‚™: {device}\")\n",
    "    \n",
    "    model = Ensemble_Hybrid(input_dim=K_FEATURES, num_classes=num_classes, dropout_rate=0.2)\n",
    "    \n",
    "    use_multi_gpu = torch.cuda.device_count() > 1\n",
    "    if use_multi_gpu:\n",
    "        print(f\"\\næª¢æ¸¬åˆ° {torch.cuda.device_count()} å€‹GPUï¼Œå•Ÿç”¨ä¸¦è¡Œè¨“ç·´ã€‚\")\n",
    "        model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"æ¨¡å‹ç¸½åƒæ•¸æ•¸é‡: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    best_model_path = train_ensemble_model(\n",
    "        model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "        device=device, num_epochs=30, patience=8, use_multi_gpu=use_multi_gpu\n",
    "    )\n",
    "\n",
    "    # --- æ¨¡å‹è©•ä¼°èˆ‡æ–‡ä»¶ä¿å­˜ ---\n",
    "    del model, train_loader, val_loader; gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\n--- è¼‰å…¥æœ€ä½³æ¨¡å‹é€²è¡Œæœ€çµ‚è©•ä¼° ---\")\n",
    "    final_model = Ensemble_Hybrid(input_dim=K_FEATURES, num_classes=num_classes, dropout_rate=0.2)\n",
    "    checkpoint = torch.load(best_model_path, map_location=device)\n",
    "    \n",
    "    # å…¼å®¹å–®GPUå’Œå¤šGPUä¿å­˜çš„æ¨¡å‹\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if use_multi_gpu and not list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {'module.' + k: v for k, v in state_dict.items()}\n",
    "    elif not use_multi_gpu and list(state_dict.keys())[0].startswith('module.'):\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    final_model.load_state_dict(state_dict)\n",
    "\n",
    "    if use_multi_gpu:\n",
    "        final_model = nn.DataParallel(final_model)\n",
    "    final_model.to(device)\n",
    "    print(f\"æˆåŠŸè¼‰å…¥ä¾†è‡ª Epoch {checkpoint['epoch']+1} çš„æœ€ä½³æ¨¡å‹ã€‚\")\n",
    "\n",
    "    report = evaluate_ensemble_model(\n",
    "        final_model, test_loader, device, label_encoder\n",
    "    )\n",
    "\n",
    "    print(\"\\nğŸ‰ğŸ‰ğŸ‰ --- æœ€çµ‚è©•ä¼°å ±å‘Š (æ¸¬è©¦é›†) --- ğŸ‰ğŸ‰ğŸ‰\")\n",
    "    print(report)\n",
    "\n",
    "    # --- ä¿å­˜æ‰€æœ‰éƒ¨ç½²æ‰€éœ€çš„æ–‡ä»¶ ---\n",
    "    save_model_files(\n",
    "        model=final_model,\n",
    "        scaler=scaler,\n",
    "        label_encoder=label_encoder,\n",
    "        feature_selector=selector,\n",
    "        selected_features=selected_features\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… è¨“ç·´èˆ‡æ¨¡å‹æ–‡ä»¶æ‰“åŒ…å…¨éƒ¨å®Œæˆï¼\")\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 7. ç¨‹åºå…¥å£\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main()\n",
    "        print(\"\\nğŸ‰ è…³æœ¬åŸ·è¡ŒæˆåŠŸï¼\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ åŸ·è¡Œéç¨‹ä¸­å‡ºç¾éŒ¯èª¤: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
