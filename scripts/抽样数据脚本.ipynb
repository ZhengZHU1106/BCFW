{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T11:46:19.302552Z",
     "iopub.status.busy": "2025-07-13T11:46:19.301536Z",
     "iopub.status.idle": "2025-07-13T11:46:46.221060Z",
     "shell.execute_reply": "2025-07-13T11:46:46.220237Z",
     "shell.execute_reply.started": "2025-07-13T11:46:19.302478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- æ­£åœ¨å¾è·¯å¾‘è®€å–çœŸå¯¦æ•¸æ“š: /kaggle/input/cicids2017 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨è¼‰å…¥ Parquet æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:01<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ çœŸå¯¦æ•¸æ“šè¼‰å…¥å®Œæˆï¼ŒåŸå§‹å½¢ç‹€: (2313810, 78)\n",
      "--- æ­£åœ¨é€²è¡Œæ•¸æ“šé è™•ç†èˆ‡æ¨™ç±¤åˆä½µ...\n",
      "ç§»é™¤äº† 82004 ç­†é‡è¤‡è¨˜éŒ„ã€‚\n",
      "æª¢æ¸¬åˆ° 4 å€‹ç¨€æœ‰é¡åˆ¥ (æ¨£æœ¬æ•¸ < 1000)ï¼Œå°‡åˆä½µç‚º 'RARE_ATTACK'ã€‚\n",
      "ç¨€æœ‰é¡åˆ¥åˆ—è¡¨: ['WEB ATTACK - XSS', 'INFILTRATION', 'WEB ATTACK - SQL INJECTION', 'HEARTBLEED']\n",
      "âœ“ æ•¸æ“šé è™•ç†èˆ‡æ¨™ç±¤åˆä½µå®Œæˆã€‚\n",
      "\n",
      "========================= æœ€çµ‚é¡åˆ¥è³‡è¨Š =========================\n",
      "ç¶“éç¨€æœ‰é¡åˆ¥åˆä½µå¾Œï¼Œæœ€çµ‚ç”¨æ–¼æŠ½æ¨£çš„é¡åˆ¥æœ‰:\n",
      "- BENIGN\n",
      "- BOT\n",
      "- DDOS\n",
      "- DOS GOLDENEYE\n",
      "- DOS HULK\n",
      "- DOS SLOWHTTPTEST\n",
      "- DOS SLOWLORIS\n",
      "- FTP-PATATOR\n",
      "- PORTSCAN\n",
      "- RARE_ATTACK\n",
      "- SSH-PATATOR\n",
      "- WEB ATTACK - BRUTE FORCE\n",
      "=================================================================\n",
      "\n",
      "--- æ­£åœ¨ä½¿ç”¨ã€å…¨éƒ¨æ•¸æ“šã€‘ä¾†è¨­å®šæ•¸æ“šè™•ç†è¦å‰‡...\n",
      "âœ“ æ‰€æœ‰æ•¸æ“šè™•ç†è¦å‰‡è¨­å®šå®Œæˆã€‚\n",
      "\n",
      "--- æ­£åœ¨æŒ‰æœ€çµ‚é¡åˆ¥åˆ—è¡¨å’Œæ•¸é‡é€²è¡Œç²¾æº–æŠ½æ¨£...\n",
      "  âœ“ å·²æŠ½å– 'BENIGN' é¡åˆ¥æ•¸æ“š 120 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'BOT' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'DDOS' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'DOS GOLDENEYE' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'DOS HULK' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'DOS SLOWHTTPTEST' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'DOS SLOWLORIS' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'FTP-PATATOR' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'PORTSCAN' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'RARE_ATTACK' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'SSH-PATATOR' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "  âœ“ å·²æŠ½å– 'WEB ATTACK - BRUTE FORCE' é¡åˆ¥æ•¸æ“š 10 ç­†ã€‚\n",
      "\n",
      "--- æ­£åœ¨è™•ç†æŠ½å‡ºçš„ 230 ç­†æ¨£æœ¬...\n",
      "âœ“ æ¨£æœ¬è™•ç†å®Œæˆï¼æœ€çµ‚æ•¸æ“šå½¢ç‹€: torch.Size([230, 64])\n",
      "\n",
      "ğŸ‰ æˆåŠŸï¼å·²å°‡ 230 ç­†ç²¾æº–æŠ½æ¨£çš„æ•¸æ“šä¿å­˜è‡³ï¼š\n",
      "   - ç‰¹å¾µæ•¸æ“š: /kaggle/working/inference_data.pt\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ã€æœ€çµ‚ç‰ˆ v4ã€‘åªä¿å­˜æ•¸æ“šçš„ç²¾æº–æŠ½æ¨£è…³æœ¬\n",
    "#\n",
    "# æ›´æ–°æ—¥èªŒï¼š\n",
    "# 1. æ ¹æ“šç”¨æˆ¶è¦æ±‚ï¼Œç§»é™¤ä¿å­˜æ¨™ç±¤æª”æ¡ˆçš„æ­¥é©Ÿï¼Œæœ€çµ‚åªè¼¸å‡ºä¸€å€‹\n",
    "#    inference_data.pt æª”æ¡ˆã€‚\n",
    "# ===================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. è«‹è¨­å®šæ‚¨çš„æ•¸æ“šé›†è·¯å¾‘ ---\n",
    "DATASET_PATH = '/kaggle/input/cicids2017'\n",
    "\n",
    "# --- 2. è«‹è¨­å®šæŠ½æ¨£æ•¸é‡ ---\n",
    "NUM_BENIGN_SAMPLES = 120\n",
    "NUM_SAMPLES_PER_ATTACK_CLASS = 10\n",
    "# ç¨€æœ‰é¡åˆ¥çš„åˆ¤æ–·é–¾å€¼ (èˆ‡åŸå§‹è¨“ç·´ç¢¼ä¿æŒä¸€è‡´)\n",
    "RARE_LABEL_THRESHOLD = 1000\n",
    "\n",
    "\n",
    "def load_real_data(path):\n",
    "    \"\"\"å¾æŒ‡å®šè·¯å¾‘åŠ è¼‰æ‰€æœ‰ Parquet æª”æ¡ˆ\"\"\"\n",
    "    print(f\"--- æ­£åœ¨å¾è·¯å¾‘è®€å–çœŸå¯¦æ•¸æ“š: {path} ---\")\n",
    "    if not os.path.exists(path) or not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ•¸æ“šè·¯å¾‘ '{path}'ã€‚\")\n",
    "    parquet_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parquet')]\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"éŒ¯èª¤ï¼šåœ¨ '{path}' ä¸­æ‰¾ä¸åˆ° .parquet æª”æ¡ˆã€‚\")\n",
    "    df_list = [pd.read_parquet(file) for file in tqdm(parquet_files, desc=\"æ­£åœ¨è¼‰å…¥ Parquet æª”æ¡ˆ\")]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n",
    "    print(f\"âœ“ çœŸå¯¦æ•¸æ“šè¼‰å…¥å®Œæˆï¼ŒåŸå§‹å½¢ç‹€: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def preprocess_and_group_labels(df):\n",
    "    \"\"\"\n",
    "    å° DataFrame é€²è¡Œæ¸…æ´—ï¼Œä¸¦åŸ·è¡Œèˆ‡åŸå§‹è¨“ç·´ç¢¼å®Œå…¨ä¸€è‡´çš„æ¨™ç±¤è™•ç†é‚è¼¯ã€‚\n",
    "    \"\"\"\n",
    "    print(\"--- æ­£åœ¨é€²è¡Œæ•¸æ“šé è™•ç†èˆ‡æ¨™ç±¤åˆä½µ...\")\n",
    "    label_column = 'Label'\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(\"åœ¨æ•¸æ“šä¸­æ‰¾ä¸åˆ° 'Label' æ¬„ä½ã€‚\")\n",
    "    \n",
    "    # çµ±ä¸€æ ¼å¼ï¼Œå¼·åˆ¶è½‰ç‚ºå¤§å¯«ï¼Œä¸¦æ¸…ç†ç‰¹æ®Šå­—ç¬¦\n",
    "    df[label_column] = df[label_column].str.replace(r'[^a-zA-Z0-9\\s-]', '', regex=True).str.strip().str.upper()\n",
    "    # è™•ç† WEB ATTACK ä¸­é–“å¤šå€‹ç©ºæ ¼çš„å•é¡Œ\n",
    "    df[label_column] = df[label_column].str.replace(r'WEB ATTACK\\s+', 'WEB ATTACK - ', regex=True)\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"ç§»é™¤äº† {initial_rows - df.shape[0]} ç­†é‡è¤‡è¨˜éŒ„ã€‚\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # ã€é—œéµã€‘æ¢å¾©åŸå§‹è¨“ç·´è…³æœ¬çš„ã€Œç¨€æœ‰é¡åˆ¥åˆä½µã€é‚è¼¯\n",
    "    y = df[label_column]\n",
    "    label_counts = y.value_counts()\n",
    "    rare_labels = label_counts[label_counts < RARE_LABEL_THRESHOLD].index\n",
    "    \n",
    "    rare_labels = [label for label in rare_labels if label != 'BENIGN']\n",
    "\n",
    "    if rare_labels:\n",
    "        print(f\"æª¢æ¸¬åˆ° {len(rare_labels)} å€‹ç¨€æœ‰é¡åˆ¥ (æ¨£æœ¬æ•¸ < {RARE_LABEL_THRESHOLD})ï¼Œå°‡åˆä½µç‚º 'RARE_ATTACK'ã€‚\")\n",
    "        print(f\"ç¨€æœ‰é¡åˆ¥åˆ—è¡¨: {rare_labels}\")\n",
    "        df[label_column].replace(rare_labels, 'RARE_ATTACK', inplace=True)\n",
    "\n",
    "    print(\"âœ“ æ•¸æ“šé è™•ç†èˆ‡æ¨™ç±¤åˆä½µå®Œæˆã€‚\")\n",
    "    return df, label_column\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 1. è¼‰å…¥ä¸¦åŸ·è¡ŒåŒ…å«ã€Œç¨€æœ‰é¡åˆ¥åˆä½µã€çš„é è™•ç†\n",
    "        full_df, label_column = preprocess_and_group_labels(load_real_data(DATASET_PATH))\n",
    "\n",
    "        # 2. ç²å–æœ€çµ‚çš„é¡åˆ¥åˆ—è¡¨ä»¥ä¾›æŠ½æ¨£\n",
    "        final_classes = sorted(full_df[label_column].unique())\n",
    "        print(\"\\n========================= æœ€çµ‚é¡åˆ¥è³‡è¨Š =========================\")\n",
    "        print(\"ç¶“éç¨€æœ‰é¡åˆ¥åˆä½µå¾Œï¼Œæœ€çµ‚ç”¨æ–¼æŠ½æ¨£çš„é¡åˆ¥æœ‰:\")\n",
    "        for label in final_classes:\n",
    "            print(f\"- {label}\")\n",
    "        print(\"=================================================================\")\n",
    "\n",
    "        # 3. è¨­å®šæ•¸æ“šè™•ç†è¦å‰‡ (ä½¿ç”¨ã€å…¨éƒ¨ã€‘æ•¸æ“šä¾†æ“¬åˆ)\n",
    "        print(\"\\n--- æ­£åœ¨ä½¿ç”¨ã€å…¨éƒ¨æ•¸æ“šã€‘ä¾†è¨­å®šæ•¸æ“šè™•ç†è¦å‰‡...\")\n",
    "        X_full = full_df.drop(columns=[label_column])\n",
    "        y_full = full_df[label_column]\n",
    "        label_encoder = LabelEncoder().fit(y_full)\n",
    "        var_selector = VarianceThreshold(threshold=0.001).fit(X_full)\n",
    "        X_full_filtered = var_selector.transform(X_full)\n",
    "        K_FEATURES = min(64, X_full_filtered.shape[1])\n",
    "        selector = SelectKBest(f_classif, k=K_FEATURES).fit(X_full_filtered, label_encoder.transform(y_full))\n",
    "        X_full_selected = selector.transform(X_full_filtered)\n",
    "        scaler = StandardScaler().fit(X_full_selected)\n",
    "        print(\"âœ“ æ‰€æœ‰æ•¸æ“šè™•ç†è¦å‰‡è¨­å®šå®Œæˆã€‚\")\n",
    "        \n",
    "        # 4. æŒ‰æœ€çµ‚é¡åˆ¥åˆ—è¡¨é€²è¡Œå…¨è‡ªå‹•ç²¾æº–æŠ½æ¨£\n",
    "        print(\"\\n--- æ­£åœ¨æŒ‰æœ€çµ‚é¡åˆ¥åˆ—è¡¨å’Œæ•¸é‡é€²è¡Œç²¾æº–æŠ½æ¨£...\")\n",
    "        collected_samples = []\n",
    "        \n",
    "        for class_name in final_classes:\n",
    "            num_to_sample = NUM_BENIGN_SAMPLES if class_name == 'BENIGN' else NUM_SAMPLES_PER_ATTACK_CLASS\n",
    "            class_df = full_df[full_df[label_column] == class_name]\n",
    "            \n",
    "            if len(class_df) < num_to_sample:\n",
    "                print(f\"è­¦å‘Šï¼š'{class_name}' é¡åˆ¥æ•¸æ“šä¸è¶³ {num_to_sample} ç­† (åªæœ‰ {len(class_df)} ç­†)ï¼Œå°‡ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•¸æ“šã€‚\")\n",
    "                sampled_df = class_df\n",
    "            else:\n",
    "                sampled_df = class_df.sample(n=num_to_sample, random_state=42)\n",
    "            \n",
    "            collected_samples.append(sampled_df)\n",
    "            print(f\"  âœ“ å·²æŠ½å– '{class_name}' é¡åˆ¥æ•¸æ“š {len(sampled_df)} ç­†ã€‚\")\n",
    "\n",
    "        # 5. çµ„åˆã€è™•ç†ä¸¦å­˜æª”\n",
    "        df_for_inference = pd.concat(collected_samples, ignore_index=True)\n",
    "        df_for_inference = df_for_inference.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n--- æ­£åœ¨è™•ç†æŠ½å‡ºçš„ {len(df_for_inference)} ç­†æ¨£æœ¬...\")\n",
    "        X_inference = df_for_inference.drop(columns=[label_column])\n",
    "        \n",
    "        # é›–ç„¶ä¸ä¿å­˜æ¨™ç±¤ï¼Œä½†åœ¨è™•ç†éç¨‹ä¸­ä»ç„¶éœ€è¦å®ƒå€‘\n",
    "        # y_inference = df_for_inference[label_column]\n",
    "\n",
    "        # y_inference_encoded = label_encoder.transform(y_inference) # ä¸å†éœ€è¦\n",
    "        X_inference_filtered = var_selector.transform(X_inference)\n",
    "        X_inference_selected = selector.transform(X_inference_filtered)\n",
    "        X_inference_scaled = scaler.transform(X_inference_selected)\n",
    "        \n",
    "        X_inference_final = np.nan_to_num(X_inference_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        inference_tensor = torch.from_numpy(X_inference_final).float()\n",
    "        print(f\"âœ“ æ¨£æœ¬è™•ç†å®Œæˆï¼æœ€çµ‚æ•¸æ“šå½¢ç‹€: {inference_tensor.shape}\")\n",
    "\n",
    "        # 6. ã€ä¿®æ”¹è™•ã€‘åªä¿å­˜ç‰¹å¾µæ•¸æ“šï¼Œä¸ä¿å­˜æ¨™ç±¤\n",
    "        output_data_path = 'inference_data.pt'\n",
    "        torch.save(inference_tensor, output_data_path)\n",
    "        \n",
    "        print(f\"\\nğŸ‰ æˆåŠŸï¼å·²å°‡ {len(df_for_inference)} ç­†ç²¾æº–æŠ½æ¨£çš„æ•¸æ“šä¿å­˜è‡³ï¼š\")\n",
    "        print(f\"   - ç‰¹å¾µæ•¸æ“š: {os.path.abspath(output_data_path)}\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"\\nâŒ æ“ä½œå¤±æ•—ï¼š{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
