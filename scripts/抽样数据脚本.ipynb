{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T11:46:19.302552Z",
     "iopub.status.busy": "2025-07-13T11:46:19.301536Z",
     "iopub.status.idle": "2025-07-13T11:46:46.221060Z",
     "shell.execute_reply": "2025-07-13T11:46:46.220237Z",
     "shell.execute_reply.started": "2025-07-13T11:46:19.302478Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在從路徑讀取真實數據: /kaggle/input/cicids2017 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在載入 Parquet 檔案: 100%|██████████| 8/8 [00:01<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 真實數據載入完成，原始形狀: (2313810, 78)\n",
      "--- 正在進行數據預處理與標籤合併...\n",
      "移除了 82004 筆重複記錄。\n",
      "檢測到 4 個稀有類別 (樣本數 < 1000)，將合併為 'RARE_ATTACK'。\n",
      "稀有類別列表: ['WEB ATTACK - XSS', 'INFILTRATION', 'WEB ATTACK - SQL INJECTION', 'HEARTBLEED']\n",
      "✓ 數據預處理與標籤合併完成。\n",
      "\n",
      "========================= 最終類別資訊 =========================\n",
      "經過稀有類別合併後，最終用於抽樣的類別有:\n",
      "- BENIGN\n",
      "- BOT\n",
      "- DDOS\n",
      "- DOS GOLDENEYE\n",
      "- DOS HULK\n",
      "- DOS SLOWHTTPTEST\n",
      "- DOS SLOWLORIS\n",
      "- FTP-PATATOR\n",
      "- PORTSCAN\n",
      "- RARE_ATTACK\n",
      "- SSH-PATATOR\n",
      "- WEB ATTACK - BRUTE FORCE\n",
      "=================================================================\n",
      "\n",
      "--- 正在使用【全部數據】來設定數據處理規則...\n",
      "✓ 所有數據處理規則設定完成。\n",
      "\n",
      "--- 正在按最終類別列表和數量進行精準抽樣...\n",
      "  ✓ 已抽取 'BENIGN' 類別數據 120 筆。\n",
      "  ✓ 已抽取 'BOT' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'DDOS' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'DOS GOLDENEYE' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'DOS HULK' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'DOS SLOWHTTPTEST' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'DOS SLOWLORIS' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'FTP-PATATOR' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'PORTSCAN' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'RARE_ATTACK' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'SSH-PATATOR' 類別數據 10 筆。\n",
      "  ✓ 已抽取 'WEB ATTACK - BRUTE FORCE' 類別數據 10 筆。\n",
      "\n",
      "--- 正在處理抽出的 230 筆樣本...\n",
      "✓ 樣本處理完成！最終數據形狀: torch.Size([230, 64])\n",
      "\n",
      "🎉 成功！已將 230 筆精準抽樣的數據保存至：\n",
      "   - 特徵數據: /kaggle/working/inference_data.pt\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 【最終版 v4】只保存數據的精準抽樣腳本\n",
    "#\n",
    "# 更新日誌：\n",
    "# 1. 根據用戶要求，移除保存標籤檔案的步驟，最終只輸出一個\n",
    "#    inference_data.pt 檔案。\n",
    "# ===================================================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, VarianceThreshold\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. 請設定您的數據集路徑 ---\n",
    "DATASET_PATH = '/kaggle/input/cicids2017'\n",
    "\n",
    "# --- 2. 請設定抽樣數量 ---\n",
    "NUM_BENIGN_SAMPLES = 120\n",
    "NUM_SAMPLES_PER_ATTACK_CLASS = 10\n",
    "# 稀有類別的判斷閾值 (與原始訓練碼保持一致)\n",
    "RARE_LABEL_THRESHOLD = 1000\n",
    "\n",
    "\n",
    "def load_real_data(path):\n",
    "    \"\"\"從指定路徑加載所有 Parquet 檔案\"\"\"\n",
    "    print(f\"--- 正在從路徑讀取真實數據: {path} ---\")\n",
    "    if not os.path.exists(path) or not os.path.isdir(path):\n",
    "        raise FileNotFoundError(f\"錯誤：找不到數據路徑 '{path}'。\")\n",
    "    parquet_files = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.parquet')]\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"錯誤：在 '{path}' 中找不到 .parquet 檔案。\")\n",
    "    df_list = [pd.read_parquet(file) for file in tqdm(parquet_files, desc=\"正在載入 Parquet 檔案\")]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n",
    "    print(f\"✓ 真實數據載入完成，原始形狀: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def preprocess_and_group_labels(df):\n",
    "    \"\"\"\n",
    "    對 DataFrame 進行清洗，並執行與原始訓練碼完全一致的標籤處理邏輯。\n",
    "    \"\"\"\n",
    "    print(\"--- 正在進行數據預處理與標籤合併...\")\n",
    "    label_column = 'Label'\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(\"在數據中找不到 'Label' 欄位。\")\n",
    "    \n",
    "    # 統一格式，強制轉為大寫，並清理特殊字符\n",
    "    df[label_column] = df[label_column].str.replace(r'[^a-zA-Z0-9\\s-]', '', regex=True).str.strip().str.upper()\n",
    "    # 處理 WEB ATTACK 中間多個空格的問題\n",
    "    df[label_column] = df[label_column].str.replace(r'WEB ATTACK\\s+', 'WEB ATTACK - ', regex=True)\n",
    "\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.fillna(0, inplace=True)\n",
    "    initial_rows = df.shape[0]\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(f\"移除了 {initial_rows - df.shape[0]} 筆重複記錄。\")\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 【關鍵】恢復原始訓練腳本的「稀有類別合併」邏輯\n",
    "    y = df[label_column]\n",
    "    label_counts = y.value_counts()\n",
    "    rare_labels = label_counts[label_counts < RARE_LABEL_THRESHOLD].index\n",
    "    \n",
    "    rare_labels = [label for label in rare_labels if label != 'BENIGN']\n",
    "\n",
    "    if rare_labels:\n",
    "        print(f\"檢測到 {len(rare_labels)} 個稀有類別 (樣本數 < {RARE_LABEL_THRESHOLD})，將合併為 'RARE_ATTACK'。\")\n",
    "        print(f\"稀有類別列表: {rare_labels}\")\n",
    "        df[label_column].replace(rare_labels, 'RARE_ATTACK', inplace=True)\n",
    "\n",
    "    print(\"✓ 數據預處理與標籤合併完成。\")\n",
    "    return df, label_column\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # 1. 載入並執行包含「稀有類別合併」的預處理\n",
    "        full_df, label_column = preprocess_and_group_labels(load_real_data(DATASET_PATH))\n",
    "\n",
    "        # 2. 獲取最終的類別列表以供抽樣\n",
    "        final_classes = sorted(full_df[label_column].unique())\n",
    "        print(\"\\n========================= 最終類別資訊 =========================\")\n",
    "        print(\"經過稀有類別合併後，最終用於抽樣的類別有:\")\n",
    "        for label in final_classes:\n",
    "            print(f\"- {label}\")\n",
    "        print(\"=================================================================\")\n",
    "\n",
    "        # 3. 設定數據處理規則 (使用【全部】數據來擬合)\n",
    "        print(\"\\n--- 正在使用【全部數據】來設定數據處理規則...\")\n",
    "        X_full = full_df.drop(columns=[label_column])\n",
    "        y_full = full_df[label_column]\n",
    "        label_encoder = LabelEncoder().fit(y_full)\n",
    "        var_selector = VarianceThreshold(threshold=0.001).fit(X_full)\n",
    "        X_full_filtered = var_selector.transform(X_full)\n",
    "        K_FEATURES = min(64, X_full_filtered.shape[1])\n",
    "        selector = SelectKBest(f_classif, k=K_FEATURES).fit(X_full_filtered, label_encoder.transform(y_full))\n",
    "        X_full_selected = selector.transform(X_full_filtered)\n",
    "        scaler = StandardScaler().fit(X_full_selected)\n",
    "        print(\"✓ 所有數據處理規則設定完成。\")\n",
    "        \n",
    "        # 4. 按最終類別列表進行全自動精準抽樣\n",
    "        print(\"\\n--- 正在按最終類別列表和數量進行精準抽樣...\")\n",
    "        collected_samples = []\n",
    "        \n",
    "        for class_name in final_classes:\n",
    "            num_to_sample = NUM_BENIGN_SAMPLES if class_name == 'BENIGN' else NUM_SAMPLES_PER_ATTACK_CLASS\n",
    "            class_df = full_df[full_df[label_column] == class_name]\n",
    "            \n",
    "            if len(class_df) < num_to_sample:\n",
    "                print(f\"警告：'{class_name}' 類別數據不足 {num_to_sample} 筆 (只有 {len(class_df)} 筆)，將使用所有可用數據。\")\n",
    "                sampled_df = class_df\n",
    "            else:\n",
    "                sampled_df = class_df.sample(n=num_to_sample, random_state=42)\n",
    "            \n",
    "            collected_samples.append(sampled_df)\n",
    "            print(f\"  ✓ 已抽取 '{class_name}' 類別數據 {len(sampled_df)} 筆。\")\n",
    "\n",
    "        # 5. 組合、處理並存檔\n",
    "        df_for_inference = pd.concat(collected_samples, ignore_index=True)\n",
    "        df_for_inference = df_for_inference.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\n--- 正在處理抽出的 {len(df_for_inference)} 筆樣本...\")\n",
    "        X_inference = df_for_inference.drop(columns=[label_column])\n",
    "        \n",
    "        # 雖然不保存標籤，但在處理過程中仍然需要它們\n",
    "        # y_inference = df_for_inference[label_column]\n",
    "\n",
    "        # y_inference_encoded = label_encoder.transform(y_inference) # 不再需要\n",
    "        X_inference_filtered = var_selector.transform(X_inference)\n",
    "        X_inference_selected = selector.transform(X_inference_filtered)\n",
    "        X_inference_scaled = scaler.transform(X_inference_selected)\n",
    "        \n",
    "        X_inference_final = np.nan_to_num(X_inference_scaled, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        inference_tensor = torch.from_numpy(X_inference_final).float()\n",
    "        print(f\"✓ 樣本處理完成！最終數據形狀: {inference_tensor.shape}\")\n",
    "\n",
    "        # 6. 【修改處】只保存特徵數據，不保存標籤\n",
    "        output_data_path = 'inference_data.pt'\n",
    "        torch.save(inference_tensor, output_data_path)\n",
    "        \n",
    "        print(f\"\\n🎉 成功！已將 {len(df_for_inference)} 筆精準抽樣的數據保存至：\")\n",
    "        print(f\"   - 特徵數據: {os.path.abspath(output_data_path)}\")\n",
    "\n",
    "    except (FileNotFoundError, ValueError) as e:\n",
    "        print(f\"\\n❌ 操作失敗：{e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 發生未預期的錯誤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 2395943,
     "sourceId": 4059877,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
