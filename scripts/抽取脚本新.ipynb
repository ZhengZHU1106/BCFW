{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4059877,"sourceType":"datasetVersion","datasetId":2395943}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport torch\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom tqdm import tqdm\n\ndef create_final_inference_file(data_path, output_file):\n    \"\"\"\n    一个完整的流程：加载数据、拟合处理器、按最终规则采样、处理数据，并保存为.pt文件。\n    \"\"\"\n    # --- 步骤 1: 加载数据并进行清洗和标签映射 ---\n    print(\"步骤 1/5: 加载数据并进行清洗和标签映射...\")\n    parquet_files = [os.path.join(data_path, f) for f in os.listdir(data_path)\n                     if f.endswith('.parquet')]\n    if not parquet_files:\n        raise FileNotFoundError(f\"错误：在路径 '{data_path}' 下没有找到任何 .parquet 文件。\")\n    \n    df = pd.concat([pd.read_parquet(f) for f in tqdm(parquet_files, desc=\"加载文件\")], ignore_index=True)\n    \n    # 清理列名中的空格\n    df.rename(columns={col: col.strip() for col in df.columns}, inplace=True)\n    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n    df.fillna(0, inplace=True)\n    \n    # 清理原始标签中的异常字符\n    df['Label'] = df['Label'].astype(str).str.replace(r'[^a-zA-Z0-9\\s-]', '', regex=True).str.strip()\n\n    # 映射到7个主要类别\n    multi_class_mapping = {\n        'DoS Hulk': 'DoS', 'DoS GoldenEye': 'DoS', 'DoS slowloris': 'DoS',\n        'DoS Slowhttptest': 'DoS', 'FTP-Patator': 'Brute_Force',\n        'SSH-Patator': 'Brute_Force', 'Web Attack Brute Force': 'Web_Attack',\n        'Web Attack XSS': 'Web_Attack', 'Web Attack Sql Injection': 'Web_Attack',\n        'PortScan': 'PortScan', 'Bot': 'Bot', 'Infiltration': 'Rare_Attacks',\n        'Heartbleed': 'Rare_Attacks'\n    }\n    df['Multi_Label'] = df['Label'].replace(multi_class_mapping)\n\n    # 关键步骤：删除模型未学习的小众类别\n    original_rows = len(df)\n    df = df[~df['Multi_Label'].isin(['Rare_Attacks'])]\n    print(f\"移除了 {original_rows - len(df)} 条 'Rare_Attacks' (Infiltration, Heartbleed) 数据。\")\n    \n    df.drop_duplicates(inplace=True)\n    print(\"数据加载和预处理完成。\")\n\n    # --- 步骤 2: 准备并拟合数据处理器 ---\n    print(\"\\n步骤 2/5: 准备并拟合数据处理器以匹配模型...\")\n    feature_columns = [col for col in df.columns if col not in ['Label', 'Multi_Label']]\n    X = df[feature_columns]\n    y = df['Multi_Label']\n    \n    label_encoder = LabelEncoder()\n    label_encoder.fit(y)\n    print(f\"标签编码器拟合完成，最终类别: {list(label_encoder.classes_)}\")\n\n    scaler = StandardScaler()\n    scaler.fit(X)\n    print(\"标准化处理器(Scaler)拟合完成。\")\n    \n    # --- 步骤 3: 从处理后的数据中按最终规则采样 ---\n    print(\"\\n步骤 3/5: 按最终规则进行分层采样...\")\n    def get_sample_size(group_name):\n        return 130 if group_name == 'Benign' else 10\n\n    sampled_dfs = []\n    for group_name, group_df in df.groupby('Multi_Label'):\n        n = get_sample_size(group_name)\n        sampled_dfs.append(group_df.sample(n=n, random_state=42))\n        print(f\"  - 类别 '{group_name}': 成功抽取 {n} 条数据\")\n\n    sampled_df = pd.concat(sampled_dfs, ignore_index=True)\n    print(\"采样完成。\")\n\n    # --- 步骤 4: 处理采样数据并转换为Tensor ---\n    print(\"\\n步骤 4/5: 处理采样数据并转换为PyTorch Tensor...\")\n    x_sample_raw = sampled_df[feature_columns]\n    y_sample_str = sampled_df['Multi_Label']\n\n    x_sample_scaled = scaler.transform(x_sample_raw)\n    y_sample_encoded = label_encoder.transform(y_sample_str)\n    \n    features_tensor = torch.tensor(x_sample_scaled, dtype=torch.float32)\n    labels_tensor = torch.tensor(y_sample_encoded, dtype=torch.long)\n    print(f\"数据已转换为Tensor。特征形状: {features_tensor.shape}, 标签形状: {labels_tensor.shape}\")\n\n    # --- 步骤 5: 保存为.pt文件 ---\n    print(f\"\\n步骤 5/5: 保存到文件 '{output_file}'...\")\n    data_to_save = {\n        'features': features_tensor,\n        'labels': labels_tensor,\n        'class_names': list(label_encoder.classes_)\n    }\n    torch.save(data_to_save, output_file)\n    print(\"保存成功！\")\n\n# --- 主程序入口 ---\nif __name__ == \"__main__\":\n    DATA_DIRECTORY = '/kaggle/input/cicids2017' \n    OUTPUT_FILE = 'inference_data.pt'\n\n    try:\n        create_final_inference_file(DATA_DIRECTORY, OUTPUT_FILE)\n        print(f\"\\n🎉 最终文件 '{OUTPUT_FILE}' 已成功生成！\")\n        \n    except Exception as e:\n        print(f\"\\n❌ 发生错误: {e}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-24T12:36:55.575238Z","iopub.execute_input":"2025-08-24T12:36:55.575626Z","iopub.status.idle":"2025-08-24T12:37:17.127307Z","shell.execute_reply.started":"2025-08-24T12:36:55.575594Z","shell.execute_reply":"2025-08-24T12:37:17.126134Z"}},"outputs":[{"name":"stdout","text":"步骤 1/5: 加载数据并进行清洗和标签映射...\n","output_type":"stream"},{"name":"stderr","text":"加载文件: 100%|██████████| 8/8 [00:00<00:00,  8.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"移除了 47 条 'Rare_Attacks' (Infiltration, Heartbleed) 数据。\n数据加载和预处理完成。\n\n步骤 2/5: 准备并拟合数据处理器以匹配模型...\n标签编码器拟合完成，最终类别: ['Benign', 'Bot', 'Brute_Force', 'DDoS', 'DoS', 'PortScan', 'Web Attack  Brute Force', 'Web Attack  Sql Injection', 'Web Attack  XSS']\n标准化处理器(Scaler)拟合完成。\n\n步骤 3/5: 按最终规则进行分层采样...\n  - 类别 'Benign': 成功抽取 130 条数据\n  - 类别 'Bot': 成功抽取 10 条数据\n  - 类别 'Brute_Force': 成功抽取 10 条数据\n  - 类别 'DDoS': 成功抽取 10 条数据\n  - 类别 'DoS': 成功抽取 10 条数据\n  - 类别 'PortScan': 成功抽取 10 条数据\n  - 类别 'Web Attack  Brute Force': 成功抽取 10 条数据\n  - 类别 'Web Attack  Sql Injection': 成功抽取 10 条数据\n  - 类别 'Web Attack  XSS': 成功抽取 10 条数据\n采样完成。\n\n步骤 4/5: 处理采样数据并转换为PyTorch Tensor...\n数据已转换为Tensor。特征形状: torch.Size([210, 77]), 标签形状: torch.Size([210])\n\n步骤 5/5: 保存到文件 'inference_data.pt'...\n保存成功！\n\n🎉 最终文件 'inference_data.pt' 已成功生成！\n","output_type":"stream"}],"execution_count":3}]}