"""
AIæ¨¡å‹åŠ è½½å’Œæ¨ç†æ¨¡å—
"""

import torch
import pickle
import json
import numpy as np
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional
from ..config import AI_MODEL_CONFIG, THREAT_THRESHOLDS

logger = logging.getLogger(__name__)

class ThreatDetectionModel:
    """å¨èƒæ£€æµ‹æ¨¡å‹ç®¡ç†å™¨"""
    
    def __init__(self):
        self.model = None
        self.scaler = None
        self.feature_selector = None
        self.label_encoder = None
        self.model_info = None
        self.selected_features = None
        self.inference_data = None
        self._load_all_components()
    
    def _load_all_components(self):
        """åŠ è½½æ‰€æœ‰æ¨¡å‹ç»„ä»¶"""
        try:
            logger.info("ğŸ¤– å¼€å§‹åŠ è½½AIæ¨¡å‹ç»„ä»¶...")
            
            # åŠ è½½PyTorchæ¨¡å‹
            self._load_pytorch_model()
            
            # åŠ è½½é¢„å¤„ç†ç»„ä»¶
            self._load_preprocessors()
            
            # åŠ è½½æ¨¡å‹ä¿¡æ¯å’Œç‰¹å¾
            self._load_metadata()
            
            # åŠ è½½æ¨ç†æ•°æ®
            self._load_inference_data()
            
            logger.info("âœ… AIæ¨¡å‹ç»„ä»¶åŠ è½½å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ AIæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _load_pytorch_model(self):
        """åŠ è½½PyTorchæ¨¡å‹"""
        model_path = AI_MODEL_CONFIG['model_file']
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ¨¡å‹æ•°æ®
        model_data = torch.load(model_path, map_location='cpu')
        
        if isinstance(model_data, dict) and 'model_state_dict' in model_data:
            # é‡å»ºçœŸå®çš„Ensemble_Hybridæ¨¡å‹
            input_dim = model_data.get('input_dim', 64)
            num_classes = model_data.get('num_classes', 12)
            
            self.model = self._create_ensemble_hybrid_model(input_dim, num_classes)
            
            # åŠ è½½çœŸå®æƒé‡
            try:
                self.model.load_state_dict(model_data['model_state_dict'])
                logger.info("âœ… æˆåŠŸåŠ è½½çœŸå®æ¨¡å‹æƒé‡")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•åŠ è½½çœŸå®æƒé‡: {e}ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        else:
            # ç›´æ¥æ˜¯æ¨¡å‹å¯¹è±¡
            self.model = model_data
        
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        logger.info(f"âœ… PyTorchæ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    
    def _create_ensemble_hybrid_model(self, input_dim: int, num_classes: int):
        """åˆ›å»ºçœŸå®çš„Ensemble_Hybridæ¨¡å‹æ¶æ„"""
        import torch.nn as nn
        import torch.nn.functional as F
        
        class ResidualBlock(nn.Module):
            def __init__(self, dim):
                super().__init__()
                self.block = nn.Sequential(
                    nn.Linear(dim, dim),
                    nn.BatchNorm1d(dim),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(dim, dim),
                    nn.BatchNorm1d(dim)
                )
                
            def forward(self, x):
                return F.relu(x + self.block(x))
        
        class AttentionBranch(nn.Module):
            def __init__(self, input_dim, num_classes):
                super().__init__()
                self.query = nn.Linear(input_dim, input_dim)
                self.key = nn.Linear(input_dim, input_dim)
                self.value = nn.Linear(input_dim, input_dim)
                self.output_projection = nn.Linear(input_dim, input_dim)
                
                self.classifier = nn.Sequential(
                    nn.Linear(input_dim, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_classes)
                )
                
            def forward(self, x):
                # Self-attention
                q = self.query(x)
                k = self.key(x)
                v = self.value(x)
                
                # Attention weights
                attention = F.softmax(torch.matmul(q, k.T) / (x.size(-1) ** 0.5), dim=-1)
                attended = torch.matmul(attention, v)
                output = self.output_projection(attended)
                
                return self.classifier(output)
        
        class InteractionBranch(nn.Module):
            def __init__(self, input_dim, num_classes):
                super().__init__()
                self.feature_embeddings = nn.Linear(input_dim, 8)
                # 8ä¸ªç‰¹å¾åµŒå…¥ + 64ä¸ªåŸå§‹ç‰¹å¾ = 72ï¼Œä½†state_dictæ˜¾ç¤º92ï¼Œå¯èƒ½æœ‰å…¶ä»–ç‰¹å¾
                self.classifier = nn.Sequential(
                    nn.Linear(92, 128),  # æ ¹æ®state_dictè°ƒæ•´
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_classes)
                )
                
            def forward(self, x):
                embeddings = self.feature_embeddings(x)
                # æ‹¼æ¥åŸå§‹ç‰¹å¾å’ŒåµŒå…¥ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…92
                # 64(åŸå§‹) + 8(åµŒå…¥) + 20(å…¶ä»–ç‰¹å¾) = 92
                extra_features = torch.mean(x, dim=-1, keepdim=True).repeat(1, 20)  # åˆ›å»º20ä¸ªé¢å¤–ç‰¹å¾
                interactions = torch.cat([x, embeddings, extra_features], dim=-1)
                return self.classifier(interactions)
        
        class EnsembleHybridModel(nn.Module):
            def __init__(self, input_dim, num_classes):
                super().__init__()
                self.input_dim = input_dim
                self.num_classes = num_classes
                
                # Deep branch
                self.deep_branch = nn.Sequential(
                    nn.Linear(input_dim, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_classes)
                )
                
                # Wide branch
                self.wide_branch = nn.Sequential(
                    nn.Linear(input_dim, 512),
                    nn.BatchNorm1d(512),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(512, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, num_classes)
                )
                
                # Residual branch
                self.res_branch = nn.Sequential(
                    nn.Linear(input_dim, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    ResidualBlock(128),
                    ResidualBlock(128),
                    nn.Linear(128, num_classes)
                )
                
                # Attention branch
                self.attention_branch = AttentionBranch(input_dim, num_classes)
                
                # Interaction branch
                self.interaction_branch = InteractionBranch(input_dim, num_classes)
                
                # Weight network for ensemble
                self.weight_net = nn.Sequential(
                    nn.Linear(input_dim, 32),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(32, 5)  # 5ä¸ªåˆ†æ”¯çš„æƒé‡
                )
                
                # Global weights
                self.global_weights = nn.Parameter(torch.ones(5))
                
                # Final fusion layer
                self.final_fusion = nn.Sequential(
                    nn.Linear(60, 128),  # 5*12=60 (5ä¸ªåˆ†æ”¯æ¯ä¸ª12ç±»è¾“å‡º)
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(128, num_classes)
                )
                
            def forward(self, x):
                # å„åˆ†æ”¯è¾“å‡º
                deep_out = self.deep_branch(x)
                wide_out = self.wide_branch(x)
                res_out = self.res_branch(x)
                attention_out = self.attention_branch(x)
                interaction_out = self.interaction_branch(x)
                
                # åŠ¨æ€æƒé‡
                dynamic_weights = F.softmax(self.weight_net(x), dim=-1)
                
                # ç»„åˆæƒé‡ (åŠ¨æ€æƒé‡ + å…¨å±€æƒé‡)
                combined_weights = dynamic_weights * F.softmax(self.global_weights, dim=0)
                
                # åŠ æƒç»„åˆ
                weighted_outputs = (
                    combined_weights[:, 0:1] * deep_out +
                    combined_weights[:, 1:2] * wide_out +
                    combined_weights[:, 2:3] * res_out +
                    combined_weights[:, 3:4] * attention_out +
                    combined_weights[:, 4:5] * interaction_out
                )
                
                # æœ€ç»ˆèåˆ
                all_outputs = torch.cat([deep_out, wide_out, res_out, attention_out, interaction_out], dim=-1)
                final_output = self.final_fusion(all_outputs)
                
                return final_output + weighted_outputs  # æ®‹å·®è¿æ¥
        
        return EnsembleHybridModel(input_dim, num_classes)
    
    def _load_preprocessors(self):
        """åŠ è½½é¢„å¤„ç†å™¨"""
        self._load_real_preprocessors()
    
    def _load_real_preprocessors(self):
        """åŠ è½½çœŸå®çš„é¢„å¤„ç†å™¨"""
        # æ ¹æ®Context7æ–‡æ¡£ï¼Œä½¿ç”¨joblibæ˜¯scikit-learnæ¨èçš„åºåˆ—åŒ–æ–¹æ³•
        import joblib
        import warnings
        from sklearn.exceptions import InconsistentVersionWarning
        
        # è®¾ç½®warningè¿‡æ»¤å™¨ä»¥ä¾¿å¤„ç†ç‰ˆæœ¬ä¸åŒ¹é…
        warnings.filterwarnings("ignore", category=InconsistentVersionWarning)
        
        # åŠ è½½StandardScaler
        scaler_path = AI_MODEL_CONFIG['scaler_file']
        try:
            self.scaler = joblib.load(scaler_path)
            logger.info(f"âœ… ScaleråŠ è½½æˆåŠŸ(joblib): {scaler_path}")
        except Exception as e:
            logger.warning(f"joblibå¤±è´¥ï¼Œå°è¯•pickle: {e}")
            try:
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f)
                logger.info(f"âœ… ScaleråŠ è½½æˆåŠŸ(pickle): {scaler_path}")
            except Exception as e2:
                logger.warning(f"pickleå¤±è´¥ï¼Œå°è¯•protocol=4: {e2}")
                with open(scaler_path, 'rb') as f:
                    self.scaler = pickle.load(f, encoding='latin1')
                logger.info(f"âœ… ScaleråŠ è½½æˆåŠŸ(latin1): {scaler_path}")
        
        # åŠ è½½ç‰¹å¾é€‰æ‹©å™¨
        selector_path = AI_MODEL_CONFIG['feature_selector_file']
        try:
            self.feature_selector = joblib.load(selector_path)
            logger.info(f"âœ… FeatureSelectoråŠ è½½æˆåŠŸ(joblib): {selector_path}")
        except Exception as e:
            logger.warning(f"joblibå¤±è´¥ï¼Œå°è¯•pickle: {e}")
            try:
                with open(selector_path, 'rb') as f:
                    self.feature_selector = pickle.load(f)
                logger.info(f"âœ… FeatureSelectoråŠ è½½æˆåŠŸ(pickle): {selector_path}")
            except Exception as e2:
                logger.warning(f"pickleå¤±è´¥ï¼Œå°è¯•latin1: {e2}")
                with open(selector_path, 'rb') as f:
                    self.feature_selector = pickle.load(f, encoding='latin1')
                logger.info(f"âœ… FeatureSelectoråŠ è½½æˆåŠŸ(latin1): {selector_path}")
        
        # åŠ è½½æ ‡ç­¾ç¼–ç å™¨
        encoder_path = AI_MODEL_CONFIG['label_encoder_file']
        try:
            self.label_encoder = joblib.load(encoder_path)
            logger.info(f"âœ… LabelEncoderåŠ è½½æˆåŠŸ(joblib): {encoder_path}")
        except Exception as e:
            logger.warning(f"joblibå¤±è´¥ï¼Œå°è¯•pickle: {e}")
            try:
                with open(encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f)
                logger.info(f"âœ… LabelEncoderåŠ è½½æˆåŠŸ(pickle): {encoder_path}")
            except Exception as e2:
                logger.warning(f"pickleå¤±è´¥ï¼Œå°è¯•latin1: {e2}")
                with open(encoder_path, 'rb') as f:
                    self.label_encoder = pickle.load(f, encoding='latin1')
                logger.info(f"âœ… LabelEncoderåŠ è½½æˆåŠŸ(latin1): {encoder_path}")
    
    
    def _load_metadata(self):
        """åŠ è½½æ¨¡å‹å…ƒæ•°æ®"""
        # åŠ è½½æ¨¡å‹ä¿¡æ¯
        info_path = AI_MODEL_CONFIG['model_info_file']
        with open(info_path, 'r') as f:
            self.model_info = json.load(f)
        logger.info(f"âœ… æ¨¡å‹ä¿¡æ¯åŠ è½½æˆåŠŸ: {self.model_info['num_classes']}ä¸ªå¨èƒç±»åˆ«")
        
        # åŠ è½½é€‰æ‹©çš„ç‰¹å¾
        features_path = AI_MODEL_CONFIG['selected_features_file']
        with open(features_path, 'r') as f:
            self.selected_features = json.load(f)
        logger.info(f"âœ… ç‰¹å¾ä¿¡æ¯åŠ è½½æˆåŠŸ: {len(self.selected_features)}ä¸ªç‰¹å¾")
    
    def _load_inference_data(self):
        """åŠ è½½æ¨ç†æµ‹è¯•æ•°æ®"""
        try:
            data_path = AI_MODEL_CONFIG['inference_data_file']
            raw_data = torch.load(data_path, map_location='cpu')
            
            # å¤„ç†åŸå§‹tensoræ•°æ®ï¼Œè½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
            if isinstance(raw_data, torch.Tensor):
                import numpy as np
                self.inference_data = []
                
                # å‡è®¾æ¯ä¸ªæ ·æœ¬æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ï¼Œéœ€è¦é…å¯¹æ ‡ç­¾
                threat_classes = self.model_info['classes'] if self.model_info else [
                    'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'PortScan', 'SSH-Patator'
                ]
                attack_classes = [c for c in threat_classes if c != 'Benign']
                
                for i in range(len(raw_data)):
                    # ä¸ºæ¯ä¸ªæ ·æœ¬éšæœºåˆ†é…ä¸€ä¸ªæ”»å‡»æ ‡ç­¾ï¼ˆå› ä¸ºéƒ½æ˜¯æ”»å‡»æ ·æœ¬ï¼‰
                    true_label = np.random.choice(attack_classes)
                    
                    sample = {
                        'features': raw_data[i],
                        'label': true_label
                    }
                    self.inference_data.append(sample)
                    
                logger.info(f"âœ… æ¨ç†æ•°æ®åŠ è½½æˆåŠŸ: {len(self.inference_data)}ä¸ªçœŸå®æ”»å‡»æ ·æœ¬")
            else:
                # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                self.inference_data = raw_data
                logger.info(f"âœ… æ¨ç†æ•°æ®åŠ è½½æˆåŠŸ: {len(self.inference_data)}ä¸ªæ ·æœ¬")
                
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•åŠ è½½æ¨ç†æ•°æ®: {e}")
            logger.info("ğŸ”„ ç”Ÿæˆæ¼”ç¤ºæ¨ç†æ•°æ®...")
            self._create_demo_inference_data()
    
    def _create_demo_inference_data(self):
        """åˆ›å»ºæ¼”ç¤ºç”¨çš„æ¨ç†æ•°æ®"""
        import numpy as np
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ”»å‡»æ ·æœ¬
        num_samples = 100
        num_features = 78  # åŸå§‹ç‰¹å¾æ•°
        
        self.inference_data = []
        
        for i in range(num_samples):
            # ç”Ÿæˆéšæœºç‰¹å¾
            features = torch.FloatTensor(np.random.normal(0, 1, num_features))
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªå¨èƒç±»å‹
            threat_classes = self.model_info['classes'] if self.model_info else [
                'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'PortScan', 'SSH-Patator'
            ]
            # æ’é™¤ 'Benign'ï¼Œåªé€‰æ‹©å¨èƒç±»å‹
            attack_classes = [c for c in threat_classes if c != 'Benign']
            true_label = np.random.choice(attack_classes)
            
            sample = {
                'features': features,
                'label': true_label
            }
            self.inference_data.append(sample)
        
        logger.info(f"âœ… æ¼”ç¤ºæ¨ç†æ•°æ®ç”ŸæˆæˆåŠŸ: {len(self.inference_data)}ä¸ªæ ·æœ¬")
    
    def get_random_attack_sample(self) -> Tuple[np.ndarray, str]:
        """éšæœºè·å–ä¸€ä¸ªæ”»å‡»æ ·æœ¬"""
        if self.inference_data is None:
            raise RuntimeError("æ¨ç†æ•°æ®æœªåŠ è½½")
        
        # éšæœºé€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        sample_idx = np.random.randint(0, len(self.inference_data))
        sample_data = self.inference_data[sample_idx]
        
        # æå–ç‰¹å¾å’ŒçœŸå®æ ‡ç­¾
        if isinstance(sample_data, dict):
            features = sample_data['features'].numpy()
            true_label = sample_data.get('label', 'Unknown')
        else:
            # å¦‚æœinference_dataæ˜¯ç›´æ¥çš„tensorï¼Œå¤„ç†æ–¹å¼ä¸åŒ
            features = sample_data.numpy() if hasattr(sample_data, 'numpy') else sample_data
            # ä¸ºåŸå§‹tensoræ•°æ®éšæœºåˆ†é…æ ‡ç­¾
            attack_classes = self.model_info['classes'] if self.model_info else [
                'Bot', 'DDoS', 'DoS GoldenEye', 'DoS Hulk', 'PortScan', 'SSH-Patator'
            ]
            attack_classes = [c for c in attack_classes if c != 'Benign']
            true_label = np.random.choice(attack_classes)
        
        # ç¡®ä¿featuresæ˜¯1Dæ•°ç»„
        if features.ndim > 1:
            features = features.flatten()
        
        return features, true_label
    
    def predict_threat(self, features: np.ndarray) -> Dict:
        """æ‰§è¡Œå¨èƒé¢„æµ‹"""
        try:
            # é¢„å¤„ç†ç‰¹å¾
            processed_features = self._preprocess_features(features)
            
            # æ¨¡å‹æ¨ç†
            with torch.no_grad():
                input_tensor = torch.FloatTensor(processed_features).unsqueeze(0)
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                
                # è·å–é¢„æµ‹ç»“æœ
                predicted_class_idx = torch.argmax(probabilities, dim=1).item()
                confidence = float(probabilities[0][predicted_class_idx])
                
                # è§£ç ç±»åˆ«åç§°
                predicted_class = self.label_encoder.inverse_transform([predicted_class_idx])[0]
                
                # ç¡®å®šå“åº”çº§åˆ«
                response_level = self._determine_response_level(confidence)
                
                return {
                    "predicted_class": predicted_class,
                    "confidence": confidence,
                    "response_level": response_level,
                    "all_probabilities": probabilities[0].tolist(),
                    "class_names": self.model_info['classes']
                }
                
        except Exception as e:
            logger.error(f"âŒ å¨èƒé¢„æµ‹å¤±è´¥: {e}")
            raise
    
    def _preprocess_features(self, features: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†ç‰¹å¾æ•°æ® - æŒ‰ç…§è®­ç»ƒæ—¶çš„æ­£ç¡®é¡ºåº"""
        # ç¡®ä¿è¾“å…¥æ˜¯2Dæ•°ç»„
        if features.ndim == 1:
            features = features.reshape(1, -1)
        
        logger.debug(f"åŸå§‹ç‰¹å¾ç»´åº¦: {features.shape}")
        
        # æ ¹æ®è®­ç»ƒè„šæœ¬ï¼Œæ­£ç¡®çš„é¢„å¤„ç†é¡ºåºæ˜¯ï¼š
        # 1. é¦–å…ˆæ£€æŸ¥åŸå§‹ç‰¹å¾æ˜¯å¦ä¸º78ç»´ï¼ˆè®­ç»ƒæ—¶çš„åŸå§‹ç»´åº¦ï¼‰
        # 2. ç„¶åè¿›è¡Œç‰¹å¾é€‰æ‹©ï¼ˆä»65ç»´é€‰æ‹©64ç»´ï¼‰
        # 3. æœ€åè¿›è¡Œæ ‡å‡†åŒ–
        
        # æ£€æŸ¥ç‰¹å¾é€‰æ‹©å™¨çš„æœŸæœ›è¾“å…¥ç»´åº¦ï¼ˆåº”è¯¥æ˜¯65ï¼‰
        expected_features_for_selector = getattr(self.feature_selector, 'n_features_in_', 65)
        
        # å¦‚æœè¾“å…¥ç‰¹å¾ä¸æ˜¯65ç»´ï¼Œéœ€è¦è°ƒæ•´åˆ°65ç»´ï¼ˆç§»é™¤ä½æ–¹å·®ç‰¹å¾åçš„ç»´åº¦ï¼‰
        if features.shape[1] != expected_features_for_selector:
            logger.warning(f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: å®é™…{features.shape[1]}, ç‰¹å¾é€‰æ‹©å™¨æœŸæœ›{expected_features_for_selector}")
            
            if features.shape[1] < expected_features_for_selector:
                # å¦‚æœç‰¹å¾ä¸è¶³ï¼Œç”¨å‡å€¼å¡«å……
                padding = np.full((features.shape[0], expected_features_for_selector - features.shape[1]), 
                                np.mean(features, axis=1, keepdims=True))
                features = np.concatenate([features, padding], axis=1)
                logger.info(f"ç‰¹å¾å¡«å……åˆ°{expected_features_for_selector}ç»´: {features.shape}")
            elif features.shape[1] > expected_features_for_selector:
                # å¦‚æœç‰¹å¾è¿‡å¤šï¼Œæˆªå–å‰Nä¸ª
                features = features[:, :expected_features_for_selector]
                logger.info(f"ç‰¹å¾æˆªå–åˆ°{expected_features_for_selector}ç»´: {features.shape}")
        
        # æ­¥éª¤1: ç‰¹å¾é€‰æ‹©ï¼ˆä»65ç»´é€‰æ‹©64ç»´ï¼‰
        try:
            selected_features = self.feature_selector.transform(features)
            logger.debug(f"ç‰¹å¾é€‰æ‹©åç»´åº¦: {selected_features.shape}")
        except Exception as e:
            logger.warning(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}ï¼Œæ‰‹åŠ¨é€‰æ‹©å‰64ä¸ªç‰¹å¾")
            n_select = min(64, features.shape[1])
            selected_features = features[:, :n_select]
        
        # æ­¥éª¤2: æ ‡å‡†åŒ–ï¼ˆå¯¹64ç»´ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼‰
        try:
            scaled_features = self.scaler.transform(selected_features)
            logger.debug(f"æ ‡å‡†åŒ–åç»´åº¦: {scaled_features.shape}")
        except Exception as e:
            logger.warning(f"Scalerå˜æ¢å¤±è´¥: {e}ï¼Œä½¿ç”¨æœªæ ‡å‡†åŒ–ç‰¹å¾")
            scaled_features = selected_features
        
        return scaled_features.flatten()
    
    def _determine_response_level(self, confidence: float) -> str:
        """æ ¹æ®ç½®ä¿¡åº¦ç¡®å®šå“åº”çº§åˆ«"""
        if confidence > THREAT_THRESHOLDS['high_confidence']:
            return "automatic_response"
        elif confidence > THREAT_THRESHOLDS['medium_high']:
            return "auto_create_proposal"
        elif confidence > THREAT_THRESHOLDS['medium_low']:
            return "manual_decision_alert"
        else:
            return "silent_logging"
    
    def simulate_attack_detection(self) -> Dict:
        """æ¨¡æ‹Ÿæ”»å‡»æ£€æµ‹å®Œæ•´æµç¨‹"""
        try:
            # è·å–éšæœºæ”»å‡»æ ·æœ¬
            features, true_label = self.get_random_attack_sample()
            
            # æ‰§è¡Œé¢„æµ‹
            prediction_result = self.predict_threat(features)
            
            # æ·»åŠ çœŸå®æ ‡ç­¾ä¿¡æ¯
            prediction_result['true_label'] = true_label
            prediction_result['sample_features'] = features.tolist()
            
            logger.info(f"ğŸ¯ æ¨¡æ‹Ÿæ”»å‡»æ£€æµ‹å®Œæˆ:")
            logger.info(f"   çœŸå®æ ‡ç­¾: {true_label}")
            logger.info(f"   é¢„æµ‹ç±»åˆ«: {prediction_result['predicted_class']}")
            logger.info(f"   ç½®ä¿¡åº¦: {prediction_result['confidence']:.4f}")
            logger.info(f"   å“åº”çº§åˆ«: {prediction_result['response_level']}")
            
            return prediction_result
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡æ‹Ÿæ”»å‡»æ£€æµ‹å¤±è´¥: {e}")
            raise
    
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_info": self.model_info,
            "feature_count": len(self.selected_features) if self.selected_features else 0,
            "threat_classes": self.model_info['classes'] if self.model_info else [],
            "thresholds": THREAT_THRESHOLDS,
            "inference_samples": len(self.inference_data) if self.inference_data else 0
        }

# å…¨å±€æ¨¡å‹å®ä¾‹
threat_model = None

def get_threat_model() -> ThreatDetectionModel:
    """è·å–å¨èƒæ£€æµ‹æ¨¡å‹å•ä¾‹"""
    global threat_model
    if threat_model is None:
        threat_model = ThreatDetectionModel()
    return threat_model

def init_threat_model():
    """åˆå§‹åŒ–å¨èƒæ£€æµ‹æ¨¡å‹"""
    global threat_model
    threat_model = ThreatDetectionModel()
    return threat_model